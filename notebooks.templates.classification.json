{"version":3,"kind":"Notebook","sha256":"2101497368618ac0549a0c6e36b2996b558676dca082bd4be95fddf7103cc6f6","slug":"notebooks.templates.classification","location":"/notebooks/templates/classification.ipynb","dependencies":[],"frontmatter":{"title":"Classification of Sentinel-2 imagery","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"authors":[{"nameParsed":{"literal":"Wolfgang Wagner","given":"Wolfgang","family":"Wagner"},"name":"Wolfgang Wagner","id":"contributors-myst-generated-uid-0"},{"nameParsed":{"literal":"Martin Schobben","given":"Martin","family":"Schobben"},"name":"Martin Schobben","id":"contributors-myst-generated-uid-1"},{"nameParsed":{"literal":"Nikolas Pikall","given":"Nikolas","family":"Pikall"},"name":"Nikolas Pikall","id":"contributors-myst-generated-uid-2"},{"nameParsed":{"literal":"Joseph Wagner","given":"Joseph","family":"Wagner"},"name":"Joseph Wagner","id":"contributors-myst-generated-uid-3"},{"nameParsed":{"literal":"Davide Festa","given":"Davide","family":"Festa"},"name":"Davide Festa","id":"contributors-myst-generated-uid-4"},{"nameParsed":{"literal":"Felix David Reuß","given":"Felix David","family":"Reuß"},"name":"Felix David Reuß","id":"contributors-myst-generated-uid-5"},{"nameParsed":{"literal":"Luka Jović","given":"Luka","family":"Jović"},"name":"Luka Jović","id":"contributors-myst-generated-uid-6"}],"open_access":true,"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"Apache-2.0","url":"https://opensource.org/licenses/Apache-2.0","name":"Apache License 2.0","free":true,"osi":true}},"copyright":"2024","affiliations":[{"id":"UAlbany","name":"University at Albany (SUNY)","department":"Atmospheric and Environmental Sciences","url":"https://www.albany.edu/daes"},{"id":"CISL","name":"NSF National Center for Atmospheric Research","department":"Computational and Information Systems Lab","url":"https://www.cisl.ucar.edu"},{"id":"Unidata","name":"NSF Unidata","url":"https://www.unidata.ucar.edu"},{"id":"Argonne","name":"Argonne National Laboratory","department":"Environmental Science Division","url":"https://www.anl.gov/evs"},{"id":"CarbonPlan","name":"CarbonPlan","url":"https://carbonplan.org"},{"id":"NVIDIA","name":"NVIDIA Corporation","url":"https://www.nvidia.com/"}],"numbering":{"title":{"offset":2}},"exports":[{"format":"ipynb","filename":"classification.ipynb","url":"/eo-datascience-cookbook/build/classification-dd1d74af33b32cd2e28735dbde3e797e.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"strong","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Finding forests with satellite imagery","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Q1sUdLcqvV"}],"key":"fbbXUoo4wr"}],"key":"fvYqzNMYjc"},{"type":"heading","depth":2,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Data Acquisition","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"pDoUhnrx9S"}],"identifier":"data-acquisition","label":"Data Acquisition","html_id":"data-acquisition","implicit":true,"key":"u6sCgNy5cQ"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"In this chapter, we will employ machine learning techniques to classify a scene using satellite imagery. Specifically, we will utilize ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"ZeP9aldQRi"},{"type":"inlineCode","value":"scikit-learn","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"K0bVThDOER"},{"type":"text","value":" to implement two distinct classifiers and subsequently compare their results. To begin, we need to import the following modules.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"ipGZPxFLnV"}],"key":"iCv4FlOzkp"}],"key":"VqzxbHISpL"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from datetime import datetime, timedelta\n\nimport cmcrameri as cmc  # noqa: F401\nimport geopandas as gpd\nimport matplotlib.colors as colors\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport odc.stac\nimport pandas as pd\nimport pystac_client\nimport rioxarray  # noqa: F401\nimport xarray as xr\nfrom odc.geo.geobox import GeoBox\nfrom shapely.geometry import Polygon\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB","key":"wQg6R3c7fE"},{"type":"outputs","id":"QBHklnRt4k8eukEoVM9vn","children":[],"key":"kujpJOV6Ll"}],"key":"aYqhHuhu5c"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Before we start, we need to load the data. We will use ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PmVxDTQjFo"},{"type":"inlineCode","value":"odc-stac","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mTCqkosGDb"},{"type":"text","value":" to obtain data from Earth Search by Element 84. Here we define the area of interest and the time frame, aswell as the EPSG code and the resolution.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ormnNw6Ota"}],"key":"jQjjJmAxWO"},{"type":"heading","depth":3,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Searching in the Catalog","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"sn8CQtGSl7"}],"identifier":"searching-in-the-catalog","label":"Searching in the Catalog","html_id":"searching-in-the-catalog","implicit":true,"key":"siKvz5rin9"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"The module ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"rxNhT69Tjt"},{"type":"inlineCode","value":"odc-stac","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"eVkWGm3KSB"},{"type":"text","value":" provides access to free, open source satelite data. To retrieve the data, we must define  several parameters that specify the location and time period for the satellite data. Additionally, we must specify the data collection we wish to access, as multiple collections are available. In this example, we will use multispectral imagery from the Sentinel-2 satellite.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"Ntzk9fddYq"}],"key":"eku5q9YhtR"}],"key":"Y6niWu4iPH"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"dx = 0.0006  # 60m resolution\nepsg = 4326\n\n# Set Spatial extent\nlatmin, latmax = 47.86, 48.407\nlonmin, lonmax = 16.32, 16.9\nbounds = (lonmin, latmin, lonmax, latmax)\n\n\n# Set Temporal extent\nstart_date = datetime(year=2024, month=5, day=1)\nend_date = start_date + timedelta(days=10)\n\ntime_format = \"%Y-%m-%d\"\ndate_query = start_date.strftime(time_format) + \"/\" + end_date.strftime(time_format)\n\n# Search for Sentinel-2 data\nitems = (\n    pystac_client.Client.open(\"https://earth-search.aws.element84.com/v1\")\n    .search(\n        bbox=bounds,\n        collections=[\"sentinel-2-l2a\"],\n        datetime=date_query,\n        limit=100,\n    )\n    .item_collection()\n)\nprint(len(items), \"scenes found\")","key":"JYkwHj0781"},{"type":"outputs","id":"3vJYS0m0BNAPlvWuCVmI8","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"10 scenes found\n"},"key":"waypgmI7Gb"}],"key":"OMcWE5luTO"}],"key":"zsTb2WBYnC"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"We will now focus on the area south-east of Vienna, where the Nationalpark ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IibKLyptcs"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Donauauen","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DRwEucMuGW"}],"key":"nQKm8X5V0v"},{"type":"text","value":" is situated. The time frame we are interested in is the beginning of May 2024.\nAfter passing these parameters to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HbVrOIZeqz"},{"type":"inlineCode","value":"stac-catalog","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UtrFcjRZq6"},{"type":"text","value":" we have found ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DZHXaa1g53"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"10 scenes","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Qmg5UAZkpH"}],"key":"eMNanCdu5y"},{"type":"text","value":" that we can use for our analysis.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"So2GrnGX6L"}],"key":"WywEONwNr1"},{"type":"heading","depth":3,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Loading the Data","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"sp6eqBjuDm"}],"identifier":"loading-the-data","label":"Loading the Data","html_id":"loading-the-data","implicit":true,"key":"O3kJBTX46k"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Now we will load the data directly into an ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ZrK85quIzB"},{"type":"inlineCode","value":"xarray","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"cy2AGLWT3a"},{"type":"text","value":" dataset, which we can use to perform computations on the data. ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ldegYDcyfJ"},{"type":"inlineCode","value":"xarray","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"AKXCYGUx08"},{"type":"text","value":" is a powerful library for working with multi-dimensional arrays, making it well-suited for handling satellite data.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"mKkOsECuhi"}],"key":"UuZm2FobUg"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Here’s how we can load the data using odc-stac and xarray:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"YmUjSKybw1"}],"key":"ewKgncI4od"}],"key":"NAePWmw6pZ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# define a geobox for my region\ngeobox = GeoBox.from_bbox(bounds, crs=f\"epsg:{epsg}\", resolution=dx)\n\n# lazily combine items into a datacube\ndc = odc.stac.load(\n    items,\n    bands=[\"scl\", \"red\", \"green\", \"blue\", \"nir\"],\n    chunks={\"time\": 5, \"x\": 600, \"y\": 600},\n    geobox=geobox,\n    resampling=\"bilinear\",\n)\ndc","key":"Gh0iLBz71Y"},{"type":"outputs","id":"SU5AFnXNqgOmmLt3ppCgI","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"execute_result","execution_count":3,"metadata":{},"data":{"text/plain":{"content":"<xarray.Dataset> Size: 79MB\nDimensions:      (latitude: 913, longitude: 967, time: 10)\nCoordinates:\n  * latitude     (latitude) float64 7kB 48.41 48.41 48.41 ... 47.86 47.86 47.86\n  * longitude    (longitude) float64 8kB 16.32 16.32 16.32 ... 16.9 16.9 16.9\n  * time         (time) datetime64[us] 80B 2024-05-01T09:57:21.858000 ... 202...\n    spatial_ref  int32 4B 4326\nData variables:\n    scl          (time, latitude, longitude) uint8 9MB dask.array<chunksize=(5, 600, 600), meta=np.ndarray>\n    red          (time, latitude, longitude) uint16 18MB dask.array<chunksize=(5, 600, 600), meta=np.ndarray>\n    green        (time, latitude, longitude) uint16 18MB dask.array<chunksize=(5, 600, 600), meta=np.ndarray>\n    blue         (time, latitude, longitude) uint16 18MB dask.array<chunksize=(5, 600, 600), meta=np.ndarray>\n    nir          (time, latitude, longitude) uint16 18MB dask.array<chunksize=(5, 600, 600), meta=np.ndarray>","content_type":"text/plain"},"text/html":{"content_type":"text/html","hash":"fb10d93f74abdc1260bb744e3139a9d7","path":"/eo-datascience-cookbook/build/fb10d93f74abdc1260bb744e3139a9d7.html"}}},"key":"lWxior4MuM"}],"key":"uUMsKiRjPf"}],"key":"CmNP3bDYyy"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Data Visualization","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FbtknNsECZ"}],"identifier":"data-visualization","label":"Data Visualization","html_id":"data-visualization","implicit":true,"key":"YAc7xatekK"},{"type":"heading","depth":3,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"RGB Image","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"fiCVgFgVph"}],"identifier":"rgb-image","label":"RGB Image","html_id":"rgb-image","implicit":true,"key":"VLGSsUyAK1"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"With the image data now in our possession, we can proceed with computations and visualizations.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"U3dJZJNwYo"}],"key":"ZFw2vah3Zq"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"First, we define a mask to exclude cloud cover and areas with missing data. Subsequently, we create a composite median image, where each pixel value represents the median value across all the scenes we have identified. This approach helps to eliminate clouds and outliers present in some of the images, thereby providing a clearer and more representative visualization of the scene.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"V87QXh9lYx"}],"key":"djc9vzofYh"}],"key":"WU8oimXGG2"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# define a mask for valid pixels (non-cloud)\n\n\ndef is_valid_pixel(data):\n    # include only vegetated, not_vegitated, water, and snow\n    return ((data > 3) & (data < 7)) | (data == 11)\n\n\ndc[\"valid\"] = is_valid_pixel(dc.scl)\n\n# compute the masked median\nrgb_median = (\n    dc[[\"red\", \"green\", \"blue\"]]\n    .where(dc.valid)\n    .to_dataarray(dim=\"band\")\n    .median(dim=\"time\")\n    .astype(int)\n)\n\n# plot the median composite\ntitle_rgb = (\n    \"RGB - Median Composite\"\n    + f\"\\n{start_date.strftime('%d.%m.%Y')} - {end_date.strftime('%d.%m.%Y')}\"\n)\nrgb_median.plot.imshow(robust=True).axes.set_title(title_rgb)\nplt.show()","key":"UOOug2JKXF"},{"type":"outputs","id":"8yDf8hG9kOCgfBnGo6GJN","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stderr","text":"/home/runner/micromamba/envs/eo-datascience-cookbook/lib/python3.13/site-packages/rasterio/warp.py:385: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n  dest = _reproject(\n"},"key":"FuqKNrLie9"},{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"ab6355953f9d4fac62a75d0c3ffd1a82","path":"/eo-datascience-cookbook/build/ab6355953f9d4fac62a75d0c3ffd1a82.png"}}},"key":"xycw1YBMlx"}],"key":"Csb3cN596e"}],"key":"I7KfSyjHIH"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"False Color Image","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TE2Zuo9FDz"}],"identifier":"false-color-image","label":"False Color Image","html_id":"false-color-image","implicit":true,"key":"CIDq6Ivcq2"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"In addition to the regular RGB Image, we can swap any of the bands from the visible spectrum with any other bands. In this specific case the red band has been changed to the near infrared band. This allows us to see vegetated areas more clearly, since they now appear in a bright red color. This is due to the fact that plants absorb regular red light while reflecting near infrared light ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"e742OQqQLE"},{"type":"citeGroup","kind":"parenthetical","children":[{"type":"cite","kind":"parenthetical","label":"nasa2020","identifier":"nasa2020","children":[{"type":"text","value":"NASA, 2020","key":"mfuX9HOX0e"}],"enumerator":"1","key":"JfCmdsEAh1"}],"key":"eRfLK7KB7b"},{"type":"text","value":".","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"iWxwwz37y1"}],"key":"vctrNTX1QJ"}],"key":"QFRya0vcqL"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# compute a false color image\n# near infrared instead of red\nfc_median = (\n    dc[[\"nir\", \"green\", \"blue\"]]\n    .where(dc.valid)\n    .to_dataarray(dim=\"band\")\n    .transpose(..., \"band\")\n    .median(dim=\"time\")\n    .astype(int)\n)\n\ntitle_fc = (\n    \"False color - Median Composite\"\n    + f\"\\n{start_date.strftime('%d.%m.%Y')} - {end_date.strftime('%d.%m.%Y')}\"\n)\nfc_median.plot.imshow(robust=True).axes.set_title(title_fc)\nplt.show()","key":"hXIAKtmycn"},{"type":"outputs","id":"1FOlqVBU32IQNN8sAConu","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"da6a8f5ac76fd1806e03b5cf70b7e830","path":"/eo-datascience-cookbook/build/da6a8f5ac76fd1806e03b5cf70b7e830.png"}}},"key":"Fdk2AQmMXu"}],"key":"QapoKRXPZ9"}],"key":"ESLZj8oZRW"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"NDVI Image","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZwmowSAvvJ"}],"identifier":"ndvi-image","label":"NDVI Image","html_id":"ndvi-image","implicit":true,"key":"JWwI719FuQ"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"To get an first impression of the data, we can calculate the NDVI (Normalized Difference Vegetation Index) and plot it. The NDVI is calculated by useing the following formula. ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"uOWXVTMtZy"},{"type":"citeGroup","kind":"parenthetical","children":[{"type":"cite","kind":"parenthetical","label":"rouse1974monitoring","identifier":"rouse1974monitoring","children":[{"type":"text","value":"Rouse ","key":"pnyKYdtArM"},{"type":"emphasis","children":[{"type":"text","value":"et al.","key":"UAI7LMcIWV"}],"key":"Zwg4ibmhwy"},{"type":"text","value":", 1974","key":"ADVT97WPEl"}],"enumerator":"2","key":"sqdNVTFovi"}],"key":"CCttAGTsJv"}],"key":"huzPqOiVcq"},{"type":"math","value":"NDVI = \\frac{NIR - Red}{NIR + Red}","position":{"start":{"line":4,"column":1},"end":{"line":6,"column":1}},"html":"<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>N</mi><mi>D</mi><mi>V</mi><mi>I</mi><mo>=</mo><mfrac><mrow><mi>N</mi><mi>I</mi><mi>R</mi><mo>−</mo><mi>R</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>N</mi><mi>I</mi><mi>R</mi><mo>+</mo><mi>R</mi><mi>e</mi><mi>d</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">NDVI = \\frac{NIR - Red}{NIR + Red}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.1408em;vertical-align:-0.7693em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3714em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">d</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">d</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7693em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>","enumerator":"1","key":"slV0apV9c7"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"This gives us a good overview of the vegetation in the area. The values can range from -1 to 1 where the following meanings are associated with these values:","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"g2iQiK7YI8"}],"key":"IXV1v8p000"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"-1 to 0 indicate dead plants or inanimate objects","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"cMAS7Ja9ZL"}],"key":"JciDt3cHY7"}],"key":"HU18boSX8N"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"0 to 0.33 are unhealthy plants","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"CQneOV9oq9"}],"key":"m5rwGpBJUn"}],"key":"TskxkYzagW"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"0.33 to 0.66 are moderatly healthy plants","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"uCmabjUuf8"}],"key":"s1eFgwfFgZ"}],"key":"ZgxyZ3OsNe"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"0.66 to 1 are very healthy plants","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"vBhummjN9A"}],"key":"qqE9Ht4YW4"}],"key":"qIUTtUez25"}],"key":"lzY8v44DlU"}],"key":"LN8tEHdmy4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Normalized Difference Vegetation Index (NDVI)\n\n\ndef normalized_difference(a, b):\n    return (a - b * 1.0) / (a + b)\n\n\nndvi = normalized_difference(dc.nir, dc.red)\nndvi.median(dim=\"time\").plot.imshow(cmap=\"cmc.cork\", vmin=-1, vmax=1).axes.set_title(\n    \"NDVI\"\n)\nplt.show()","key":"Eu5otI9pxI"},{"type":"outputs","id":"uyuu1_RWLqytYDlPMU6HM","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 2 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"13cb675f3f937f79eafb571b4a3d1c0b","path":"/eo-datascience-cookbook/build/13cb675f3f937f79eafb571b4a3d1c0b.png"}}},"key":"lna78MpBtO"}],"key":"xhcIODvOoh"}],"key":"Hu8gD8x6CP"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Classification","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TwgeHkdYoM"}],"identifier":"classification","label":"Classification","html_id":"classification","implicit":true,"key":"h1Z2lyrdqC"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"In this chapter, we will classify the satellite data to identify forested areas within the scene. By using supervised machine learning techniques, we can train classifiers to distinguish between forested and non-forested regions based on the training data we provide. We will explore two different classifiers and compare their performance in accurately identifying forest areas.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"rp2HyZH5HV"}],"key":"kCeRySekGt"},{"type":"heading","depth":3,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Regions of Interest","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"YfoWoIJmNR"}],"identifier":"regions-of-interest","label":"Regions of Interest","html_id":"regions-of-interest","implicit":true,"key":"xwQXwGIuXO"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Since this is a supervised classification, we need to have some training data. Therefore we need to define areas or regions, which we are certain represent the feature which we are classifiying. In this case we are interested in forested areas and regions that are definitly not forested. These regions will be used to train our classifiers.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"WVC6785ZVY"}],"key":"apyAwm7Hbg"}],"key":"cZoZXX0TqW"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Define Polygons\nforest_areas = {\n    0: [\n        Polygon(\n            [\n                (16.482772, 47.901753),\n                (16.465133, 47.870124),\n                (16.510142, 47.874382),\n                (16.482772, 47.901753),\n            ]\n        )\n    ],\n    1: [\n        Polygon(\n            [\n                (16.594079, 47.938855),\n                (16.581914, 47.894454),\n                (16.620233, 47.910268),\n                (16.594079, 47.938855),\n            ]\n        )\n    ],\n    2: [\n        Polygon(\n            [\n                (16.67984, 47.978998),\n                (16.637263, 47.971091),\n                (16.660376, 47.929123),\n                (16.67984, 47.978998),\n            ]\n        )\n    ],\n    3: [\n        Polygon(\n            [\n                (16.756477, 48.000286),\n                (16.723024, 47.983256),\n                (16.739446, 47.972916),\n                (16.756477, 48.000286),\n            ]\n        )\n    ],\n    4: [\n        Polygon(\n            [\n                (16.80696, 48.135923),\n                (16.780806, 48.125583),\n                (16.798445, 48.115243),\n                (16.80696, 48.135923),\n            ]\n        )\n    ],\n    5: [\n        Polygon(\n            [\n                (16.684097, 48.144438),\n                (16.664634, 48.124366),\n                (16.690788, 48.118892),\n                (16.684097, 48.144438),\n            ]\n        )\n    ],\n    6: [\n        Polygon(\n            [\n                (16.550894, 48.169984),\n                (16.530822, 48.165118),\n                (16.558801, 48.137139),\n                (16.550894, 48.169984),\n            ]\n        )\n    ],\n    7: [\n        Polygon(\n            [\n                (16.588604, 48.402329),\n                (16.556976, 48.401112),\n                (16.580697, 48.382865),\n                (16.588604, 48.402329),\n            ]\n        )\n    ],\n}\n\nnonforest_areas = {\n    0: [\n        Polygon(\n            [\n                (16.674974, 48.269126),\n                (16.623882, 48.236281),\n                (16.682272, 48.213168),\n                (16.674974, 48.269126),\n            ]\n        )\n    ],\n    1: [\n        Polygon(\n            [\n                (16.375723, 48.228374),\n                (16.357476, 48.188839),\n                (16.399444, 48.185798),\n                (16.375723, 48.228374),\n            ]\n        )\n    ],\n    2: [\n        Polygon(\n            [\n                (16.457834, 48.26426),\n                (16.418907, 48.267301),\n                (16.440804, 48.23324),\n                (16.457834, 48.26426),\n            ]\n        )\n    ],\n    3: [\n        Polygon(\n            [\n                (16.519266, 48.101861),\n                (16.470607, 48.100645),\n                (16.500411, 48.07145),\n                (16.519266, 48.101861),\n            ]\n        )\n    ],\n    4: [\n        Polygon(\n            [\n                (16.453577, 48.051986),\n                (16.412217, 48.067192),\n                (16.425598, 48.012451),\n                (16.453577, 48.051986),\n            ]\n        )\n    ],\n}","key":"GuSj1yE1IU"},{"type":"outputs","id":"eO4k-_0uupySwdvBFxrwE","children":[],"key":"xyW0Ogh1ye"}],"key":"L46lBMqZqu"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Geoppandas Dataframe from Polygons\nforest_df = gpd.GeoDataFrame(\n    {\"geometry\": [poly[0] for poly in forest_areas.values()]}, crs=\"EPSG:4326\"\n)\nnonforest_df = gpd.GeoDataFrame(\n    {\"geometry\": [poly[0] for poly in nonforest_areas.values()]},\n    crs=\"EPSG:4326\",\n)\n\n\n# Plotting Regions of Interest\nfig, ax = plt.subplots()\nrgb_median.plot.imshow(ax=ax, robust=True)\nforest_df.plot(ax=ax, ec=\"C0\", fc=\"none\")\nnonforest_df.plot(ax=ax, ec=\"C1\", fc=\"none\")\nax.set_title(\"Regions of Interest\")\nax.set_aspect(\"equal\")\nplt.show()","key":"oq0Bar6uCF"},{"type":"outputs","id":"1SYnGpVKa3jaYfkxJpXsc","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"a77ab3d265de5c070711e3b39705afa9","path":"/eo-datascience-cookbook/build/a77ab3d265de5c070711e3b39705afa9.png"}}},"key":"ROkWLFIpr3"}],"key":"qVyFDuVE6a"}],"key":"f7y0uLGi3O"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Data Preparation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zYlq8CEZ8I"}],"identifier":"data-preparation","label":"Data Preparation","html_id":"data-preparation","implicit":true,"key":"gsIf20R3LT"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"In addition to the Regions of Interest we will extract the specific bands from the loaded dataset that we intend to use for the classification, which are the ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"LZYxS8ZTiB"},{"type":"inlineCode","value":"red, green, blue","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Q05B8VpN6g"},{"type":"text","value":" and ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"boHJxi4acr"},{"type":"inlineCode","value":"near-infrared","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"IHVlfzDcpn"},{"type":"text","value":" bands, although other bands can also be utilized. Using these bands, we will create both a training and a testing dataset. The training dataset will be used to train the classifier, while the testing dataset will be employed to evaluate its performance.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Yj0RwIoIAR"}],"key":"GD0mc7vaXS"}],"key":"LlRp06oXH0"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Classifiying dataset (only necessary bands)\nbands = [\"red\", \"green\", \"blue\", \"nir\"]\nds_class = dc[bands].where(dc.valid).median(dim=\"time\")\nds_class = ds_class.fillna(0)\n\n\ndef clip_array(ds: xr.Dataset, polygons):\n    clipped = ds.rio.clip(polygons, invert=False, all_touched=False, drop=True)\n    clipped_nan = clipped.where(clipped == ds)\n    return clipped_nan\n\n\n# Dictionaries with Dataarrays, each clipped by a Polygon\ndata_dict_feat = {\n    idx: clip_array(ds_class, polygon) for idx, polygon in forest_areas.items()\n}\ndata_dict_nonfeat = {\n    idx: clip_array(ds_class, polygon) for idx, polygon in nonforest_areas.items()\n}","key":"YY2cqDErJA"},{"type":"outputs","id":"uHazW3O_UIAYWXQIARSfH","children":[],"key":"u1qARA8KXV"}],"key":"BCDpbgKMpq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Reshape the polygon dataarrays to get a tuple (one value per band) of pixel values\nfeat_data = [\n    xarray.to_array().values.reshape(len(bands), -1).T\n    for xarray in data_dict_feat.values()\n]  # replaced median_data_dict_feat with data_dict_feat\nnonfeat_data = [\n    xarray.to_array().values.reshape(len(bands), -1).T\n    for xarray in data_dict_nonfeat.values()\n]  # replaced median_data_dict_feat with data_dict_feat\n\n# The rows of the different polygons are concatenated to a single array for further processing\nfeat_values = np.concatenate(feat_data)\nnonfeat_values = np.concatenate(nonfeat_data)\n\n# Drop Nan Values\nX_feat_data = feat_values[~np.isnan(feat_values).any(axis=1)]\nX_nonfeat_data = nonfeat_values[~np.isnan(nonfeat_values).any(axis=1)]","key":"CGfxZghPZQ"},{"type":"outputs","id":"SNqIrA5s5coIsTCwFPU8r","children":[],"key":"Ov7LushyP0"}],"key":"KBZ7m6p9Dd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Creating Output Vector (1 for pixel is features; 0 for pixel is not feature)\ny_feat_data = np.ones(X_feat_data.shape[0])\ny_nonfeat_data = np.zeros(X_nonfeat_data.shape[0])\n\n# Concatenate all Classes for training\nX = np.concatenate([X_feat_data, X_nonfeat_data])\ny = np.concatenate([y_feat_data, y_nonfeat_data])\n\n# Split into Training and Testing Data.\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=42\n)","key":"PLOtypez6G"},{"type":"outputs","id":"vs_NFjwcelSm7EPzwnzOw","children":[],"key":"my4NfOIByO"}],"key":"tqipkymFX4"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now that we have prepared the training and testing data, we will create an image array of the actual scene that we intend to classify. This array will serve as the input for our classification algorithms, allowing us to apply the trained classifiers to the entire scene and identify the forested and non-forested areas accurately.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wKBuwVbXoR"}],"key":"d5mSvwMapW"}],"key":"jVuuE7FUrO"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"image_data = (\n    ds_class[bands].to_array(dim=\"band\").transpose(\"latitude\", \"longitude\", \"band\")\n)\n\n# Reshape the image data\nnum_of_pixels = ds_class.sizes[\"longitude\"] * ds_class.sizes[\"latitude\"]\nnum_of_bands = len(bands)\nX_image_data = image_data.values.reshape(num_of_pixels, num_of_bands)","key":"N3wTfHV3A0"},{"type":"outputs","id":"nFYgNaFID2qYMP08Nj_tY","children":[],"key":"z3cFhvMbON"}],"key":"UN5Juv1pev"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Classifiying with Naive Bayes","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ztjeZhZH34"}],"identifier":"classifiying-with-naive-bayes","label":"Classifiying with Naive Bayes","html_id":"classifiying-with-naive-bayes","implicit":true,"key":"piYDSIzGQl"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Now that we have prepared all the needed data, we can begin the actual classification process.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"VWKg3Su60E"}],"key":"nKiTBACo0W"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"We will start with a ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"fb7dkRMCmF"},{"type":"emphasis","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Naive Bayes","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"CczYyV8Vu4"}],"key":"AMPU8jJm1p"},{"type":"text","value":" classifier. First, we will train the classifier using our training dataset. Once trained, we will apply the classifier to the actual image to identify the forested and non-forested areas.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"my5896c7WT"}],"key":"LOznzqQUUf"}],"key":"PxgQeOK4zM"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Naive Bayes initialization and training\nnb = GaussianNB()\nnb_test = nb.fit(X_train, y_train)\nnb_predict = nb.predict(X_test)\n\n# Prediction on image\nnb_predict_img = nb.predict(X_image_data)\nnb_predict_img = nb_predict_img.reshape(\n    ds_class.sizes[\"latitude\"], ds_class.sizes[\"longitude\"]\n)\n\n# Adding the Naive Bayes Prediction to the dataset\nds_class[\"NB-forest\"] = xr.DataArray(\n    nb_predict_img,\n    dims=[\"latitude\", \"longitude\"],\n    coords={\n        \"longitude\": ds_class[\"longitude\"],\n        \"latitude\": ds_class[\"latitude\"],\n    },\n)","key":"TuiKz0Q6jy"},{"type":"outputs","id":"LC58uivDPV-SJqv6XS0oq","children":[],"key":"emIfXEIVp5"}],"key":"DDnoYEgsOX"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"To evaluate the effectiveness of the classification, we will plot the image predicted by the classifier. Additionally, we will examine the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hLotCnbK89"},{"type":"inlineCode","value":"Classification Report","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YGaiuoAgGl"},{"type":"text","value":" and the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eeXyRdmUnE"},{"type":"inlineCode","value":"Confusion Matrix","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Vhnn3FFmQ7"},{"type":"text","value":" to gain further insights into the classifier’s performance.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CtqdnApxBa"}],"key":"aGxjM5djcm"}],"key":"usgZzr4Qtj"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Plot Naive Bayes\nalpha = 1\ncmap_green = colors.ListedColormap([(1, 1, 1, alpha), \"green\"])\n\nplot = ds_class[\"NB-forest\"].plot.imshow(\n    cmap=cmap_green, cbar_kwargs={\"ticks\": [0.25, 0.75]}\n)\ncbar = plot.colorbar\ncbar.set_ticklabels([\"non-forest\", \"forest\"])\nplot.axes.set_title(\"Naive Bayes Classification\")\nplt.show()\n\n# Print the Classification report\nprint(\"NAIVE BAYES: \\n \" + classification_report(y_test, nb_predict))\n\n# Print the confusion matrix\ncon_mat_nb = pd.DataFrame(\n    confusion_matrix(y_test, nb_predict),\n    index=[\"Actual Negative\", \"Actual Positive\"],\n    columns=[\"Predicted Negative\", \"Predicted Positive\"],\n)\ndisplay(con_mat_nb)","key":"sduJbH25jh"},{"type":"outputs","id":"b7fAh5wfwN1QuRy_DTExh","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 2 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"ffb395d59a05985a2643941fffbb0475","path":"/eo-datascience-cookbook/build/ffb395d59a05985a2643941fffbb0475.png"}}},"key":"iC5Z2IBRNC"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"NAIVE BAYES: \n               precision    recall  f1-score   support\n\n         0.0       0.95      0.82      0.88      6626\n         1.0       0.81      0.95      0.87      5485\n\n    accuracy                           0.88     12111\n   macro avg       0.88      0.88      0.88     12111\nweighted avg       0.89      0.88      0.88     12111\n\n"},"key":"H3Ralimd8H"},{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"                 Predicted Negative  Predicted Positive\nActual Negative                5415                1211\nActual Positive                 290                5195","content_type":"text/plain"},"text/html":{"content":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Predicted Negative</th>\n      <th>Predicted Positive</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Actual Negative</th>\n      <td>5415</td>\n      <td>1211</td>\n    </tr>\n    <tr>\n      <th>Actual Positive</th>\n      <td>290</td>\n      <td>5195</td>\n    </tr>\n  </tbody>\n</table>\n</div>","content_type":"text/html"}}},"key":"TWdTOCIbY2"}],"key":"doyoftbJ7J"}],"key":"Mn2iOYkmPm"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Classifiying with Random Forest","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HlnhmEVaoh"}],"identifier":"classifiying-with-random-forest","label":"Classifiying with Random Forest","html_id":"classifiying-with-random-forest","implicit":true,"key":"tHn03pYL61"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"To ensure our results are robust, we will explore an additional classifier. In this section, we will use the Random Forest classifier. The procedure for using this classifier is the same as before: we will train the classifier using our training dataset and then apply it to the actual image to classify the scene.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"elBdyRq4Ue"}],"key":"oGwaGkgIm0"}],"key":"sUIv1VNjJJ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Random Forest initialization and training\nrf = RandomForestClassifier(n_estimators=100)\nrf_test = rf.fit(X_train, y_train)\nrf_predict = rf.predict(X_test)\n\n# Prediction on image\nrf_predict_img = rf.predict(X_image_data)\nrf_predict_img = rf_predict_img.reshape(\n    ds_class.sizes[\"latitude\"], ds_class.sizes[\"longitude\"]\n)\n\n# Adding the Random Forest Prediction to the dataset\nds_class[\"RF-forest\"] = xr.DataArray(\n    rf_predict_img,\n    dims=[\"latitude\", \"longitude\"],\n    coords={\n        \"longitude\": ds_class[\"longitude\"],\n        \"latitude\": ds_class[\"latitude\"],\n    },\n)\n\nplot = ds_class[\"RF-forest\"].plot.imshow(\n    cmap=cmap_green, cbar_kwargs={\"ticks\": [0.25, 0.75]}\n)\ncbar = plot.colorbar\ncbar.set_ticklabels([\"non-forest\", \"forest\"])\nplot.axes.set_title(\"Random Forest Classification\")\nplt.show()\n\n# Print the Classification report\nprint(\"RANDOM FOREST: \\n \" + classification_report(y_test, rf_predict))\n\n# Print the confusion matrix\ncon_mat_rf = pd.DataFrame(\n    confusion_matrix(y_test, rf_predict),\n    index=[\"Actual Negative\", \"Actual Positive\"],\n    columns=[\"Predicted Negative\", \"Predicted Positive\"],\n)\ndisplay(con_mat_rf)","key":"gWhViiG0Wr"},{"type":"outputs","id":"-VY2DW-Xhgnw1WnPtx3N4","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 2 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"2cce409e8ccb1f6c8c2f4a31f7e47725","path":"/eo-datascience-cookbook/build/2cce409e8ccb1f6c8c2f4a31f7e47725.png"}}},"key":"DOPVaPRE4j"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"RANDOM FOREST: \n               precision    recall  f1-score   support\n\n         0.0       0.96      0.95      0.95      6626\n         1.0       0.94      0.95      0.95      5485\n\n    accuracy                           0.95     12111\n   macro avg       0.95      0.95      0.95     12111\nweighted avg       0.95      0.95      0.95     12111\n\n"},"key":"OcMVyPJqDw"},{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"                 Predicted Negative  Predicted Positive\nActual Negative                6301                 325\nActual Positive                 274                5211","content_type":"text/plain"},"text/html":{"content":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Predicted Negative</th>\n      <th>Predicted Positive</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Actual Negative</th>\n      <td>6301</td>\n      <td>325</td>\n    </tr>\n    <tr>\n      <th>Actual Positive</th>\n      <td>274</td>\n      <td>5211</td>\n    </tr>\n  </tbody>\n</table>\n</div>","content_type":"text/html"}}},"key":"W50t8b4wBX"}],"key":"sQxUDD53eR"}],"key":"WmYEgqbVmS"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We can already see from the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"y4CVFbrqPZ"},{"type":"inlineCode","value":"classification reports","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NdgDlWzX1J"},{"type":"text","value":" and the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GY1rjO3PhB"},{"type":"inlineCode","value":"confusion matrices","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Tf1p6JGZMR"},{"type":"text","value":" that the Random Forest classifier has outperformed the Naive Bayes classifier. This is particularly evident from the lower values in the secondary diagonal, indicating minimal False Positives and False Negatives. It appears that the Naive Bayes classifier is more sensitive to False Positives, resulting in a higher rate of incorrect classifications.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nASzjYE237"}],"key":"cRliyOHU9o"},{"type":"heading","depth":3,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Comparison of the Classificators","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"cAsD6QsNFE"}],"identifier":"comparison-of-the-classificators","label":"Comparison of the Classificators","html_id":"comparison-of-the-classificators","implicit":true,"key":"RzxWFrNSCJ"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"To gain a more in-depth understanding of the classifiers’ performance, we will compare their results. Specifically, we will identify the areas where both classifiers agree and the areas where they disagree. This comparison will provide valuable insights into the strengths and weaknesses of each classifier, allowing us to better assess their effectiveness in identifying forested and non-forested regions.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"cyBvO4uBXn"}],"key":"VINiaJHSmR"}],"key":"rT0oDfbkRE"},{"type":"block","kind":"notebook-code","data":{"code-fold":true},"children":[{"type":"code","lang":"python","executable":true,"value":"cmap_trio = colors.ListedColormap([\"whitesmoke\", \"indianred\", \"goldenrod\", \"darkgreen\"])\n\n\ndouble_clf = ds_class[\"NB-forest\"] + 2 * ds_class[\"RF-forest\"]\n\nfig, ax = plt.subplots()\ncax = ax.imshow(double_clf, cmap=cmap_trio, interpolation=\"none\")\n\n# Add a colorbar with custom tick labels\ncbar = fig.colorbar(cax, ticks=[1 * 0.375, 3 * 0.375, 5 * 0.375, 7 * 0.375])\ncbar.ax.set_yticklabels([\"None\", \"Naive Bayes\", \"Random Forest\", \"Both\"])\nax.set_title(\"Classification Comparisson\")\nax.set_axis_off()\nplt.show()","key":"tfo0Kkc8YO"},{"type":"outputs","id":"Il7jo0sgQJ00QN7u1Z6TI","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 2 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"42694b7570cceaa2d812331b1318c808","path":"/eo-datascience-cookbook/build/42694b7570cceaa2d812331b1318c808.png"}}},"key":"GRV5PgW2tc"}],"key":"o7Tht6um9i"}],"key":"nv68FBWepf"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The areas where both classifiers agree include the larger forested regions, such as the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"m8nLttOLdK"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Nationalpark Donau-Auen","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ke0XwnLbRY"}],"key":"lZGQfMFP2h"},{"type":"text","value":" and the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x9Hyt9M4OQ"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Leithagebirge","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"C2kDuZaX6Q"}],"key":"SHFOkWpPJF"},{"type":"text","value":". Additionally, both classifiers accurately identified the urban areas of Vienna and correctly excluded them from being classified as forested.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eEcN4Cxjl7"}],"key":"Kst53uzAWv"}],"key":"vd4Tbw6TpA"},{"type":"block","kind":"notebook-code","data":{"code-fold":true},"children":[{"type":"code","lang":"python","executable":true,"value":"# Plot only one class, either None (0), Naive Bayes (1), Random Forest (2), or Both (3)\nfig, axs = plt.subplots(2, 2, figsize=(8, 8))\nax = axs.ravel()\n\nfor i in range(4):\n    ax[i].imshow(double_clf == i, cmap=\"cmc.oleron_r\", interpolation=\"none\")\n    category = [\n        \"by None\",\n        \"only by Naive Bayes\",\n        \"only by Random Forest\",\n        \"by Both\",\n    ][i]\n    title = \"Areas classified \" + category\n    ax[i].set_title(title)\n    ax[i].set_axis_off()\n\nplt.tight_layout()","key":"oRGtAnUYLa"},{"type":"outputs","id":"RMpqzLmUjN5mTaksGRR_Y","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 800x800 with 4 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"f7f0fb585a0663051ca101a2a5a0da48","path":"/eo-datascience-cookbook/build/f7f0fb585a0663051ca101a2a5a0da48.png"}}},"key":"CUxGr9k089"}],"key":"vrzKtPG9lX"}],"key":"MVrLeB4aVM"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"When plotting the classified areas individually, we observe that the Random Forest classifier mistakenly identified the Danube River as a forested area. Conversely, the Naive Bayes classifier erroneously classified a significant amount of cropland as forest.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TjUCjG5ySs"}],"key":"cbTnVIJoCX"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Finally, by analyzing the proportion of forested areas within the scene, we find that approximately 18% of the area is classified as forest, while around 66% is classified as non-forest. The remaining areas, which include water bodies and cropland, fall into less clearly defined categories.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"OSKGGIEijK"}],"key":"ZEEn7iHj8j"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"The accompanying bar chart illustrates the distribution of these classifications, highlighting the percentage of forested areas, non-forested areas, and regions classified by only one of the two classifiers. This visual representation helps to quantify the areas of agreement and disagreement between the classifiers, providing a clearer picture of their performance.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"GUB9rhWuK8"}],"key":"qXapGFdrsE"}],"key":"SupujlxNyC"},{"type":"block","kind":"notebook-code","data":{"code-fold":true},"children":[{"type":"code","lang":"python","executable":true,"value":"counts = {}\nfor num in range(0, 4):\n    num_2_class = {0: \"None\", 1: \"Naive Bayes\", 2: \"Random Forest\", 3: \"Both\"}\n    counts[num_2_class[num]] = int((double_clf == num).sum().values)\n\nclass_counts_df = pd.DataFrame(list(counts.items()), columns=[\"Class\", \"Count\"])\nclass_counts_df[\"Percentage\"] = (\n    class_counts_df[\"Count\"] / class_counts_df[\"Count\"].sum()\n) * 100\nax = class_counts_df.plot.bar(\n    x=\"Class\",\n    y=\"Percentage\",\n    rot=0,\n    color=\"darkgreen\",\n    ylim=(0, 100),\n    title=\"Classified Areas per Classificator (%)\",\n)\n\n# Annotate the bars with the percentage values\nfor p in ax.patches:\n    ax.annotate(\n        f\"{p.get_height():.1f}%\",\n        (p.get_x() + p.get_width() / 2.0, p.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        xytext=(0, 9),\n        textcoords=\"offset points\",\n    )","key":"XJUyNFT5F8"},{"type":"outputs","id":"bcgzuOoTIOKQvmL_VqHqx","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"d2058a70ace4c3bd274dbfd05b8bb94b","path":"/eo-datascience-cookbook/build/d2058a70ace4c3bd274dbfd05b8bb94b.png"}}},"key":"mROtYcRfB8"}],"key":"BmdVWhAPb7"}],"key":"prCCIE0F9I"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Conclusion","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PVnRCPPwmm"}],"identifier":"conclusion","label":"Conclusion","html_id":"conclusion","implicit":true,"key":"YOfpQUfhQP"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"In this chapter, we utilized machine learning to classify satellite imagery into forested and non-forested areas, comparing Naive Bayes and Random Forest classifiers. The Random Forest classifier generally outperformed Naive Bayes, with fewer errors in classification, although it misclassified the Danube River as forested, while Naive Bayes incorrectly identified cropland as forest. The analysis, supported by the bar chart, revealed that about 18% of the scene was classified as forest, 66% as non-forest, and the remainder included ambiguous categories. This comparison highlights the strengths and limitations of each classifier, underscoring the need for careful selection and evaluation of classification methods.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"I0xFjBWqTZ"}],"key":"GjtTa44skO"}],"key":"EpLLIDH5gZ"}],"key":"BOfTbKFApT"},"references":{"cite":{"order":["nasa2020","rouse1974monitoring"],"data":{"nasa2020":{"label":"nasa2020","enumerator":"1","html":"NASA. (2020). <i>Earth Observatory</i>. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://earthobservatory.nasa.gov/features/MeasuringVegetation/measuring_vegetation_2.php\">https://earthobservatory.nasa.gov/features/MeasuringVegetation/measuring_vegetation_2.php</a>","url":"https://earthobservatory.nasa.gov/features/MeasuringVegetation/measuring_vegetation_2.php"},"rouse1974monitoring":{"label":"rouse1974monitoring","enumerator":"2","html":"Rouse, J. W., Haas, R. H., Schell, J. A., Deering, D. W., & others. (1974). Monitoring vegetation systems in the Great Plains with ERTS. <i>NASA Spec. Publ</i>, <i>351</i>(1), 309."}}}},"footer":{"navigation":{"prev":{"title":"Templates","url":"/notebooks/templates/prereqs-templates","group":"Templates"},"next":{"title":"Tutorials","url":"/notebooks/tutorials/prereqs-tutorials","group":"Tutorials"}}},"domain":"http://localhost:3000"}