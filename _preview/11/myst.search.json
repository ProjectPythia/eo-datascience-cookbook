{"version":"1","records":[{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook"},"type":"lvl1","url":"/#earth-observation-data-science-cookbook","position":2},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook"},"content":"\n\n\n\n\n\n\n\nThis Project Pythia Cookbook covers a range of Earth observation examples employing\nthe Pangeo philosophy. The examples represent the main research lines and BSc/MSc\ncourses at the Department of Geodesy and Geoinformation at the TU Wien (Austria).\nThe department has strong ties with the EODC (Earth Observation Data Centre For\nWater Resources Monitoring), which hosts e.g., analysis-ready Sentinel-1\n(imaging radar mission) data, and has the computational resources to process\nlarge data volumes.","type":"content","url":"/#earth-observation-data-science-cookbook","position":3},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":4},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl2":"Motivation"},"content":"The motivation behind this book is to provide examples of Pangeo-based workflows\napplied to realistic examples in Earth observation data science. Creating an\neffective learning environment for Earth observation students is a challenging\ntask due to the rapidly growing volume of remotely sensed, climate, and other\nEarth observation data, along with the evolving demands from the tech industry.\nTodayâ€™s Earth observation students are increasingly becoming a blend of traditional\nEarth system scientists and â€œbig data scientistsâ€, with expertise spanning computer\narchitectures, programming paradigms, statistics, and machine learning for\npredictive modeling. As a result, it is essential to equip educators with the\nproper tools for instruction, including training materials, access to data, and\nthe necessary skills to support scalable and reproducible research.","type":"content","url":"/#motivation","position":5},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":6},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl2":"Authors"},"content":"Wolfgang Wagner, \n\nMartin Schobben,\n\n\nNikolas Pikall, \n\nJoseph Wagner, \n\nDavide Festa,\n\n\nFelix David ReuÃŸ, \n\nLuka Jovic","type":"content","url":"/#authors","position":7},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":8},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":9},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":10},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl2":"Structure"},"content":"This book comprises examples of data science concerning Earth Observation (EO) data,\nincluding course material on remote sensing and data products produced by the TU\nWien. It also serves to showcase the data and services offered by the EODC, including\na \n\nSTAC catalogue and a\n\n\nDask Gateway for distributed data processing.","type":"content","url":"/#structure","position":11},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl3":"Courses","lvl2":"Structure"},"type":"lvl3","url":"/#courses","position":12},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl3":"Courses","lvl2":"Structure"},"content":"This section offers an overview of notebooks, which are used in courses from\nthe Department of Geodesy and Geoinformation at TU Wien.","type":"content","url":"/#courses","position":13},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl3":"Templates","lvl2":"Structure"},"type":"lvl3","url":"/#templates","position":14},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl3":"Templates","lvl2":"Structure"},"content":"This section provides a collection of general examples of earth observation\nrelated tasks and workflows, which are not directly related to a specific course\nor product.","type":"content","url":"/#templates","position":15},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl3":"Tutorials","lvl2":"Structure"},"type":"lvl3","url":"/#tutorials","position":16},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl3":"Tutorials","lvl2":"Structure"},"content":"In this section you will find a collection of lessons, which explain certain\nproducts or methods that have been developed at the Department of Geodesy and\nGeoinformation at TU Wien.","type":"content","url":"/#tutorials","position":17},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":18},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder\nor on your local machine.","type":"content","url":"/#running-the-notebooks","position":19},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":20},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\nâ€œlaunch Binderâ€. After a moment you should be presented with a\nnotebook that you can interact with. I.e. youâ€™ll be able to execute\nand even change the example programs. Youâ€™ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.","type":"content","url":"/#running-on-binder","position":21},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":22},{"hierarchy":{"lvl1":"Earth Observation Data Science Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will\nneed to follow this workflow:\n\nClone the https://github.com/ProjectPythia/eo-datascience-cookbook repository: git clone https://github.com/TUW-GEO/eo-datascience-cookbook\n\nMove into the eo-datascience-cookbook directorycd eo-datascience-cookbook\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate eo-datascience-cookbook-dev\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":23},{"hierarchy":{"lvl1":"Microwave Remote Sensing"},"type":"lvl1","url":"/notebooks/courses/microwave-remote-sensing","position":0},{"hierarchy":{"lvl1":"Microwave Remote Sensing"},"content":"This course at the TU Wien teaches students to read, visualize and analyze\nSynthetic Aperture Radar (SAR) data. This will aid interpretation of SAR data\nbased upon a physical understanding of sensing principles and the interaction of\nmicrowaves with natural objects.\n\nConcepts\n\nImportance\n\nNotes\n\nIntro to xarray\n\nNecessary\n\n\n\nDask Arrays\n\nNecessary\n\n\n\nIntake\n\nHelpful\n\n\n\nMatplotlib\n\nHelpful\n\nPloting in Python\n\nDocumentation hvPlot\n\nHelpful\n\nInteractive plotting\n\nDocumentation odc-stac\n\nHelpful\n\nData access\n\nTime to learn: 90 min\n\nNote\n\nThese notebooks contain interactive elements. The full interactive elements can\nonly be viewed on Binder by clicking on the Binder badge or ðŸš€ button.","type":"content","url":"/notebooks/courses/microwave-remote-sensing","position":1},{"hierarchy":{"lvl1":"Discover and Read SAR Data"},"type":"lvl1","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise","position":0},{"hierarchy":{"lvl1":"Discover and Read SAR Data"},"content":"This notebook demonstrates how to access radar data in a SpatioTemporal Asset Catalog (STAC) Catalogue using the pystac library. In this example, we use Sentinel-1 data from the EODC (earth observation data and high performance computing service provider based in Vienna) STAC catalog. In the further process, we will learn how to query a STAC catalog, select specific items, and display the metadata and the actual image.\n\nimport folium\nimport pystac_client\nfrom odc import stac as odc_stac\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise","position":1},{"hierarchy":{"lvl1":"Discover and Read SAR Data","lvl2":"Data Discovery"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise#data-discovery","position":2},{"hierarchy":{"lvl1":"Discover and Read SAR Data","lvl2":"Data Discovery"},"content":"\n\neodc_catalog = pystac_client.Client.open(\"https://stac.eodc.eu/api/v1\")\n\neodc_catalog\n\nThe URL https://stac.eodc.eu/api/v1, served over Hypertext Transfer Protocol (HTTP), is a STAC-compliant API endpoint (specific URL address where an API service is available) that leads to the EODC Catalogue. Besides EODCâ€™s, other catalogues can be found on \n\nSTAC Index, such as United States Geological Survey (USGS) Landsat imagery, Sentinel Hub, Copernicus Data Space Ecosystem, and so on. Briefly spoken, STAC can be used to search, discover, and access metadata of these datasets with the same code. The EODC Catalogue can be accessed on the web via this \n\nlink as well.\n\nEach STAC catalog, composed by different providers, has many collections. To get all collections of a catalog, we can print all of them and their ids, which are used to fetch them from the catalog.\n\ncollections = eodc_catalog.get_collections()\n\n# length of string of collection.id, for pretty print\nmax_length = max(len(collection.id) for collection in collections)\n\nfor collection in eodc_catalog.get_collections():\n    print(f\"{collection.id.ljust(max_length)}: {collection.title}\")\n\nTo get a specific collection from the catalog, we can use the client.get_collection() method and provide the collection name. We can then display its description, id, temporal and spatial extent, license, etc. In this notebook, we will work with the Sentinel-1 sigma naught 20m collection.\n\ncolllection_id = \"SENTINEL1_SIG0_20M\"\n\ncollection = eodc_catalog.get_collection(colllection_id)\ncollection\n\nEach collection has multiple items. An item is one spatio-temporal instance in the collection, for instance a satellite image. If items are needed for a specific timeframe or for a specific region of interest, we can define this as a query.\n\ntime_range = \"2022-10-01/2022-10-07\"  # a closed range\n# time_range = \"2022-01\"  # whole month, same can be done for a year and a day\n# time_range = \"2022-01-01/..\"  # up to the current date, an open range\n# time_range = \"2022-01-01T05:34:46\"  # a specific time instance\n\nA spatial region of interest can be defined in different ways. One option is to define a simple bounding box:\n\nlatmin, latmax = 46.3, 49.3  # South to North\nlonmin, lonmax = 13.8, 17.8  # West to East\n\nbounding_box = [lonmin, latmin, lonmax, latmax]\n\nIf the region of interest is not rectangular, we can also define a polygon:\n\n# GEOJSON can be created on geojson.io\n\n# This specific area of interest is a rectangle, but since it is\n# a closed polygon it seems like it has five nodes\n\narea_of_interest = {\n    \"coordinates\": [\n        [\n            [17.710928010825853, 49.257630084442496],\n            [13.881798300915221, 49.257630084442496],\n            [13.881798300915221, 46.34747715326259],\n            [17.710928010825853, 46.34747715326259],\n            [17.710928010825853, 49.257630084442496],\n        ]\n    ],\n    \"type\": \"Polygon\",\n}\n\nUsing our previously loaded STAC catalog, we can now search for items fullfilling our query. In this example we are using the bounding box. If we want to use an area of interest specified in the geojson format - one has to use the intersects parameter as documented in the comment below.\n\nsearch = eodc_catalog.search(\n    collections=colllection_id,  # can also be a list of several collections\n    bbox=bounding_box,  # search by bounding box\n    # intersects=area_of_interest,  # GeoJSON search\n    datetime=time_range,\n    # max_items=1  # number of max items to load\n)\n\n# If we comment everything besides colllection_id, we will load whole\n# collection for available region and time_range\n\nitems_eodc = search.item_collection()\nprint(f\"On EODC we found {len(items_eodc)} items for the given search query\")\n\nNow, we can fetch a single item, in this case a Sentinel-1 image, from the query results. A good practice is to always check what metadata the data provider has stored on the item level. This can be done by looking into the item properties.\n\nitem = items_eodc[0]\nitem.properties\n\nFor now, letâ€™s display only the vertical-vertical (VV) polarized band of the item and some information about the data.\n\nitem.assets[\"VV\"].extra_fields.get(\"raster:bands\")[0]\n\nIn the EODC STAC catalogue an item can conveniently be displayed using its thumbnail.\n\nitem.assets[\"thumbnail\"].href\n\nNow we will plot the data on a map using the thumbnail and the python package \n\nfolium. This is an easy way to quickly check how the data found by a search query looks on a map.\n\nmap = folium.Map(\n    location=[(latmin + latmax) / 2, (lonmin + lonmax) / 2],\n    zoom_start=7,\n    zoom_control=False,\n    scrollWheelZoom=False,\n    dragging=False,\n)\n\nfolium.GeoJson(area_of_interest, name=\"Area of Interest\").add_to(map)\n\nfor item in items_eodc:\n    # url leading to display of an item, can also be used as hyperlink\n    image_url = item.assets[\"thumbnail\"].href\n    bounds = item.bbox\n    folium.raster_layers.ImageOverlay(\n        image=image_url,\n        bounds=[[bounds[1], bounds[0]], [bounds[3], bounds[2]]],\n    ).add_to(map)\n\nfolium.LayerControl().add_to(map)\n\nmap\n\nFigure 1: Map of study area. Blue rectangle is the area covered by the discovered data.","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise#data-discovery","position":3},{"hierarchy":{"lvl1":"Discover and Read SAR Data","lvl2":"Data Reading"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise#data-reading","position":4},{"hierarchy":{"lvl1":"Discover and Read SAR Data","lvl2":"Data Reading"},"content":"STAC can also be a useful tool for the discovery of data, however it only loads metadata. This saves memory, but if one would like to do further analysis, the data has to be loaded into memory or downloaded on disk.\n\nIn the following, we will demonstrate this with the library odc-stac. Here we can define what data will loaded as bands; in this case VV sigma naught. Moreover we can resample the data by providing any coordinate reference system (CRS) and resolution as well as a method for resampling of continuos data (e.g. bilinear resampling). In the example below we use the EQUI7 Grid of Europe and a 20 meter sampling. This is the native format of sigma naught stored at EODC, so there will be no actual resampling. Note, also, that resampling is not advisable for this data, as it is provided on a logarithmic scale. More about this in the notebook â€œBackscattering Coefficientsâ€.\n\nThe chunks argument is an advancement method for performing parallel computations on the data. We will not cover this in further detail.\n\nbands = \"VV\"  # Vertical-vertical polarized\ncrs = \"EPSG:27704\"  # Coordinate Reference System: EQUI7 Grid of Europe\nres = 20  # 20 meter\nchunks = {\"time\": 1, \"latitude\": 1000, \"longitude\": 1000}\nsig0_dc = odc_stac.load(\n    items_eodc,\n    bands=bands,\n    crs=crs,\n    resolution=res,\n    bbox=bounding_box,\n    chunks=chunks,\n    resampling=\"bilinear\",\n)\n\nLetâ€™s have a look at the VV polarized band of the dataset.\n\nsig0_dc.VV\n\nAs we can see, the data is stored as a xarray DataArray. Xarray is a convenient package for multidimensional labeled arrays, like temperature, humidity, pressure, different bands of satellite imagery, and so on. \n\nThe link provides detailed documentation. In a later notebook we will explore some more of the functionality of xarray. As we can see in the coordinates, the data here consists of 21 time steps.\n\nIn general, data from STAC is â€œlazilyâ€ loaded, which means that the structure of the DataArray is constructed, but the data is not loaded yet. It is loaded only at instance when it is needed, for example, for plotting, computations, and so on.\n\nSince the DataArray has currently a size of almost 18 GiB, we will subset it to the region of Vienna.\n\n# Create a bounding box covering the region of Vienna\nlatmin_smaller, latmax_smaller = 48, 48.4\nlonmin_smaller, lonmax_smaller = 16, 16.5\n\nsmaller_bounding_box = [\n    [latmin_smaller, lonmin_smaller],\n    [latmax_smaller, lonmax_smaller],\n]\n\nmap = folium.Map(\n    location=[\n        (latmin_smaller + latmax_smaller) / 2,\n        (lonmin_smaller + lonmax_smaller) / 2,\n    ],\n    zoom_start=8,\n    zoom_control=False,\n    scrollWheelZoom=False,\n    dragging=False,\n)\n\nfolium.GeoJson(area_of_interest, name=\"Area of Interest\").add_to(map)\n\nfolium.Rectangle(\n    bounds=smaller_bounding_box,\n    color=\"red\",\n).add_to(map)\n\nfor item in items_eodc:\n    image_url = item.assets[\"thumbnail\"].href\n    bounds = item.bbox\n    folium.raster_layers.ImageOverlay(\n        image=image_url,\n        bounds=[[bounds[1], bounds[0]], [bounds[3], bounds[2]]],\n    ).add_to(map)\n\nfolium.LayerControl().add_to(map)\n\nmap\n\nFigure 2: Map of study area. Blue rectangle is the area covered by the discovered data. Red rectangle covers the selected data.\n\nCreate a new dataset with the smaller bounding box covering the region of Vienna. We will leave out the arguments for resampling and directly use the native format as defined in the metadata.\n\nsig0_dc = odc_stac.load(\n    items_eodc,\n    bands=bands,\n    bbox=[lonmin_smaller, latmin_smaller, lonmax_smaller, latmax_smaller],\n    chunks=chunks,\n)\n\nDue to the way the data is acquired and stored, some items include â€œno dataâ€ areas. In our case, no data has the value -9999, but this can vary from data provider to data provider. This information can usually be found in the metadata. Furthermore, to save memory, data is often stored as integer (e.g. 25) and not in float (e.g. 2.5) format. For this reason, the backscatter values are often multiplied by a scale factor, in this case factor 10.\n\nAs Sentinel-1 satellites overpasses Austria every few days, only some time steps of the dataset will have physical data. As a final step, we will now decode the data and create a plot of two consecutive Sentinel-1 acquisitions of Vienna.\n\n# Retrieve the scale factor and NoData value from the metadata. raster:bands is\n# a STAC raster extension\nscale = item.assets[\"VV\"].extra_fields.get(\"raster:bands\")[0][\"scale\"]\nnodata = item.assets[\"VV\"].extra_fields.get(\"raster:bands\")[0][\"nodata\"]\n\n# Decode data with the NoData value and the scale factor\nsig0_dc = sig0_dc.where(sig0_dc != nodata) / scale\n\n# We should remove unnecessary dates when there was no data\n# (no satellite overpass)\nsig0_dc = sig0_dc.dropna(dim=\"time\")\n\nsig0_dc.VV.plot(col=\"time\", robust=True, cmap=\"Greys_r\", aspect=1, size=10)\n\nFigure 3: Sentinel-1 microwave backscatter image for two timeslices.","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise#data-reading","position":5},{"hierarchy":{"lvl1":"Unit Conversion"},"type":"lvl1","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-1","position":0},{"hierarchy":{"lvl1":"Unit Conversion"},"content":"In this notebook, we are going to have a look at the conversion of units. Sentinel-1 data, and most other SAR data, is usually provided in decibels (dB). In this notebook, we will discover the advantages of displaying SAR data in decibels and why we need to convert the data to a linear scale in order to make meaningful calculations. Letâ€™s start with importing some libraries.\\text{logarithmic} \\longleftrightarrow \\text{linear}[\\text{dB}] \\longleftrightarrow [\\text{m}^2 \\cdot \\text{m}^{-2}]\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport odc.stac\nimport pystac_client\nimport rioxarray  # noqa: F401\nimport xarray as xr\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-1","position":1},{"hierarchy":{"lvl1":"Unit Conversion","lvl2":"Exploring the Data"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-1#exploring-the-data","position":2},{"hierarchy":{"lvl1":"Unit Conversion","lvl2":"Exploring the Data"},"content":"Letâ€™s start by loading some sample data, in order to demonstrate why this conversion is important.\nHere we will have a look at some SAR data from the Sentinel-1. The data is provided in decibels (dB).\nIn the following example, we will:\n\nload data from Sentinel-1\n\nvisualize the data in logarithmic scale\n\ncompare the data with linear scale","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-1#exploring-the-data","position":3},{"hierarchy":{"lvl1":"Unit Conversion","lvl2":"Search for some Data"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-1#search-for-some-data","position":4},{"hierarchy":{"lvl1":"Unit Conversion","lvl2":"Search for some Data"},"content":"Now, we start by loading data from Sentinel-1 from the EODC STAC Catalogue. We do this in the same way as in the previous notebook â€œDiscover and Read SAR Dataâ€.\n\nlatmin, latmax = 48, 48.5\nlonmin, lonmax = 16, 17\nbounds = (lonmin, latmin, lonmax, latmax)\n\ntime_range = \"2022-07-01/2022-07-31\"\n\nitems = (\n    pystac_client.Client.open(\"https://stac.eodc.eu/api/v1\")\n    .search(\n        bbox=bounds,\n        collections=[\"SENTINEL1_SIG0_20M\"],\n        datetime=time_range,\n        limit=100,\n    )\n    .item_collection()\n)\n\nprint(len(items), \"scenes found\")\n\nbands = \"VV\"\ncrs = \"EPSG:27704\"  # Coordinate Reference System: EQUI7 Grid of Europe\nres = 20  # 20 meter\n\nsig0_dc = odc.stac.stac_load(\n    items,\n    bands=bands,\n    bbox=bounds,\n    chunks={\"time\": 5, \"x\": 600, \"y\": 600},\n)\n\nnodata = items[0].assets[\"VV\"].extra_fields[\"raster:bands\"][0][\"nodata\"]\nscale = items[0].assets[\"VV\"].extra_fields[\"raster:bands\"][0][\"scale\"]\n\nsig0_dc = (sig0_dc.where(sig0_dc != nodata) / scale).VV\nsig0_dc\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-1#search-for-some-data","position":5},{"hierarchy":{"lvl1":"Unit Conversion","lvl2":"Comparison of the Data in dB and Linear Scale"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-1#comparison-of-the-data-in-db-and-linear-scale","position":6},{"hierarchy":{"lvl1":"Unit Conversion","lvl2":"Comparison of the Data in dB and Linear Scale"},"content":"In the next two cells we will select a subset of the data. This is done to reduce the amount of data we are working with. The precise workflow is not important for now, since the theory is explained after the cells. They are just here to show the data we are working with.\n\nsubset = sig0_dc.sel(time=slice(\"2022-07-01\", \"2022-07-07\"))\nsubset = subset.dropna(\"time\", how=\"all\")\n\nNow plot the data.\n\naoi = subset.isel(time=0, x=slice(0, 500), y=slice(0, 500))\naoi_lin = 10 ** (aoi / 10)\n\nfig, ax = plt.subplots(2, 3, figsize=(14, 8))\n# upper left\nax_ul = ax[0, 0]\naoi.plot.imshow(robust=True, ax=ax_ul, cmap=\"Greys_r\")\nax_ul.set_title(r\"$\\sigma^0$ [$dB$] (robust plot)\")\nax_ul.axes.set_aspect(\"equal\")\n\n# upper middle\nax_um = ax[0, 1]\naoi.plot.imshow(robust=False, ax=ax_um, cmap=\"Greys_r\")\nax_um.set_title(r\"$\\sigma^0$ [$dB$] (not robust plot)\")\nax_um.axes.set_aspect(\"equal\")\n\n# upper right\nax_ur = ax[0, 2]\naoi.plot.hist(bins=50, ax=ax_ur, edgecolor=\"black\")\nax_ur.set_xlabel(r\"$\\sigma^0$ [$dB$]\")\nax_ur.set_title(r\"$\\sigma^0$ [$dB$] distribution\")\nax_ur.set_ylabel(\"n (number of pixels)\")\n\n# lower left\nax_ll = ax[1, 0]\naoi_lin.plot.imshow(robust=True, ax=ax_ll, cmap=\"Greys_r\")\nax_ll.set_title(r\"$\\sigma^0$ [$m^2 \\cdot m^{-2}$] (robust plot)\")\nax_ll.axes.set_aspect(\"equal\")\n\n# lower middle\nax_lm = ax[1, 1]\naoi_lin.plot.imshow(robust=False, ax=ax_lm, cmap=\"Greys_r\")\nax_lm.set_title(r\"$\\sigma^0$ [$m^2 \\cdot m^{-2}$] (not robust plot)\")\nax_lm.axes.set_aspect(\"equal\")\n\n# lower right\nax_lr = ax[1, 2]\naoi_lin.plot.hist(bins=50, ax=ax_lr, edgecolor=\"black\")\nax_lr.set_xlabel(r\"$\\sigma^0$ [$m^2 \\cdot m^{-2}$]\")\nax_lr.set_ylabel(\"n (number of pixels)\")\nax_lr.set_title(r\"$\\sigma^0$ [$m^2 \\cdot m^{-2}$] distribution\")\n\ntitle = (\n    r\"Sentinel-1 backscatter $\\sigma^0$ comparison\"\n    + r\" in linear and logarithmic domain\"\n)\nfig.suptitle(title, horizontalalignment=\"center\")\nplt.tight_layout()\n\nFigure 1: Visually comparing \\sigma^0 on a logarithmic and linear scale (left column). In addition, the benefit of using the robust plotting method is shown (middle column). The robust argument uses the 2^nd^ and 98^th^ percentiles of the data to compute the color limits to eliminate washing out the plot due to data outliers.\n\nIn the plot above you can see the difference between the two scales. The values in dB are more evenly distributed and are therefore easier to plot. The values in linear scale are more spread out and are therefore harder to interpret.\nThis is why we use the dB scale for plotting/visualization.\n\nWhile the logarithmic scale facilitates visual interpretation, it has implications for mathematical operations. In the following, weâ€™ll have a closer look at this. But first, letâ€™s see how we can convert between the linear and the logarithmic domains.","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-1#comparison-of-the-data-in-db-and-linear-scale","position":7},{"hierarchy":{"lvl1":"Unit Conversion","lvl2":"Conversion Formulas"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-1#conversion-formulas","position":8},{"hierarchy":{"lvl1":"Unit Conversion","lvl2":"Conversion Formulas"},"content":"The decibel (dB) is a logarithmic unit used to express the ratio of two values of a physical quantity, often power or intensity. In the case of SAR data, the backscatter coefficient is often expressed in dB to facilitate visualization.\n\nIn order to convert the data from dB to linear scale, we use the following formula.\nLet D be the original value (dB) and I the converted value (m^2m^{-2}). The conversion of units can be expressed as:D =  10  \\cdot \\log_{10} (I) = 10 \\cdot \\log_{10} (e) \\cdot \\ln (I)\\longrightarrow [dB]\n\nSimilarly, the conversion back to the original unit can be expressed as:I = e^{\\frac{D}{10\\cdot \\log_{10}(e)}} = 10^{\\frac{D}{10}} \\longrightarrow [m^2m^{-2}]\n\nYou can find these formulas in the script for Microwave Remote Sensing on page 136 (equation 6.40).\n\nNow letâ€™s implement the conversion in Python.\n\ndef lin2db(val: float | int) -> float:\n    \"\"\"\n    Converts value from linear to dB units.\n\n    :param val: Value in linear units.\n    :type val: float|int\n    :return: Value in dB.\n    :rtype: float\n    \"\"\"\n    return 10 * np.log10(val)\n\n\ndef db2lin(val: float | int) -> float:\n    \"\"\"\n    Converts value from dB to linear units.\n\n    :param val: Value in dB.\n    :type val: float|int\n    :return: Value in linear units.\n    :rtype: float\n    \"\"\"\n    return 10 ** (val / 10)\n\nWhen performing mathematical operations with SAR data it is important to be aware, that adding values in the logarithmic scale doesnâ€™t work in the same way as adding regular (linear) values. This is because in the logarithmic scale, each unit step represents an equal multiplication. This means that an addition of two values in the logarithmic scale equals a multiplication of the values in the linear scale. Vice versa, a subtraction in a logarithmic scale equals a division in a linear scale. Letâ€™s have a look at an example, where we add two values, once without the conversion to linear scale and once with the conversion to linear scale.\n\n# Logarithmic addition\n# Values in linear and decibel units\nval1_db, val2_db = 10, 12\n\n# Logarithmic addition\nsum_db = val1_db + val2_db\nprint(\"Logarithmic Addition:\")\nprint(f\"Logarithmic values: \\t{val1_db: <5}, {val2_db: <5} [dB]\")\nprint(f\"Logarithmic sum: \\t{val1_db} + {val2_db} = {sum_db: <5} [dB]\")\n\n# Linear addition\nval1_lin, val2_lin = db2lin(val1_db), db2lin(val2_db)\nsum_lin = val1_lin + val2_lin\nprint(\"\\nLinear Addition:\")\nprint(\n    f\"\"\"Linear values: \\t\\t{val1_lin: <5}, {val2_lin: <5.2f} [lin]\n      \\t\\t\\t(converted from dB)\"\"\"\n)\nprint(f\"Linear sum: \\t\\t{val1_lin} + {val2_lin: .2f} = {sum_lin: .2f} [lin]\")\nprint(f\"\\t\\t\\t= {lin2db(sum_lin): .2f} [dB]\")\n\nAs you can see, the values in dB and in linear scale differ quite a bit. In the example above, the values differ by a factor of around 6 when looked at in linear scale.\n\nNow that we have some data, we will have a look at some practical examples where we will convert the data to linear scale.\nWhen we try to calculate the average \\sigma^0 value across the scene, we need to do this by converting the data to linear scale first and then calculating the average and converting it back to dB.","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-1#conversion-formulas","position":9},{"hierarchy":{"lvl1":"Unit Conversion","lvl2":"Creating a Monthly Mosaic"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-1#creating-a-monthly-mosaic","position":10},{"hierarchy":{"lvl1":"Unit Conversion","lvl2":"Creating a Monthly Mosaic"},"content":"So in the beginning we have lazily loaded data for an area across a whole year. We therefore have around 700 images. We will now essentially compress the data of each month into one timestamp. This is done by using the resampling method together with an operation method like mean that includes summation. Since the data is in dB we need to convert it to linear scale first, then we can resample the data and convert it back to dB.\n\n# Convert to linear scale and calculate monthly means\n# Conversion by calculating with the xarray Object\nsig0_lin = 10 ** (sig0_dc / 10)\n\n# Resample to monthly means. Time accepts intervals identical to the pandas\n# resample function. 'D' for days, 'W' for weeks, 'ME' for months.\nsig0_lin_monthly = sig0_lin.resample(time=\"1ME\").mean()\n\n# Convert back to dB scale\n# Conversion by applying a function\nsig0_monthly = lin2db(sig0_lin_monthly)\nsig0_monthly\n\nThe dataset has now only 12 timestamps, one for each month. Next, we want to calculate the average \\sigma^0 value across the scene for one month. We will do this again by converting the data to linear scale first and then calculating the average and converting it back to dB.\n\n# Lets take a data array with db values\ndb_array = sig0_monthly.sel(time=\"2022-07-30\", method=\"nearest\")\n\n# Compute the linear values\nlin_array = db2lin(db_array)\n\n# Compute the average backscatter value in linear units across the whole scene\nlin_mean = lin_array.mean()\nprint(f\"Average backscatter value in linear units: {lin_mean.values: .3f}\")\ndb_from_lin_mean = lin2db(lin_mean)\nprint(f\"That value in dB: {db_from_lin_mean.values: .3f}\\n\")\n\n# Compute the average backscatter value in dB across the whole scene\ndb_mean = db_array.mean()\nprint(f\"Average backscatter value in dB: {db_mean.values: .3f}\")\n\nAs you can see in the example, the mean values across the scene are different in dB and linear scale. Therefore, it is important to be aware in which scale the data is stored to perform the correct type of mathematical operation or always convert the data to linear scale before doing any calculations.","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-1#creating-a-monthly-mosaic","position":11},{"hierarchy":{"lvl1":"Unit Conversion","lvl2":"Save Mean Mosaic as Tif File"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-1#save-mean-mosaic-as-tif-file","position":12},{"hierarchy":{"lvl1":"Unit Conversion","lvl2":"Save Mean Mosaic as Tif File"},"content":"Often we want to store the output of a computation permanently on a file system. The most common file format for this is a GeoTIFF (TIF file with additional information on the georeference). The following cell indicates how this can be easily done with Xarray. When we want to store the data as a GeoTIFF we need to make sure to provide a spatial reference to geolocate the data. The best way to check whether the Xarray has a coordinate reference system (CRS) is by using the rioxarray rio.crs accessor. More about this in notebook â€œDatacubesâ€.\n\n# Select some data which we want to save\ndata_2_save = sig0_monthly.sel(time=\"2022-07-30\", method=\"nearest\")\ndata_2_save.rio.crs\n\nIn this case, the spatial reference is the EPSG Code EPSG:27704, which is the Equi7Grid Europe.\nAs the output data array already has a spatial reference we now save it as a raster file\n\n# Save the data\ndata_2_save.rio.to_raster(\n    \"sig0_mean_mosaic_july.tif\", tiled=True, driver=\"GTiff\", compress=\"LZW\"\n)\n\n# Load the data again (for demonstration purposes)\nloaded_data = xr.open_dataset(\"sig0_mean_mosaic_july.tif\", engine=\"rasterio\")\nloaded_data","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-1#save-mean-mosaic-as-tif-file","position":13},{"hierarchy":{"lvl1":"Backscattering Coefficients"},"type":"lvl1","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-2","position":0},{"hierarchy":{"lvl1":"Backscattering Coefficients"},"content":"In this notebook, we will introduce some of the steps involved in the processing of Sentinel-1 Level1 Ground Range Detected (GRD) data to \\sigma^0 (sig0) and \\gamma^0 (gmr). Moreover, the notebook illustrates the importance and impact of geometric and radiometric terrain correction. As the processing of SAR data is a very time and hardware-intense task, we wonâ€™t perform the actual processing in this notebook. Instead, data at different processing steps is illustrated to highlight the impact of the processing steps.\n\nimport hvplot.xarray  # noqa: F401\nimport intake\nimport matplotlib.pyplot as plt  # noqa: F401\nimport numpy as np\nimport rioxarray  # noqa: F401\nimport xarray as xr\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-2","position":1},{"hierarchy":{"lvl1":"Backscattering Coefficients","lvl3":"Loading Backscatter Data"},"type":"lvl3","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-2#loading-backscatter-data","position":2},{"hierarchy":{"lvl1":"Backscattering Coefficients","lvl3":"Loading Backscatter Data"},"content":"We first load our data from the following \n\nintake catalog. Intake is somewhat similar to STAC in that it makes it easy to discover and load data. More importantly, this package allows us to hide some of the complexities involved with getting the data in the right format, which are not of concern in this notebook.\n\nurl = \"https://huggingface.co/datasets/martinschobben/microwave-remote-sensing/resolve/main/microwave-remote-sensing.yml\"\ncat = intake.open_catalog(url)\ngtc_dc = cat.gtc.read().compute()\ngtc_dc\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-2#loading-backscatter-data","position":3},{"hierarchy":{"lvl1":"Backscattering Coefficients","lvl3":"Geometric Terrain Correction"},"type":"lvl3","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-2#geometric-terrain-correction","position":4},{"hierarchy":{"lvl1":"Backscattering Coefficients","lvl3":"Geometric Terrain Correction"},"content":"Level 1 SAR data, such as the â€œGRDâ€ product only takes into account the ellipsoidal model of the earth (i.e. slant-range distortion; script Chapter 4), without the description of the relief. This makes mountains appear to lean towards the radar system as visible in the below plot of the GRD scene. This latter distortion originates from the side-looking geometry of the SAR system. Due to this, the radar pulse reaches mountain slope facing the sensor before it reaches the base. Consequently, these slopes appear compressed and leaning toward the sensor. During processing to a level 1C product, these slant range distortions are partly corrected using a terrain correction algorithm and a Digital Elevation Model (DEM). The most common algorithm for this is the Range Doppler Terrain Correction.\n\nFigure 1: Geometric terrain correction. The lower bar shows the GRD without geometric terrain correction  in slant geometry. In areas where the ground is elevated, the time of the signal to travel to the earthâ€™s surface and back to the sensor is distorted, causing geometric shifts (foreshortening, lengthening, etc). Using a DEM and the Range Doppler Terrain Correction, the distortions are corrected and the image is orthorectified. (Source: ESRI)\n\nLetâ€™s visualize this geometric terrain correction (GTC) with some actual data using the xarray method hvplot of the gtc_dc object.\n\ngtc_dc.hvplot.image(\n    x=\"x\",\n    y=\"y\",\n    robust=True,\n    data_aspect=1,\n    cmap=\"Greys_r\",\n    groupby=\"band\",\n    rasterize=True,\n).opts(frame_height=600, framewise=False, aspect=\"equal\")\n\nFigure 2: The ground range detected values and geometrically terrain corrected values can be selected on the right-hand side of the graphic.\n\nThe geometrically terrain corrected values from the gtc_dc object (Figure 1) can be approximated to a certain extent, as we have sufficiently detailed information of topography in this area. This corrects for at least one typically occurring distortion in mountainous regions: â€œforeshorteningâ€.\n\nFigure 3: Side Looking radar distortions (script Chapter 4).\n\nForeshortening can be spotted by eye, as it often has a radiometric consequence, where unusually bright areas fringe mountain ridges; a phenomenon called â€œhighlightingâ€. This geometric artifact occurs due to the compression of the distance in the image of slopes facing the radar system and the consequentially higher density of scatterers per unit length. Now letâ€™s zoom in on an example from the same datacube and display the original and corrected values side-by-side.\n\nfor_dc = gtc_dc.sel(x=slice(9.651, 9.706), y=slice(47.134, 47.079)).band_data\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 8))\n\nbbox = dict(boxstyle=\"round\", fc=\"0.8\")\n\n\nax[1].annotate(\n    \"foreshortening/layover\",\n    xy=(9.674, 47.092),\n    xytext=(0.574, 0.192),\n    textcoords=\"subfigure fraction\",\n    bbox=bbox,\n    arrowprops=dict(facecolor=\"red\", shrink=0.05),\n)\nax[1].annotate(\n    \"radar shadows\",\n    xy=(9.68, 47.119),\n    xytext=(0.6, 0.625),\n    textcoords=\"subfigure fraction\",\n    bbox=bbox,\n    arrowprops=dict(facecolor=\"red\", shrink=0.05),\n)\n\nax[0].axes.set_aspect(\"equal\")\nax[1].axes.set_aspect(\"equal\")\n\nfor_dc.sel(band=\"grd\").plot(ax=ax[0], robust=True, cmap=\"Greys_r\")\nfor_dc.sel(band=\"sig0_gtc\").plot(ax=ax[1], robust=True, cmap=\"Greys_r\")\n\nFigure 4: Close-up inspection of geometric distortions in side-looking radar\n\nAs we can see, not all the geometric distortions can be corrected by the algorithm. Some of the pixels at the mountain ranges appear stretched, as in these areas not enough valid measurements are available. Moreover, we can see dark areas which are indicating radar shadows. These are image areas that could not be captured by the radar sensor and have values close to the noise floor of the Sensor (minimum detectable signal strength) ~ -28dB. It is important to note, that radar shadows are not the same for every image, as they depend on the acquisition geometry, in particular, the incidence angle and the flight direction of the satellite.","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-2#geometric-terrain-correction","position":5},{"hierarchy":{"lvl1":"Backscattering Coefficients","lvl3":"Backscattering Coefficients"},"type":"lvl3","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-2#backscattering-coefficients","position":6},{"hierarchy":{"lvl1":"Backscattering Coefficients","lvl3":"Backscattering Coefficients"},"content":"In this chapter, we will look at some of the different backscatter coefficients in more detail (\\sigma^0_E or \\gamma^0_E), where both coefficients are geometrically terrain corrected. The difference is the plane of the reference area, which is the ground area as a tangent on an ellipsoidal Earth model for \\sigma^0_E and perpendicular to the line of sight for \\gamma^0_E (Figure 5). For this, we load a new datacube which includes \\sigma^0_E and the Incidence Angle for each pixel. We visualize the cube with the same method as before.\n\ncoef_dc = cat.coef.read().compute()\ncoef_dc.hvplot.image(\n    x=\"x\",\n    y=\"y\",\n    robust=True,\n    data_aspect=1,\n    cmap=\"Greys_r\",\n    groupby=\"band\",\n    rasterize=True,\n).opts(frame_height=600, framewise=False, aspect=\"equal\")\n\nFigure 5: The \\sigma^0_E and the incidence angle can be selected on the right-hand side of the graphic.\n\nIn Figure 5 we can see the incidence angle image of our scene. We can see, that it depicts the differences between near to far range, but not the actual terrain as it refers to the ellipsoid. The slight patterns of the terrain that are visible are originating from the geometric terrain correction. We will use this information now to convert our (\\sigma^0_E to \\gamma^0_E) with the following equation (equation 6.20 in the script):\\gamma^0_E = \\sigma^0_E / \\cos(\\theta_i)\n\nWe can perform this transformation with basic numpy operations on the xarray datacube.\n\n# linear scale\nsig0_db = coef_dc.sel(band=\"sig0_gtc\") / 10\nsig0_lin = 10 ** (coef_dc.sel(band=\"sig0_gtc\") / 10)\n# conversion to gamma\ngam0_lin = sig0_lin / np.cos(np.radians(coef_dc.sel(band=\"incidence_angle\")))\n# dB scale\ngam0_db = 10 * np.log(gam0_lin)\n# add to existing cube\ncoef_dc = xr.concat(\n    [coef_dc.sel(band=\"sig0_gtc\"), gam0_db.expand_dims(band=[\"gam0_gtc\"])], dim=\"band\"\n)\n\ncoef_dc.hvplot.image(\n    x=\"x\",\n    y=\"y\",\n    robust=False,\n    data_aspect=1,\n    cmap=\"Greys_r\",\n    groupby=\"band\",\n    rasterize=True,\n).opts(frame_height=600, framewise=False, aspect=\"equal\")\n\nFigure 6: \\sigma^0_E, and \\gamma^0_E can be selected on the right-hand side of the graphic.\n\nComparing \\sigma^0_E and \\gamma^0_E in the figure, we can see that both look identical except for the range. This is because the only difference between \\sigma^0_E and \\gamma^0_E is the change of the reference area. While \\sigma^0_E is defined to be ground range, \\gamma^0_E is defined to be in the plane perpendicular to the line of sight from the sensor. This way, \\gamma^0_E mitigates the impact of the incidence angle. However, \\gamma^0_E is still based on the ellipsoid and does not account for the impact of the terrain on the radiometry.","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-2#backscattering-coefficients","position":7},{"hierarchy":{"lvl1":"Backscattering Coefficients","lvl2":"Radiometric Terrain Correction"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-2#radiometric-terrain-correction","position":8},{"hierarchy":{"lvl1":"Backscattering Coefficients","lvl2":"Radiometric Terrain Correction"},"content":"So far, we corrected geometric distortions and compared the impact of the choice of the reference area. However, we still havenâ€™t corrected the backscatter intensity of pixels which are distorted by the terrain. In this last step, we will show that we can also correct radiometric artifacts to a certain degree. For this, we will load radiometrically terrain corrected (rtc) \\gamma^0_T and plot it along the other coefficients.\n\nrtc_dc = cat.rtc.read().compute()\n\n# add to existing cube\nrtc_dc = xr.concat([coef_dc, rtc_dc], dim=\"band\")\n\nrtc_dc.hvplot.image(\n    x=\"x\",\n    y=\"y\",\n    robust=True,\n    data_aspect=1,\n    cmap=\"Greys_r\",\n    groupby=\"band\",\n    rasterize=True,\n).opts(frame_height=600, framewise=False, aspect=\"equal\")\n\nFigure 7: \\sigma^0_E, \\gamma^0_E, and \\gamma^0_T can be selected on the right-hand side of the graphic.\n\nWhen comparing \\gamma^0_E and \\gamma^0_T in the plot we can clearly see the impact of the radiometric correction in the mountainous areas. This correction is necessary, because for slopes facing towards the sensor, a larger ground area contributes to the backscatter value of a slant range resolution cell, than for slopes lying in the opposite direction. This results in significant brightness changes, where foreshortening areas appear brighter and lengthening areas darker. \\gamma^0_T adjusts the backscatter to represent what it would be if the terrain was flat, thus reducing these effects. This significantly reduces the impact of the terrain on the backscatter values, allowing for more accurate comparisons across different terrain types and locations. The correction is done by using a DEM to determine the local illuminated area at each radar position. The above illustrated approach is also referred to as terrain flattening because in the resulting image, mountains appear flat. As \\gamma^0_T is corrected for geometric and radiometric distortions, it is also referred to as Normalized Radar Backscatter (NRB) and is the current standard for Analysis-Ready-Backscatter (ARD).","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-2#radiometric-terrain-correction","position":9},{"hierarchy":{"lvl1":"Datacubes"},"type":"lvl1","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-3","position":0},{"hierarchy":{"lvl1":"Datacubes"},"content":"In this notebook we discuss how we can easily compare images of two or more different time slices, satellites or other earth observation products. We limit our selves to products on a regular grid with an associated coordinate reference system (CRS), known as a raster. This means that each cell of the raster contains an attribute value and location coordinates. The process of combining such rasters to form datacubes is called raster stacking. We can have datacubes in many forms, such as the spatiotemporal datacube:Z = f(x,y,t) \\quad \\text{,}\n\nor when dealing with electromagnetic spectrum, the spectral wavelengths may form an additional dimension of a cube:Z = f(x,y,t, \\lambda ) \\quad \\text{.}\n\nWe also have already encountered the case where Z consists of multiple variables, such as seen in the xarray dataset.{Z_1,Z_2,...,Z_3} = f(x,y,t)\n\nTo perform raster stacking, we generally follow a certain routine (see also Figure 1).\n\nCollect data (GeoTIFF, NetCDF, Zarr)\n\nSelect an area of interest\n\nReproject all rasters to the same projection, resolution, and region\n\nStack the individual rasters\n\nTo get the same projection, resolution, and region we have to resample one (or more) products. The desired projection, resolution, and region can be adopted from one of the original rasters or it can be a completely new projection of the data.\n\nFigure 1: Stacking of arrays to form datacubes (source: \n\nhttps://eox.at).\n\nIn this notebook we will study two different SAR products. SAR data from the Advanced Land Observing Satellite (Alos-2), which is a Japanese platform with an L-band sensor from the Japan Aerospace Exploration Agency (JAXA), and C-band data from the Copernicus Sentinel-1 mission. It is our goal to compare C- with L-band, so we need to somehow stack these arrays.\n\nfrom functools import partial\nfrom pathlib import Path\n\nimport folium\nimport hvplot.xarray  # noqa: F401\nimport numpy as np\nimport pandas as pd\nimport rioxarray  # noqa: F401\nimport xarray as xr\nfrom huggingface_hub import snapshot_download\nfrom rasterio.enums import Resampling\nfrom shapely import affinity\nfrom shapely.geometry import box, mapping\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-3","position":1},{"hierarchy":{"lvl1":"Datacubes","lvl2":"Download Data"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-3#download-data","position":2},{"hierarchy":{"lvl1":"Datacubes","lvl2":"Download Data"},"content":"For this exercise we will download a set of GeoTIFF files for both Sentinel-1 and Alos-2, where each image has itâ€™s own timestamp for the acquisition.\n\ndata_path = snapshot_download(\n    repo_id=\"martinschobben/microwave-remote-sensing\",\n    repo_type=\"dataset\",\n    allow_patterns=\"*.tif\",\n)\ndata_path = Path(data_path)\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-3#download-data","position":3},{"hierarchy":{"lvl1":"Datacubes","lvl2":"Loading Data"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-3#loading-data","position":4},{"hierarchy":{"lvl1":"Datacubes","lvl2":"Loading Data"},"content":"Before loading the data into memory we will first look at the area covered by the Sentinel-1 dataset on a map. This way we can select a region of interest for our hypothetical study. We will extract and transform the bounds of the data to longitude and latitude.\n\nbbox = xr.open_mfdataset(\n    (data_path / \"sentinel-1\").glob(\"**/*.tif\"),\n    engine=\"rasterio\",\n    combine=\"nested\",\n    concat_dim=\"band\",\n).rio.transform_bounds(\"EPSG:4326\")\n\nbbox = box(*bbox)\n\nmap = folium.Map(\n    max_bounds=True,\n    location=[bbox.centroid.y, bbox.centroid.x],\n    scrollWheelZoom=False,\n)\n\n# bounds of image\nfolium.GeoJson(mapping(bbox), name=\"Area of Interest\", color=\"red\").add_to(map)\n\n# minimum longitude, minimum latitude, maximum longitude, maximum latitude\narea_of_interest = box(10.3, 45.5, 10.6, 45.6)\n\nfolium.GeoJson(mapping(area_of_interest), name=\"Area of Interest\").add_to(map)\n\nmap\n\nFigure 2: Map of study area. Red rectangle is the area covered by the Sentinel-1 raster. Blue rectangle is the area of interest.\n\nOn the map we have drawn rectangles of the area covered by the images and of our selected study area. To prevent loading too much data we will now only load the data as defined by the blue rectangle on the folium map.\n\nThe Sentinel-1 data is now stored on disk as separate two-dimensional GeoTIFF files with a certain timestamp. The following s1_preprocess function allows to load all files in one go as a spatiotemporal datacube. Basically, the preprocessing function helps reading the timestamp from the file and adds this as a new dimension to the array. The latter allows a concatenation procedure where all files are joined along the new time dimension. In addition by providing area_of_interest.bounds to the parameter bbox we will only load the data of the previously defined area of interest.\n\ndef s1_preprocess(x, bbox, scale):\n    \"\"\"\n    Preprocess file.\n\n    Parameters\n    ----------\n    x : xarray.Dataset\n    bbox: tuple\n      minimum longitude minimum latitude maximum longitude maximum latitude\n    scale: float\n      scaling factor\n    Returns\n    -------\n    xarray.Dataset\n    \"\"\"\n\n    path = Path(x.encoding[\"source\"])\n    filename = path.name\n    x = x.rio.clip_box(*bbox, crs=\"EPSG:4326\")\n\n    date_str = filename.split(\"_\")[0][1:]\n    time_str = filename.split(\"_\")[1][:6]\n    datetime_str = date_str + time_str\n    date = pd.to_datetime(datetime_str, format=\"%Y%m%d%H%M%S\")\n    x = x.expand_dims(dim={\"time\": [date]})\n\n    x = (\n        x.rename({\"band_data\": \"s1_\" + path.parent.stem})\n        .squeeze(\"band\")\n        .drop_vars(\"band\")\n    )\n\n    return x * scale\n\nWe load the data again with open_mfdataset and by providing the preprocess function, including the bounds of the area of interest and the scaling factor, as follows:\n\npartial_ = partial(s1_preprocess, bbox=area_of_interest.bounds, scale=0.01)\n\ns1_ds = xr.open_mfdataset(\n    (data_path / \"sentinel-1\").glob(\"**/*.tif\"),\n    engine=\"rasterio\",\n    combine=\"nested\",\n    chunks=-1,\n    preprocess=partial_,\n)\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-3#loading-data","position":5},{"hierarchy":{"lvl1":"Datacubes","lvl2":"Unlocking Geospatial Information"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-3#unlocking-geospatial-information","position":6},{"hierarchy":{"lvl1":"Datacubes","lvl2":"Unlocking Geospatial Information"},"content":"To enable further stacking of ALOS-2 and Sentinel-1 data we need to know some more information about the raster. Hence we define the following function print_raster to get the projection (CRS), resolution, and region (bounds). The function leverages the functionality of rioxarray; a package for rasters.\n\ndef print_raster(raster, name):\n    \"\"\"\n    Print Raster Metadata\n\n    Parameters\n    ----------\n    raster: xarray.DataArray|xarray.DataSet\n        raster to process\n    y: string\n        name of product\n    \"\"\"\n\n    print(\n        f\"{name} Raster: \\n----------------\\n\"\n        f\"resolution: {raster.rio.resolution()} {raster.rio.crs.units_factor}\\n\"  # noqa\n        f\"bounds: {raster.rio.bounds()}\\n\"\n        f\"CRS: {raster.rio.crs}\\n\"\n    )\n\n\nprint_raster(s1_ds, \"Sentinel-1\")\n\nThe CRS â€œEPSG 27704â€ is part of the EQUI7Grid. This grid provides equal-area tiles, meaning each tile represents the same area, which helps reducing distorsions. This feature is important for remote sensing as it reduces the so-called oversampling due to geometric distortions when projecting on a sphere. This particular projection is developed by TUWien.\n\nNow we will proceed with loading the ALOS-2 L-band data in much the same fashion as for Sentinel-1. Again timeslices are stored separately as individual GeoTIFFS and they need to be concatenated along the time dimension. We use a slightly different preprocessing function alos_preprocess for this purpose. The most notable difference of this function is the inclusion of a scaling factor for the 16-bit digital numbers (DN):\\gamma^0_T = 10 * log_{10}(\\text{DN}^2) - 83.0 \\,dB\n\nto correctly convert the integers to \\gamma^0_T in the dB range.\n\ndef alos_preprocess(x, bbox):\n    \"\"\"\n    Preprocess file.\n\n    Parameters\n    ----------\n    x : xarray.Dataset\n    bbox: tuple\n      minimum longitude minimum latitude maximum longitude maximum latitude\n    Returns\n    -------\n    xarray.Dataset\n    \"\"\"\n\n    path = Path(x.encoding[\"source\"])\n    filename = path.name\n    x = x.rio.clip_box(*bbox, crs=\"EPSG:4326\")\n\n    date_str = filename.split(\"_\")[0][15:22]\n    date = pd.to_datetime(date_str, format=\"%y%m%d\")\n    x = x.expand_dims(dim={\"time\": [date]})\n\n    x = (\n        x.rename({\"band_data\": \"alos_\" + path.parent.stem})\n        .squeeze(\"band\")\n        .drop_vars(\"band\")\n    )\n\n    # conversion to dB scale of alos\n    return 10 * np.log10(x**2) - 83.0\n\nNow we load the data with the open_mfdataset function of xarray and we provide the preprocessing function (see above), which includes the selection of the bounds of an area of interest and the extraction of time stamps from the file name.\n\narea_of_interest = affinity.scale(area_of_interest, xfact=1.7, yfact=1.7)\npartial_ = partial(alos_preprocess, bbox=area_of_interest.bounds)\n\nalos_ds = xr.open_mfdataset(\n    (data_path / \"alos-2\").glob(\"**/*.tif\"),\n    engine=\"rasterio\",\n    combine=\"nested\",\n    chunks=-1,\n    preprocess=partial_,\n)\n\nAlso, for this dataset we will look at the metadata in order to compare it with Sentinel-1.\n\nprint_raster(alos_ds, \"ALOS-2\")\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-3#unlocking-geospatial-information","position":7},{"hierarchy":{"lvl1":"Datacubes","lvl2":"Reprojecting"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-3#reprojecting","position":8},{"hierarchy":{"lvl1":"Datacubes","lvl2":"Reprojecting"},"content":"The ALOS-2 is projected on an UTM grid. We would therefore like to reproject this data to match the projection of Sentinel-1. Furthermore, we will upsample the data to match the Sentinel-1 sampling. The rioxarray package has a very convenient method that can do this all in one go:reproject_match. For continuous data it is best to use a bilinear resampling strategy. As always you have to consider again that we deal with values in the dB range, so we need to convert to the linear scale before bilinear resampling.\n\nalos_ds_lin = 10 ** (alos_ds / 10)\nalos_ds_lin = alos_ds_lin.rio.reproject_match(\n    s1_ds,\n    resampling=Resampling.bilinear,\n)\nalos_ds = 10 * np.log10(alos_ds_lin)\n\nWe will overwrite the coordinate values of ALOS-2 with those of Sentinel-1. If we would not do this last step, small errors in how the numbers are stored would prevent stacking of the rasters.\n\nalos_ds = alos_ds.assign_coords(\n    {\n        \"x\": s1_ds.x.data,\n        \"y\": s1_ds.y.data,\n    }\n)\n\nLastly, we will turn the xarray.DataSet to an xarray.DataArray where a new dimension will constitute the sensor for measurement (satellite + polarization).\n\ns1_da = s1_ds.to_array(dim=\"sensor\")\nalos_da = alos_ds.to_array(dim=\"sensor\")\ns1_da\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-3#reprojecting","position":9},{"hierarchy":{"lvl1":"Datacubes","lvl2":"Stacking of Multiple Arrays"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-3#stacking-of-multiple-arrays","position":10},{"hierarchy":{"lvl1":"Datacubes","lvl2":"Stacking of Multiple Arrays"},"content":"Now we are finally ready to stack Sentinel-1 C-band and ALOS-2 L-band arrays with the function concat of xarray.  Now we can use the newly defined \"sensor\"  dimension to concatenate the two arrays.\n\nfused_da = xr.concat([s1_da, alos_da], dim=\"sensor\").rename(\"gam0\")\nfused_da\n\nThe measurements for both satellites donâ€™t occur at the same time. Hence the cube is now padded with 2-D arrays entirely filled with NaN (Not A Number) for some time slices. As we have learned in notebook 2 we can use the resample method to make temporally coherent timeslices for each month. To deal with the dB scale backscatter values as well as the low number of observations per month we use a median of the samples. As taking the median only sorts the samples according to the sample quantiles we do not have to convert the observations to the linear scale.\n\nfused_da = fused_da.resample(time=\"ME\", skipna=True).median().compute()\n\nWe can plot each of the variables: â€œALOS-2â€ and â€œSentinel-1â€ to check our results.\n\nfused_da.hvplot.image(robust=True, data_aspect=1, cmap=\"Greys_r\", rasterize=True).opts(\n    frame_height=600, aspect=\"equal\"\n)\n\nFigure 3: Stacked array with ALOS-2 L-band and Sentinel-1 C-band \\gamma^0_T (dB).","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-3#stacking-of-multiple-arrays","position":11},{"hierarchy":{"lvl1":"Wavelength and Polarization"},"type":"lvl1","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-4","position":0},{"hierarchy":{"lvl1":"Wavelength and Polarization"},"content":"In this notebook, we aim to demonstrate how C-band (4â€“8 GHz, wavelengths of approximately 3.75â€“7.5 cm) and L-band (1â€“2 GHz, wavelengths of approximately 15â€“30 cm) radio frequencies differ for different land covers and times of the year. In addition, weâ€™ll look at co- and cross-polarized backscattering:\n\nSentinel-1 (C-band)\n\nVV\n\nVH\n\nAlos-2 (L-band):\n\nHH\n\nHV\n\nimport holoviews as hv\nimport hvplot.xarray  # noqa: F401\nimport intake\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-4","position":1},{"hierarchy":{"lvl1":"Wavelength and Polarization","lvl2":"Data Loading"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-4#data-loading","position":2},{"hierarchy":{"lvl1":"Wavelength and Polarization","lvl2":"Data Loading"},"content":"We load the data again with the help of intake.\n\nurl = \"https://huggingface.co/datasets/martinschobben/microwave-remote-sensing/resolve/main/microwave-remote-sensing.yml\"\ncat = intake.open_catalog(url)\nfused_ds = cat.fused.read()\nfused_ds\n\nThe loaded data contains the Leaf Area Index (LAI), which is used as an estimate of foliage cover of forest canopies. So high LAI is interpreted as forested area, whereas low values account for less vegetated areas (shrubs, grass-land, and crops).\n\nFirst weâ€™ll have a look at the mean and standard deviation of LAI over all timeslices. This can be achieved by using the mean and std methods of the xarray object and by supplying a dimension over which these aggregating operations will be applied. We use the dimension â€œtimeâ€, thereby flattening the cube to a 2-D array with dimensions x and y.\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 6))\n\nLAI_dc = fused_ds.LAI\nLAI_mean = LAI_dc.mean(\"time\")\nLAI_std = LAI_dc.std(\"time\")\n\nLAI_mean.plot(ax=ax[0], vmin=0, vmax=6).axes.set_aspect(\"equal\")\nLAI_std.plot(ax=ax[1], vmin=0, vmax=3).axes.set_aspect(\"equal\")\nplt.tight_layout()\n\nFigure 1: Map of mean LAI (left) and the associated standard deviation (right) for each pixel over time around Lake Garda.\n\nIt appears that the northern parts of our study area contain more and variable amounts of green elements per unit area. This might indicate a more complete coverage of foliage and thus forest.","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-4#data-loading","position":3},{"hierarchy":{"lvl1":"Wavelength and Polarization","lvl2":"Timeseries"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-4#timeseries","position":4},{"hierarchy":{"lvl1":"Wavelength and Polarization","lvl2":"Timeseries"},"content":"Now that we have detected possible forested areas, letâ€™s delve a bit deeper into the data. Remember that we deal with a spatiotemporal datacube. This gives us the possibility to study changes for each time increment. Hence we can show what happens to LAI for areas marked with generally low values as well as high values. We can achieve this by filtering the datacube with the where method for areas marked with low and high mean LAI values. In turn we will aggregate the remaining datacube over the spatial dimensions (â€œxâ€ and â€œyâ€) to get a mean values for each time increment.\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 4))\n\nLAI_low = LAI_dc.where(LAI_mean < 4)\nLAI_high = LAI_dc.where(LAI_mean > 4)\n\nLAI_low.mean([\"x\", \"y\"]).plot.scatter(x=\"time\", ax=ax[0], ylim=(0, 6))\nLAI_high.mean([\"x\", \"y\"]).plot.scatter(x=\"time\", ax=ax[1], ylim=(0, 6))\nax[0].set_title(\"Low Mean LAI ($\\\\bar{LAI} < 4$)\")\nax[1].set_title(\"High Mean LAI ($\\\\bar{LAI} > 4$)\")\nplt.tight_layout()\n\nFigure 2: Timeseries of mean LAI per timeslice for areas with low (left) and high (right) mean LAI of Figure1.\n\nNow we can see that areas with high mean LAI values (Figure 1) show a drop-off to values as low as those for areas with low mean LAI during the autumn months (Figure 2 ; right panel). Hence we might deduce that we deal with deciduous forest that becomes less green during autumn, as can be expected for the study area.\n\nRemember that longer wavelengths like L-bands are more likely to penetrate through a forest canopy and would interact more readily with larger object like tree trunks and the forest floor. In turn, C-band microwaves are more likely to interact with sparse and shrub vegetation. The polarization of the emitted and received microwaves is on the other hand dependent on the type of backscattering with co-polarization (HH and VV) happening more frequently with direct backscatter or double bounce scattering. Whereas volume scattering occurs when the radar signal is subject to multiple reflections within 3-dimensional matter, as the orientation of the main scatterers is random, the polarization of the backscattered signal is also random. Volume scattering can therefore cause an increase of cross-polarized intensity.\n\nLetâ€™s put this to the test by checking the microwave backscatter signatures over forested and sparsely vegetated areas as well as water bodies (Lake Garda). Letâ€™s first look at the different sensor readings for the beginning of summer and autumn.\n\nhv.output(widget_location=\"bottom\")\n\nt1 = (\n    fused_ds.gam0.isel(time=2)\n    .hvplot.image(\n        robust=True, data_aspect=1, cmap=\"Greys_r\", rasterize=True, clim=(-25, 0)\n    )\n    .opts(frame_height=400, aspect=\"equal\")\n)\n\nt2 = (\n    fused_ds.gam0.isel(time=-1)\n    .hvplot.image(\n        robust=True, data_aspect=1, cmap=\"Greys_r\", rasterize=True, clim=(-25, 0)\n    )\n    .opts(frame_height=400, aspect=\"equal\")\n)\n\nt1 + t2\n\nFigure 3: Maps of Sentinel-1 and Alos-2 \\gamma^0_T \\,[dB] for the beginning of summer (left) and autumn (right).\n\nThe most notable difference is the lower energy received for cross-polarized than for co-polarized microwaves for both Sentinel-1 and Alos-2. The latter differences are independent of the time of year. However, one can also note small changes in the received energy for the same satellite dependent on the time of year. To get a better feel for these changes over time we generate the following interactive plot. On the following plot one can select areas of a certain mean LAI (by clicking on the map) and see the associated timeseries of \\gamma^0_T for each of the sensors.\n\nLAI_image = LAI_mean.hvplot.image(rasterize=True, cmap=\"viridis\", clim=(0, 6)).opts(\n    title=\"Mean LAI (Selectable)\", frame_height=400, aspect=\"equal\"\n)\n\n\ndef get_timeseries(x, y):\n    \"\"\"\n    Callback Function Holoviews\n\n    Parameters\n    ----------\n    x: float\n        numeric value for x selected on LAI map\n    y: float\n        numeric value for y selected on LAI map\n    \"\"\"\n\n    lai_value = LAI_mean.sel(x=x, y=y, method=\"nearest\").values\n\n    if np.isnan(lai_value):\n        select = fused_ds.where(LAI_mean.isnull())\n        label = \"Water\"\n    else:\n        mask = np.isclose(LAI_mean, lai_value, atol=0.05)\n        select = fused_ds.where(mask)\n        label = \"Mean LAI: \" + str(np.round(lai_value, 1))\n\n    time_series = (\n        select.gam0.to_dataset(\"sensor\")\n        .median([\"x\", \"y\"], skipna=True)\n        .hvplot.scatter(ylim=(-30, 5))\n        .opts(title=label, frame_height=400)\n    )\n\n    return time_series\n\n\npoint_stream = hv.streams.SingleTap(source=LAI_image)\ntime_series = hv.DynamicMap(get_timeseries, streams=[point_stream])\nLAI_image + time_series\n\nFigure 4: Map of MEAN LAI around Lake Garda. The pixel values can be seen by hovering your mouse over the pixels. Clicking on the pixel will generate the timeseries for the associated mean LAI on the right hand-side. (Right) Timeseries of for Sentinel-1 and Alos-2 \\gamma^0_T [dB].\n\nCan you see some patterns when analyzing the different wavelengths and polarizations?\n\nRemember again that we deal with a logarithmic scale. A measurement of 10 dB is 10 times brighter than the intensity measured at 0 dB, and 100 times brighter at 20 dB. The most notable difference is that the offset between cross- and co-polarised signals becomes larger at low LAI and lower at higher LAI. This might indicate the effect of volume scattering in forested areas where co- and cross-polarization render backscattering values more equal. You will study the differences among cross- and co-polarized backscattering in more detail in the homework exercise.","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-4#timeseries","position":5},{"hierarchy":{"lvl1":"Dielectric Properties"},"type":"lvl1","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-5","position":0},{"hierarchy":{"lvl1":"Dielectric Properties"},"content":"In this notebook, we will investigate the varying backscatter values associated with different land surfaces like water bodies, forests, grasslands and urban areas. We will use backscatter data from the Sentinel-1 satellite and we will utilize the CORINE Land Cover dataset to classify and extrapolate these surfaces, enabling us to analyze how different land cover types influence backscatter responses.\n\nimport json\n\nimport holoviews as hv\nimport intake\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport rioxarray  # noqa: F401\nimport xarray as xr\nfrom holoviews.streams import RangeXY\nfrom matplotlib.colors import BoundaryNorm, ListedColormap\n\nhv.extension(\"bokeh\")\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-5","position":1},{"hierarchy":{"lvl1":"Dielectric Properties","lvl2":"Load Sentinel-1 Data"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-5#load-sentinel-1-data","position":2},{"hierarchy":{"lvl1":"Dielectric Properties","lvl2":"Load Sentinel-1 Data"},"content":"For our analysis we are using sigma naught backscatering data from Sentinel-1. The images we are analyzing cover the region south of Vienna and west of Lake Neusiedl. We load the data and and apply again a preprocessing function. Here we extract the scaling factor and the date the image was taken from the metadata. We will focus our attention to a smaller area containing a part of the Lake Neusiedl Lake and its surrounding land. The obtainedxarray dataset and is then converted to an array, because we only have one variable, the VV backscatter values.\n\nurl = \"https://huggingface.co/datasets/martinschobben/microwave-remote-sensing/resolve/main/microwave-remote-sensing.yml\"\ncat = intake.open_catalog(url)\nsig0_da = cat.neusiedler.read().sig0.compute()\n\nLetâ€™s have a look at the data by plotting the first timeslice.\n\nsig0_da.isel(time=0).plot(robust=True, cmap=\"Greys_r\").axes.set_aspect(\"equal\")\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-5#load-sentinel-1-data","position":3},{"hierarchy":{"lvl1":"Dielectric Properties","lvl2":"Load CORINE Landcover Data"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-5#load-corine-landcover-data","position":4},{"hierarchy":{"lvl1":"Dielectric Properties","lvl2":"Load CORINE Landcover Data"},"content":"We will load the CORINE Land Cover, which is a pan-European land cover and land use inventory with 44 thematic classes. The resolution of this classification is 100 by 100m and the file was created in 2018\n(\n\nCORINE Land Cover).\n\ncor_da = cat.corine.read().land_cover.compute()\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-5#load-corine-landcover-data","position":5},{"hierarchy":{"lvl1":"Dielectric Properties","lvl3":"Colormapping and Encoding","lvl2":"Load CORINE Landcover Data"},"type":"lvl3","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-5#colormapping-and-encoding","position":6},{"hierarchy":{"lvl1":"Dielectric Properties","lvl3":"Colormapping and Encoding","lvl2":"Load CORINE Landcover Data"},"content":"For the different land cover types we use the official color encoding.\n\n# Load encoding\nwith cat.corine_cmap.read()[0] as f:\n    color_mapping_data = json.load(f)\n\n# Get mapping\ncolor_mapping = {item[\"value\"]: item for item in color_mapping_data[\"land_cover\"]}\n\n# Create cmap and norm for plotting\ncolors = [info[\"color\"] for info in color_mapping.values()]\ncategories = [info[\"value\"] for info in color_mapping.values()]\ncmap = ListedColormap(colors)\nnorm = BoundaryNorm(categories + [max(categories) + 1], len(categories))\n\nNow we can plot the CORINE Land Cover dataset.\n\n# Get landcover codes present in the image\npresent_landcover_codes = np.unique(cor_da.values[~np.isnan(cor_da.values)].astype(int))\n\n# Get colors + text for legend\nhandles = [\n    mpatches.Patch(color=info[\"color\"], label=(f'{info[\"value\"]} - ' + (info[\"label\"])))\n    for info in color_mapping.values()\n    if info[\"value\"] in present_landcover_codes\n]\n\n# Create the plot\ncor_da.plot(figsize=(10, 10), cmap=cmap, norm=norm, add_colorbar=False).axes.set_aspect(\n    \"equal\"\n)\n\nplt.legend(\n    handles=handles,\n    bbox_to_anchor=(1.01, 1),\n    loc=\"upper left\",\n    borderaxespad=0,\n    fontsize=7,\n)\nplt.title(\"CORINE Land Cover (EPSG:27704)\")\n\nNow we are ready to merge the backscatter data (sig0_da) with the land cover dataset (cor_da) to have one dataset combining all data.\n\nvar_ds = xr.merge([sig0_da, cor_da]).drop_vars(\"band\")\nvar_ds\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-5#colormapping-and-encoding","position":7},{"hierarchy":{"lvl1":"Dielectric Properties","lvl2":"Backscatter Variability"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-5#backscatter-variability","position":8},{"hierarchy":{"lvl1":"Dielectric Properties","lvl2":"Backscatter Variability"},"content":"With this combined dataset we can study backscatter variability in relation to natural media. For example we can look at the backscatter variability for water by clipping the dataset to only contain the land cover class water, like so:\n\n# 41 = encoded value for water bodies\nwaterbodies_mask = var_ds.land_cover == 41\nwaterbodies_mask.plot().axes.set_aspect(\"equal\")\n\nThis gives use backscatter values over water only.\n\nwaterbodies_sig0 = var_ds.sig0.isel(time=0).where(waterbodies_mask)\nwaterbodies_sig0.plot(robust=True, cmap=\"Greys_r\").axes.set_aspect(\"equal\")\n\nTo get an idea of the variability we can create a histogram. Radar backscatter from water bodies fluctuates with surface roughness, which changes with wind conditions, creating spatial and temporal variations in signal intensity.\n\nwaterbodies_sig0.plot.hist(bins=50, edgecolor=\"black\")\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-5#backscatter-variability","position":9},{"hierarchy":{"lvl1":"Dielectric Properties","lvl2":"Variability over Time"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-5#variability-over-time","position":10},{"hierarchy":{"lvl1":"Dielectric Properties","lvl2":"Variability over Time"},"content":"Next we will look at the changes in variability in backscatter values over time for each of the CORINE Land Cover types. We do this by creating the following interactive plot. We can spot that backscatter in agricultural fields varies due to seasonal cycles like planting, growing, and harvesting, each of which changes vegetation structure. Changes in backscatter are strongly related to soil moisture content from irrigation or rainfall. Ultimately, phenological stages of crops and canopy moisture dynamics can affect the backscatter signal.\n\nrobust_min = var_ds.sig0.quantile(0.02).item()\nrobust_max = var_ds.sig0.quantile(0.98).item()\n\nbin_edges = [\n    i + j * 0.5\n    for i in range(int(robust_min) - 2, int(robust_max) + 2)\n    for j in range(2)\n]\n\nland_cover = {\"\\xa0\\xa0\\xa0 Complete Land Cover\": 1}\nland_cover.update(\n    {\n        f\"{int(value): 02} {color_mapping[value]['label']}\": int(value)\n        for value in present_landcover_codes\n    }\n)\ntime = var_ds.sig0[\"time\"].values\n\nrangexy = RangeXY()\n\n\ndef load_image(time, land_cover, x_range, y_range):\n    \"\"\"\n    Callback Function Landcover.\n\n    Parameters\n    ----------\n    time: panda.datatime\n        time slice\n    landcover: int\n        land cover type\n    x_range: array_like\n        longitude range\n    y_range: array_like\n        latitude range\n\n    Returns\n    -------\n    holoviews.Image\n    \"\"\"\n\n    if land_cover == \"\\xa0\\xa0\\xa0 Complete Land Cover\":\n        sig0_selected_ds = var_ds.sig0.sel(time=time)\n\n    else:\n        land_cover_value = int(land_cover.split()[0])\n        mask_ds = var_ds.land_cover == land_cover_value\n        sig0_selected_ds = var_ds.sig0.sel(time=time).where(mask_ds)\n\n    hv_ds = hv.Dataset(sig0_selected_ds)\n    img = hv_ds.to(hv.Image, [\"x\", \"y\"])\n\n    if x_range and y_range:\n        img = img.select(x=x_range, y=y_range)\n\n    return hv.Image(img)\n\n\ndmap = (\n    hv.DynamicMap(load_image, kdims=[\"Time\", \"Landcover\"], streams=[rangexy])\n    .redim.values(Time=time, Landcover=land_cover)\n    .hist(normed=True, bins=bin_edges)\n)\n\nimage_opts = hv.opts.Image(\n    cmap=\"Greys_r\",\n    colorbar=True,\n    tools=[\"hover\"],\n    clim=(robust_min, robust_max),\n    aspect=\"equal\",\n    framewise=False,\n    frame_height=500,\n    frame_width=500,\n)\n\nhist_opts = hv.opts.Histogram(width=350, height=555)\n\ndmap.opts(image_opts, hist_opts)","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-5#variability-over-time","position":11},{"hierarchy":{"lvl1":"Speckle Statistics"},"type":"lvl1","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-6","position":0},{"hierarchy":{"lvl1":"Speckle Statistics"},"content":"This notebook will provide an empirical demonstration of speckle - how it originates, how it visually and statistically looks like, and some of the most common approaches to filter it.\n\nSpeckle is defined as a kind of noise that affects all radar images. Given the multiple scattering contributions originating from the various elementary objects present within a resolution cell, the resulting backscatter signal can be described as a random constructive and destructive interference of wavelets. As a consequence, speckle is the reason why a granular pattern normally affects SAR images, making it more challenging to interpret and analyze them.\n\nCredits: ESRI\n\nimport json\nfrom functools import partial\n\nimport holoviews as hv\nimport intake\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom holoviews.streams import RangeXY\nfrom scipy.ndimage import uniform_filter\n\nhv.extension(\"bokeh\")\n\nLetâ€™s make an example of a cornfield (with a typical backscattering value of about -10 dB). According to the following equation:\\sigma^0 = \\frac{1}{\\text{area}} \\sum_{n \\in \\text{area}} \\sigma_n\n\nWe should ideally have a uniform discrete sigma naught \\sigma^0 value, given that the cornfield pixel is the only individual contributor.\n\nHowever, since we already learned from the previous notebooks that a pixelâ€™s ground size can be in the order of tens of meters (i.e., 10 meters for Sentinel-1), we can imagine that different distributed targets in the scene contribute to the global backscattered information.\n\nLetÂ´s replicate this behavior with an ideal uniform area constituted by 100 pixels and then by adding 30% of speckle.\n\nideal_backscatter = -10  # in dB, a typical value for cornfields\nwidth = 12\nsize = (width, width)\nideal_data = np.full(size, ideal_backscatter)\nideal_data_linear = 10 ** (\n    ideal_data / 10\n)  # Convert dB to linear scale for speckle addition\n\nspeckle_fraction = 0.3\nnum_speckled_pixels = int(\n    size[0] * size[1] * speckle_fraction\n)  # Rayleigh speckle noise\nspeckled_indices = np.random.choice(\n    width * width, num_speckled_pixels, replace=False\n)  # random indices for speckle\n\n# Initialize speckled data as the same as the ideal data\nspeckled_data_linear = ideal_data_linear.copy()\n\nspeckle_noise = np.random.gumbel(scale=1.0, size=num_speckled_pixels)\nspeckled_data_linear.ravel()[\n    speckled_indices\n] *= speckle_noise  # Add speckle to the selected pixels\n\nideal_data_dB = 10 * np.log10(ideal_data_linear)\nspeckled_data_dB = 10 * np.log10(speckled_data_linear)\nplt.figure(figsize=(16, 10))\n\n# Ideal data\nplt.subplot(2, 2, 1)\nplt.imshow(ideal_data_dB, cmap=\"gray\", vmin=-20, vmax=0)\nplt.title(\"Ideal Backscatter (Cornfield)\")\nplt.colorbar(label=\"Backscatter (dB)\")\n\n# Speckled data\nplt.subplot(2, 2, 2)\nplt.imshow(speckled_data_dB, cmap=\"gray\", vmin=-20, vmax=0)\nplt.title(f\"Speckled Backscatter ({int(speckle_fraction * 100)}% of Pixels)\")\nplt.colorbar(label=\"Backscatter (dB)\")\n\nbins = 25\nhist_ideal, bins_ideal = np.histogram(ideal_data_dB.ravel(), bins=bins, range=(-20, 0))\nhist_speckled, bins_speckled = np.histogram(\n    speckled_data_dB.ravel(), bins=bins, range=(-20, 0)\n)\nmax_freq = max(\n    hist_ideal.max(), hist_speckled.max()\n)  # maximum frequency for normalization\n\n# Histogram for ideal data\nplt.subplot(2, 2, 3)\nplt.hist(ideal_data_dB.ravel(), bins=bins, range=(-20, 0), color=\"gray\", alpha=0.7)\nplt.ylim(0, max_freq)\nplt.title(\"Histogram of Ideal Backscatter\")\nplt.xlabel(\"Backscatter (dB)\")\nplt.ylabel(\"Frequency\")\n\n# Histogram for speckled data\nplt.subplot(2, 2, 4)\nplt.hist(speckled_data_dB.ravel(), bins=bins, range=(-20, 0), color=\"gray\", alpha=0.7)\nplt.ylim(0, max_freq)\nplt.title(f\"Histogram of Speckled Backscatter ({int(speckle_fraction * 100)}%)\")\nplt.xlabel(\"Backscatter (dB)\")\nplt.ylabel(\"Frequency\")\n\nplt.tight_layout()\n\nFigure 1: Synthetic data that emulates speckles in microwave backscattering\n\nWe can imagine that the second plot represents a real SAR acquisition over a cornfield, while the first plot represents an ideal uniform SAR image over a cornfield land (no speckle). The introduction of a simulated 30% speckle noise could be related to the presence of distributed scatterers of any sort present in the scene, which would cause a pixel-to-pixel variation in terms of intensity.\n\nAll the random contributions (such as the wind) would result in a different speckle pattern each time a SAR scene is acquired over the same area. Many subpixel contributors build up a complex scattered pattern in any SAR image, making it erroneous to rely on a single pixel intensity for making reliable image analysis. In order to enhance the degree of usability of a SAR image, several techniques have been put in place to mitigate speckle.\nWe will now show two of the most common approaches: the temporal and the spatial filter.","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-6","position":1},{"hierarchy":{"lvl1":"Speckle Statistics","lvl2":"Lake Neusiedl data"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-6#lake-neusiedl-data","position":2},{"hierarchy":{"lvl1":"Speckle Statistics","lvl2":"Lake Neusiedl data"},"content":"We load a dataset that contains the CORINE land cover and Sentinel-1 \\sigma^0_E at a 20 meter resolution.  This is the same data presented in notebook 6.\n\nurl = \"https://huggingface.co/datasets/martinschobben/microwave-remote-sensing/resolve/main/microwave-remote-sensing.yml\"\ncat = intake.open_catalog(url)\nfused_ds = cat.speckle.read().compute()\nfused_ds\n\nWe also create the same dashboard for backscatter of different landcover types over time. In order to make this code reusable and adaptable we will define the following function plot_variability, which allows the injection of a spatial and/or temporal filter. It is not important to understand all the code of the following cell!\n\n# Load encoding\nwith cat.corine_cmap.read()[0] as f:\n    color_mapping_data = json.load(f)\n\n# Get mapping\ncolor_mapping = {item[\"value\"]: item for item in color_mapping_data[\"land_cover\"]}\n\n# Get landcover codes present in the image\npresent_landcover_codes = np.unique(\n    fused_ds.land_cover.values[~np.isnan(fused_ds.land_cover.values)].astype(int)\n)\n\n\ndef load_image(var_ds, time, land_cover, x_range, y_range, filter_fun_spatial=None):\n    \"\"\"\n    Callback Function Landcover.\n\n    Parameters\n    ----------\n    time: panda.datetime\n        time slice\n    landcover: int\n        land cover type\n    x_range: array_like\n        longitude range\n    y_range: array_like\n        latitude range\n\n    Returns\n    -------\n    holoviews.Image\n    \"\"\"\n\n    if time is not None:\n        var_ds = var_ds.sel(time=time)\n\n    if land_cover == \"\\xa0\\xa0\\xa0 Complete Land Cover\":\n        sig0_selected_ds = var_ds.sig0\n    else:\n        land_cover_value = int(land_cover.split()[0])\n        mask_ds = var_ds.land_cover == land_cover_value\n        sig0_selected_ds = var_ds.sig0.where(mask_ds)\n\n    if filter_fun_spatial is not None:\n        sig0_np = filter_fun_spatial(sig0_selected_ds.values)\n    else:\n        sig0_np = sig0_selected_ds.values\n\n    # Convert unfiltered data into Holoviews Image\n    img = hv.Dataset(\n        (sig0_selected_ds[\"x\"], sig0_selected_ds[\"y\"], sig0_np), [\"x\", \"y\"], \"sig0\"\n    )\n\n    if x_range and y_range:\n        img = img.select(x=x_range, y=y_range)\n\n    return hv.Image(img)\n\n\ndef plot_variability(var_ds, filter_fun_spatial=None, filter_fun_temporal=None):\n\n    robust_min = var_ds.sig0.quantile(0.02).item()\n    robust_max = var_ds.sig0.quantile(0.98).item()\n\n    bin_edges = [\n        i + j * 0.5\n        for i in range(int(robust_min) - 2, int(robust_max) + 2)\n        for j in range(2)\n    ]\n\n    land_cover = {\"\\xa0\\xa0\\xa0 Complete Land Cover\": 1}\n    land_cover.update(\n        {\n            f\"{int(value): 02} {color_mapping[value]['label']}\": int(value)\n            for value in present_landcover_codes\n        }\n    )\n    time = var_ds.sig0[\"time\"].values\n\n    rangexy = RangeXY()\n\n    if filter_fun_temporal is not None:\n        var_ds = filter_fun_temporal(var_ds)\n        load_image_ = partial(\n            load_image, var_ds=var_ds, filter_fun_spatial=filter_fun_spatial, time=None\n        )\n        dmap = (\n            hv.DynamicMap(load_image_, kdims=[\"Landcover\"], streams=[rangexy])\n            .redim.values(Landcover=land_cover)\n            .hist(normed=True, bins=bin_edges)\n        )\n\n    else:\n        load_image_ = partial(\n            load_image, var_ds=var_ds, filter_fun_spatial=filter_fun_spatial\n        )\n        dmap = (\n            hv.DynamicMap(load_image_, kdims=[\"Time\", \"Landcover\"], streams=[rangexy])\n            .redim.values(Time=time, Landcover=land_cover)\n            .hist(normed=True, bins=bin_edges)\n        )\n\n    image_opts = hv.opts.Image(\n        cmap=\"Greys_r\",\n        colorbar=True,\n        tools=[\"hover\"],\n        clim=(robust_min, robust_max),\n        aspect=\"equal\",\n        framewise=False,\n        frame_height=500,\n        frame_width=500,\n    )\n\n    hist_opts = hv.opts.Histogram(width=350, height=555)\n\n    return dmap.opts(image_opts, hist_opts)\n\nNow, lets work on the real-life dataset to see how speckle actually looks like.\n\nplot_variability(fused_ds)\n\nFigure 2: Lake Neusiedl \\sigma^0_E without any filter.\n\nThe speckle noise typically appears as a â€œsalt-and-pepperâ€ pattern. Also, please note the distribution of backscatter for each land cover. Even though speckle is known for following non-normal distributions (i.e., Rayleigh distribution for amplitude in the linear domain, and the Gumple for intensity in the log domain), we can assume that due to the Central Limit Theorem, the overall backscatter means (dB) tend to follow a Gaussian distribution.\n\nWe can mitigate speckle (it is impossible to remove it completely) by following approaches such as:\n\nspatial filtering - taking mean backscatter value over the same land cover, or\n\ntemporal filtering - taking the average backscatter value over some time period.\n\nEither way, one pixel is never representative of ground truth! Therefore we need to look at samples and distributions.","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-6#lake-neusiedl-data","position":3},{"hierarchy":{"lvl1":"Speckle Statistics","lvl2":"Spatial filtering"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-6#spatial-filtering","position":4},{"hierarchy":{"lvl1":"Speckle Statistics","lvl2":"Spatial filtering"},"content":"We first introduce a common spatial filter. The Lee filter is an adaptive speckle filter. The filter works using a kernel window with a configurable size, which refers to the dimensions of the neighborhood over which the filter operates. The kernel slides across the data, applying the smoothing operation at each pixel position of the image. It follows three assumptions:\n\nSAR speckle is modeled as a multiplicative noise - the brighter the area the noisier the data.\n\nThe noise and the signal are statistically independent of each other.\n\nThe sample mean and sample variance of a pixel is equal to its local mean and local variance.\n\nThis approach comes with some limitations: it reduces the spatial resolution of the SAR image.\n\nLetâ€™s build up a function for applying a Lee filter with a kernel window size of 7 (do not forget to switch back to linear units before doing this computation and to dB after it):\n\ndef lee_filter(raster, size=7):\n    \"\"\"\n    Parameters:\n    raster: ndarray\n        2D array representing the noisy image (e.g., radar image with speckle)\n    size: int\n        Size of the kernel window for the filter (must be odd, default is 7)\n\n    Returns:\n    filtered_image (ndarray): The filtered image with reduced speckle noise\n    \"\"\"\n\n    raster = np.nan_to_num(raster)\n    raster = 10 ** (raster / 10)\n\n    # Mean and variance over local kernel window\n    mean_window = uniform_filter(raster, size=size)\n    mean_sq_window = uniform_filter(raster**2, size=size)\n    variance_window = mean_sq_window - mean_window**2\n\n    # Noise variance estimation (this could also be set manually)\n    overall_variance = np.var(raster)\n\n    # Compute the Lee filter\n    weights = variance_window / (variance_window + overall_variance)\n\n    return 10 * np.log10(mean_window + weights * (raster - mean_window))\n\nplot_variability(fused_ds, filter_fun_spatial=lee_filter)\n\nFigure 3: Lake Neusiedl \\sigma^0_E with a Lee filter applied.","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-6#spatial-filtering","position":5},{"hierarchy":{"lvl1":"Speckle Statistics","lvl2":"Temporal filtering"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-6#temporal-filtering","position":6},{"hierarchy":{"lvl1":"Speckle Statistics","lvl2":"Temporal filtering"},"content":"Temporal filtering would involve taking the average of all previous (past) observations for each pixel. This approach comes with some limitations: it takes out the content-rich information tied to the temporal variability of backscatter.\n\ndef temporal_filter(raster):\n    \"\"\"\n    Parameters:\n    raster: ndarray\n        3D array representing the noisy image over time\n        (e.g., radar image with speckle)\n\n    Returns:\n    filtered_image (ndarray): The filtered image with reduced speckle noise\n    \"\"\"\n\n    return raster.mean(\"time\")\n\nplot_variability(fused_ds, filter_fun_temporal=temporal_filter)\n\nFigure 4: Lake Neusiedl \\sigma^0_E with a temporal filter applied.\n\nLetÂ´s observe the histograms of the two plots. Especially in the region around the lake, it is clear that the distribution is now less dispersed and more centered around a central value.","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-6#temporal-filtering","position":7},{"hierarchy":{"lvl1":"Interferograms"},"type":"lvl1","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-7","position":0},{"hierarchy":{"lvl1":"Interferograms"},"content":"On July 5th, 2019, an earthquake with a magnitude of 7.1 mainshock struck eastern California, near the city of Ridgecrest. The seismic event produced a surface rupture spanning more than 50 kilometers with a complex vertical and horizontal offset pattern along the main fault line.\nSAR imagery can be employed for accurately measuring and describing ground motion through a well-established technique called SAR Interferometry. In this framework, the phase information contained in Synthetic Aperture Radar (SAR) data is employed. In this notebook, we will dive into the main interferometric SAR processing operations which involves retrieving the difference between the phase signals of repeated SAR acquisitions to analyze the shape and deformation of the Earthâ€™s surface. In our case, we will use a pair of Single Look Complex (SLC) Sentinel-1 images to obtain an interferogram of the Ridgecrest earthquake.\n\nThis notebook will outline the process of working with interferograms and the steps needed to extract valuable information. Here, we will focus on displaying products generated by the Sentinel Application Platform (SNAP) software from the European Space Agency (ESA).\n\nPhoto by Brian Olson / California Geological Survey\n\nimport base64\nfrom io import BytesIO\n\nimport folium\nimport holoviews as hv\nimport hvplot.xarray  # noqa: F401\nimport intake\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nhv.extension(\"bokeh\")\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-7","position":1},{"hierarchy":{"lvl1":"Interferograms","lvl2":"Single Look Complex (SLC) Data"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-7#single-look-complex-slc-data","position":2},{"hierarchy":{"lvl1":"Interferograms","lvl2":"Single Look Complex (SLC) Data"},"content":"We introduce now another level-1 radar product type, which is called Single Look Complex (SLC). Interferometry can only be performed with SLC data. What are the main differences between SLC and GRD (the other level-1 radar product)?\n\nSLC vs GRD:\n\nSLC contains complex-value data (amplitude and phase) vs GRD contains intensity only (amplitude)\n\nSLC geometry is Slant Range (radarâ€™s line of sight) vs GRD data are projected onto ground range\n\nSLC resolution is full vs GRD has lower resolution (it is multi-looked)\n\nSLC supports phase-based applications (Interferometry) vs GRD supports only amplitude-based ones\n\nSLC has larger file sizes compared to GRD\n\nurl = \"https://huggingface.co/datasets/martinschobben/microwave-remote-sensing/resolve/main/microwave-remote-sensing.yml\"\ncat = intake.open_catalog(url)\niw1_ds = cat.iw1.read()\niw2_ds = cat.iw2.read()\niw3_ds = cat.iw3.read()\n\nLetâ€™s plot all three sub-swaths to view the full scene acquired by the satellite. The acquisition times for each swath on July 10th, 2019 are the following:\n\nIW1 at 01:50:01 - 01:50:26\n\nIW2 at 01:49:59 - 01:50:24\n\nIW3 at 01:50:00 - 01:50:25\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 7), sharey=True)\n\ndatasets = [iw1_ds, iw2_ds, iw3_ds]\nval_range = dict(vmin=0, vmax=255, cmap=\"gray\")\n\nfor i, ds in enumerate(datasets):\n    im = ds.intensity.plot(ax=ax[i], add_colorbar=False, **val_range)\n    ax[i].tick_params(axis=\"both\", which=\"major\")\n\ncbar = fig.colorbar(im, ax=ax, orientation=\"horizontal\", shrink=0.9, pad=0.2)\n\nplt.show()\n\nWe donâ€™t need all three of the subswaths for our notebook, so we will focus on IW2 and display its intensity and phase measurements.\n\n# Compute the intensity and phase from complex data\ncmap_hls = sns.hls_palette(as_cmap=True)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\nds.intensity.plot(ax=axes[0], cmap=\"gray\", robust=True)\naxes[0].set_title(\"Intensity Measurement of IW2\")\n\nds.phase.plot(ax=axes[1], cmap=cmap_hls)\naxes[1].set_title(\"Phase Measurement of IW2\")\n\nplt.tight_layout()\n\nIntensity is represented in an 8-bit format (ranging from 0 to 255), while phase measurements range from - \\pi to Ï€ . At first glance, phase does not correspond to easily observable physical properties of the ground.  However, the phase becomes incredibly valuable when, for example, it is used comparatively between two successive phase measurements (two Sentinel-1 images acquired at different times over the same area). Here are the processing steps needed to retrieve a difference between the phases of two radar acquisitions:","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-7#single-look-complex-slc-data","position":3},{"hierarchy":{"lvl1":"Interferograms","lvl2":"Coregistering"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-7#coregistering","position":4},{"hierarchy":{"lvl1":"Interferograms","lvl2":"Coregistering"},"content":"Before creating an interferogram, measurements from two different dates need to be coregistered. This means that each pixel from the two acquisitions must be precisely aligned so that they are representing the same ground object. Accurate and successful co-registration of the two (or more) images is vital for interferometry processing.\nWe call the â€œmasterâ€ image the reference image (typically the earliest acquisition in time) to which we coregister the â€œslaveâ€ image (typically acquired later in time).\n\ncoregistered_ds = cat.coreg.read()\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 8))\ncoregistered_ds.band_data.sel(band=1).plot(ax=axes[0], cmap=\"gray\", robust=True)\naxes[0].set_title(\"Master Phase Measurement - 28 Jun 2019\")\n\ncoregistered_ds.band_data.sel(band=2).plot(ax=axes[1], cmap=\"gray\", robust=True)\naxes[1].set_title(\"Slave Phase Measurement - 10 Jul 2019\")\n\nplt.tight_layout()\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-7#coregistering","position":5},{"hierarchy":{"lvl1":"Interferograms","lvl2":"Interferogram Formation and Coherence Estimation"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-7#interferogram-formation-and-coherence-estimation","position":6},{"hierarchy":{"lvl1":"Interferograms","lvl2":"Interferogram Formation and Coherence Estimation"},"content":"The interferogram formation process combines the amplitudes of both images and calculates the difference between their respective phases at each SAR image pixel (cross-multiplication of the master image with the complex conjugate of the slave image).\n\nAfter building up the interferogram, we have to take into account the presence of other contributing terms that could hinder our goal of measuring the surface deformation due to the earthquake. For example, we need to subtract from the interferogram the flat-earth phase contribution, which is a signal contribution due to the curvature of the Earthâ€™s surface.\nThis is here done automatically through the SNAP software operators.\n\nIn general, the accuracy of interferometric measurements are influenced by many contributors that could result in a loss of coherence. But what is coherence? It is a measure of phase correlation between the master and slave image.\nInterferometric coherence (Î³) can be expressed as:Î³ = Î³_{proc}*Î³_{geom}*Î³_{vol}*Î³_{SNR}*Î³_{temp}\n\nwhere Î³_{proc} refers to inaccuracies in the processing (e.g., coregistration errors), Î³_{geom} refers to the baseline decorrelation (different position of satellites during the two acquisitions), Î³_{vol} refers to volume decorrelation (vegetation related), Î³_{SNR} refers to the radar instrument thermal noise and Î³_{temp} refers to the decorrelation caused by change of position of the objects in the scene during the time interval of the images acquisitions (e.g., plant growth, wind-induced movements or ground deformation due to earthquakes, landslides).\n\nTherefore, we can conclude that interferometric accuracy is sensitive to many processes, hence isolating the ground deformation signal involves several operations. On the other hand, interferometric coherence sensitivity could be exploited to track and map phenomena that cause its degradation (e.g., vegetation features, and water content).\n\ninterferogram_ds = cat.inter.read()\n\ncmap_hls_hex = sns.color_palette(\"hls\", n_colors=256).as_hex()\n\ninterferogram_ds = interferogram_ds.where(interferogram_ds != 0)\nigf_da = interferogram_ds.sel(band=1).band_data\ncoh_da = interferogram_ds.sel(band=2).band_data\n\n# Invert y axis\nigf_da[\"y\"] = igf_da.y[::-1]\ncoh_da[\"y\"] = coh_da.y[::-1]\n\nigf_plot = igf_da.hvplot.image(\n    x=\"x\",\n    y=\"y\",\n    cmap=cmap_hls_hex,\n    width=600,\n    height=600,\n    dynamic=False,\n)\n\ncoh_plot = coh_da.hvplot.image(\n    x=\"x\",\n    y=\"y\",\n    cmap=\"viridis\",\n    width=600,\n    height=600,\n    dynamic=False,\n).opts(clim=(0, 1))\n\n(igf_plot + coh_plot).opts(shared_axes=True)\n\nNow we can observe patterns that emerged between the two acquisitions. If you look at the data range in the interferogram (left plot), youâ€™ll notice it spans approximately one wavelength, from -\\pi to Ï€. On the right, you find a plot of the interferometric coherence (values ranging between 0 and 1), where low coherence is found along the ground surface ruptures caused by the earthquake.\nPlease note, that the interferogram has already undergone a deburst operation (all bursts merged into a single image).","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-7#interferogram-formation-and-coherence-estimation","position":7},{"hierarchy":{"lvl1":"Interferograms","lvl2":"Topographic Phase Removal"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-7#topographic-phase-removal","position":8},{"hierarchy":{"lvl1":"Interferograms","lvl2":"Topographic Phase Removal"},"content":"Since the local topography is an additional phase term constituting the interferogram that we built up so far, we need to make an estimate of its impact in order to further remove it to keep only the ground deformation-related phase. For this purpose, we use a reference known DEM to simulate an interferogram and to subtract it from the original interferogram.\n\ntopo_ds = cat.topo.read()\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nigf_da.plot(ax=axes[0], cmap=cmap_hls)\naxes[0].set_title(\"Interferogram With Topographic Phase\")\n\ntopo_ds.topo.plot(ax=axes[1], cmap=\"gist_earth\")\naxes[1].set_title(\"Topography\")\n\ntopo_ds.Phase.plot(ax=axes[2], cmap=cmap_hls)\naxes[2].set_title(\"Interferogram Without Topographic Phase\")\n\nplt.tight_layout()\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-7#topographic-phase-removal","position":9},{"hierarchy":{"lvl1":"Interferograms","lvl2":"Multi-looking, Goldstein Phase Filtering and Geocoding"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-7#multi-looking-goldstein-phase-filtering-and-geocoding","position":10},{"hierarchy":{"lvl1":"Interferograms","lvl2":"Multi-looking, Goldstein Phase Filtering and Geocoding"},"content":"In order to improve the phase signatures contained within our interferogram and get a generally higher signal-to-noise (SNR) ratio, we will perform two additional operations called multi-looking and Goldstein phase filtering.\nMulti-looking is the process of averaging adjacent pixels using a moving window of the interferogram to reduce noise (at the cost of reducing the spatial resolution). Coherence is involved in this operation to flag and set areas to no data that are considered unreliable (low coherence) and to keep the reliable ones (high coherence).\n\nFinally, to make data interpretable, we geocode the wrapped interferogram. So far we performed the interferometric processing in the radar geometry. The transformation into geographic coordinates will help us to perform further comparisons in a real-world coordinate system.\n\ngeocoded_ds = cat.geocode.read()\n\nstep = 4  # if you want to zoom in, suggestion is to make this step smaller\n\ngeocoded_ds = geocoded_ds.where(geocoded_ds != 0)\nigf_data = geocoded_ds.sel(band=1).band_data\ncoh_da = geocoded_ds.sel(band=2).band_data\n\nigf_plot = igf_data.isel(x=slice(0, -1, step), y=slice(0, -1, step)).hvplot.image(\n    x=\"x\", y=\"y\", cmap=cmap_hls_hex, width=600, height=600, dynamic=False\n)\n\ncoh_plot = (\n    coh_da.isel(x=slice(0, -1, step), y=slice(0, -1, step))\n    .hvplot.image(x=\"x\", y=\"y\", cmap=\"viridis\", width=600, height=600, dynamic=False)\n    .opts(clim=(0, 1))\n)\n\n(igf_plot + coh_plot).opts(shared_axes=True)\n\nIn the above plot, we can compare georeferenced data in the form of the interferogram (left) and the coherence (right). Along the earthquake fault line, low coherence between the two phase acquisitions is visible. This occurs due to extreme changes in terrain heights or displacements, which are beyond the sensitivity of the SAR sensor. This area of low coherence indicates higher uncertainty in the interferogram. However, this isnâ€™t necessarily a drawback, as it helps to clearly identify the earthquake epicenter.\n\nYou can also explore and zoom into regions with â€œfringe patternsâ€ to observe ground movement. Each fringe cycle (e.g., from red to red or blue to blue) corresponds to ground motion in this case. The fringe patterns indicate motion in the line-of-sight (LOS) of the satellite (Sentinel-1 has a mean incidence angle of 38Â°) in terms of either uplift (relative motion of the ground towards the satellite) or sinking (relative motion of the ground away from the satellite).\nIf the interferogram phase changes from 0 to -3.14 (cycles in the negative direction), the surface is moving away from the satellite (i.e., sinking movement). Vice versa, if cycles go in the positive direction (from 0 to +3.14), it would mean a relative uplifting movement of the ground.\nIn areas with no ground motion, fringe patterns disappear.\nThe radarâ€™s sensitivity to motion depends on its wavelength. For Sentinel-1 (~5.6cm), a full fringe cycle (2\\pi) represents a displacement of ~2.8 cm in the LOS direction.","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-7#multi-looking-goldstein-phase-filtering-and-geocoding","position":11},{"hierarchy":{"lvl1":"Interferograms","lvl2":"Visualisation of the Earthquake Event on July 5th, 2019"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-7#visualisation-of-the-earthquake-event-on-july-5th-2019","position":12},{"hierarchy":{"lvl1":"Interferograms","lvl2":"Visualisation of the Earthquake Event on July 5th, 2019"},"content":"\n\nstep = 4  # Downsample data for visualization\nigf_data_subset = igf_data.isel(x=slice(0, -1, step), y=slice(0, -1, step))\n\n\ndef array_to_img(data_array, cmap):\n    fig, ax = plt.subplots(figsize=(6, 6), dpi=600)\n    data_array.plot(ax=ax, cmap=cmap, add_colorbar=False, add_labels=False)\n    ax.set_axis_off()\n    buf = BytesIO()\n    plt.savefig(buf, format=\"png\", bbox_inches=\"tight\", pad_inches=0, transparent=True)\n    plt.close(fig)\n    return base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n\n\nigf_image = array_to_img(igf_data_subset, cmap=cmap_hls)\nbounds = [\n    [float(igf_data[\"y\"].min()), float(igf_data[\"x\"].min())],\n    [float(igf_data[\"y\"].max()), float(igf_data[\"x\"].max())],\n]\n\nm = folium.Map(\n    location=[float(igf_data[\"y\"].mean()), float(igf_data[\"x\"].mean())],\n    zoom_start=10,\n)\nfolium.TileLayer(\n    tiles=(\n        \"https://server.arcgisonline.com/ArcGIS/rest/\"\n        + \"services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"\n    ),\n    attr=\"Tiles Â© Esri\",\n    name=\"ESRI World Imagery\",\n).add_to(m)\nfolium.TileLayer(\n    tiles=(\n        \"https://server.arcgisonline.com/ArcGIS/rest/\"\n        + \"services/Reference/World_Boundaries_and_Places/\"\n        + \"MapServer/tile/{z}/{y}/{x}\"\n    ),\n    attr=\"Tiles Â© Esri\",\n    name=\"ESRI Labels\",\n    overlay=True,\n).add_to(m)\n\nfolium.raster_layers.ImageOverlay(\n    image=f\"data:image/png;base64,{igf_image}\",\n    bounds=bounds,\n    opacity=0.65,\n    name=\"Interferogram\",\n).add_to(m)\nfolium.LayerControl().add_to(m)\n\nm","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-7#visualisation-of-the-earthquake-event-on-july-5th-2019","position":13},{"hierarchy":{"lvl1":"Phase Unwrapping"},"type":"lvl1","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-8","position":0},{"hierarchy":{"lvl1":"Phase Unwrapping"},"content":"The goal of this notebook is to read an interferogram image (i.e., 2-D array of phase values) and unwrap it. Phase unwrapping is a critical process in interferometry, which involves recovering unambiguous phase data from the interferogram.\n\nA SAR interferogram represents the phase difference between two radar acquisitions (i.e., two SLC images). The phase difference is usually wrapped within a range of 0 to 2Ï€, because the phase is inherently cyclical. When the true phase difference exceeds 2Ï€, it gets â€œwrappedâ€ into this range, creating a discontinuous phase signal. Phase unwrapping refers to the process of reconstructing the continuous phase field from the wrapped phase data.\n\nUnwrapping an interferogram is essential for extracting correct information contained in the phase such as surface topography and earth surface deformations.\n\nThere are many approaches that tried to solve the unwrapping problem, tackling challenging scenarios involving noise or large phase discontinuities. Here we present the Network-flow Algorithm for phase unwrapping (C. W. Chen and H. A. Zebker, 2000), which is implemented in the snaphu package.","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-8","position":1},{"hierarchy":{"lvl1":"Phase Unwrapping","lvl2":"Loading Data"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-8#loading-data","position":2},{"hierarchy":{"lvl1":"Phase Unwrapping","lvl2":"Loading Data"},"content":"The data is stored on the Jupyterhub server, so we need to load it using their respective paths.\nIn this notebook we will use the resulting wrapped interferogram from notebook â€œInterferogramsâ€, but we need to process it in the radar geometry in order to unwrap it (while in notebook â€œInterferogramsâ€ we end the whole process by performing the geocoding, just for better visualization purposes).\n\n# Imports\nimport cmcrameri as cmc  # noqa: F401\nimport intake\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport snaphu\nimport xarray as xr\nfrom IPython.display import clear_output\n\nurl = \"https://huggingface.co/datasets/martinschobben/microwave-remote-sensing/resolve/main/microwave-remote-sensing.yml\"\ncat = intake.open_catalog(url)\nds = cat.complex.read().compute()\nds[\"cmplx\"] = ds[\"real\"] + ds[\"imag\"] * 1j\n\n# Set cyclic and linear colormaps\ncmap_cyc = sns.color_palette(\"hls\", as_cmap=True)  # \"cmc.romaO\"\ncmap_lin = \"cmc.roma_r\"\ncmap_disp = \"cmc.vik\"\n\n# Create a mask for the areas which have no data\nmask = ds.phase.where(ds.phase == 0, True, False).astype(bool)\n\nLetâ€™s start by displaying the interferogram that needs to be unwrapped. Recall that due to the Slant Range geometry and the satellite acquisition pass (ascending, in our case), the image appears north/south flipped (with respect to the geocoded image)!\n\n# Plot Phase Interferogram Image\nfig, axs = plt.subplots(figsize=(6, 6))\n\n(\n    ds.phase.where(mask)\n    .plot.imshow(cmap=cmap_cyc, zorder=1)\n    .axes.set_title(\"Phase Interferogram Image (Wrapped)\")\n)\nplt.show()\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-8#loading-data","position":3},{"hierarchy":{"lvl1":"Phase Unwrapping","lvl2":"Phase Unwrapping"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-8#phase-unwrapping","position":4},{"hierarchy":{"lvl1":"Phase Unwrapping","lvl2":"Phase Unwrapping"},"content":"As we will be doing the unwrapping multiple times in this notebook letâ€™s create a function that does the unwrapping for us on xarray DataArray objects.\nThe actual core function where the unwrapping is happening is snaphu.unwrap_phase from the snaphu package. This function needs a 2D numpy array as input, where each pixel value is a complex number.\nTherefore we have to convert the xarray DataArray to a 2D numpy array with complex values. We do that by combining the phase and intensity bands to a complex array.\nThe actual unwrapping is essentially an addition of the phase values, such that the values are continuous and not between -\\pi and Ï€.\n\nFigure 1: Illustration of how the unwrapping of the phase works. (Source: ESA).\n\ndef unwrap_array(\n    data: xr.DataArray,\n    complex_var: str = \"cmplx\",\n    ouput_var: str = \"unwrapped\",\n    mask: xr.DataArray = True,\n    coherence: xr.DataArray = None,\n    mask_nodata_value: int = 0,\n    coh_low_threshold: float = None,\n    coh_high_threshold: float = None,\n    nlooks=1.0,\n    cost=\"smooth\",\n    init=\"mcf\",\n    **kwargs,\n) -> xr.DataArray:\n    \"\"\"\n    Unwraps the phase data using the snaphu algorithm.\n\n    Parameters\n    ----------\n\n    data: xarray DataArray with complex numbers\n    complex_var: Name of the variable with the complex numbers\n    ouput_var: Name of the variable with the unwrapped phase\n    mask: xarray DataArray with mask values\n    coherence: xarray DataArray with coherence values (optional)\n    mask_nodata_value: Value of the no data pixels in the mask\n    coh_low_threshold: Lower threshold for the coherence values\n    coh_high_threshold: Higher threshold for the coherence values\n\n    Returns\n    ----------\n    xarray DataArray with the unwrapped phase\n    \"\"\"\n    # Get the complex data\n    data_arr = data[complex_var]\n\n    # Create a mask for areas with no data\n    if mask is True:\n        mask = (data_arr.real != mask_nodata_value).astype(bool)\n\n    # Apply coherence thresholds if provided\n    if coherence is not None:\n        if coh_low_threshold is not None:\n            coh_mask = (coherence >= coh_low_threshold).astype(bool)\n            mask = mask & coh_mask\n        if coh_high_threshold is not None:\n            coh_mask = (coherence <= coh_high_threshold).astype(bool)\n            mask = mask & coh_mask\n\n    # Apply the mask to the data\n    data_arr = data_arr.where(mask)\n\n    if coherence is None:\n        coherence = np.ones_like(data_arr.real)\n\n    # Unwrap the phase (already in complex form)\n    unw, _ = snaphu.unwrap(\n        data_arr,\n        coherence,\n        nlooks=nlooks,\n        cost=cost,\n        init=init,\n        mask=mask,\n        **kwargs,\n    )\n\n    # Clear the output to avoid printing the snaphu output\n    clear_output()\n\n    # Build xarray DataArray with the unwrapped phase\n    # unw_da = xr.DataArray(unw, coords=data.coords, dims=data.dims)\n    # data = data.to_dataset()\n    data[ouput_var] = ((\"y\", \"x\"), unw)\n\n    # Mask the unwrapped phase\n    # unw_da = unw_da.where(mask)\n    data[ouput_var] = data[ouput_var].where(mask)\n    return data\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-8#phase-unwrapping","position":5},{"hierarchy":{"lvl1":"Phase Unwrapping","lvl3":"Unwrapping on a Subset","lvl2":"Phase Unwrapping"},"type":"lvl3","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-8#unwrapping-on-a-subset","position":6},{"hierarchy":{"lvl1":"Phase Unwrapping","lvl3":"Unwrapping on a Subset","lvl2":"Phase Unwrapping"},"content":"As the original image is too large to unwrap in a reasonable time, we will unwrap a subset of the image. In this case, we will unwrap an area of 500x500 pixels.\n\n# Select a subset of the data\ndx, dy = 500, 500\nx0, y0 = 2800, 1700\n\n\ndef subsetting(ds, x0: int = 0, y0: int = 0, dx: int = 500, dy: int = 500):\n    return ds.isel(x=slice(x0, x0 + dx), y=slice(y0, y0 + dy))\n\n\n# Subsetting the data arrays\nsubset = subsetting(ds.where(mask), x0, y0, dx, dy)\n\n# Unwrap the subset\nsubset = unwrap_array(subset, complex_var=\"cmplx\", ouput_var=\"unwrapped\")\n\nNow letâ€™s compare the wrapped and unwrapped phase images.\n\nfig, axs = plt.subplots(1, 3, figsize=(14, 4))\n\n# Wrapped Phase\n\n(\n    subset.phase.plot.imshow(cmap=cmap_cyc, ax=axs[0]).axes.set_title(\n        \"Wrapped Phase of the Subset\"\n    )\n)\n\n# Unwrapped Phase\n(\n    subset.unwrapped.plot.imshow(\n        cmap=cmap_cyc, ax=axs[1], vmin=-80, vmax=80\n    ).axes.set_title(\"Unwrapped Phase of the Subset\")\n)\n\n# Subset inside the complete image\n(\n    ds.phase.where(mask)\n    .plot.imshow(cmap=cmap_cyc, zorder=1, ax=axs[2])\n    .axes.set_title(\"Complete Wrapped Phase Image\")\n)\n\nx_start = ds.phase.coords[\"x\"][x0].item()\ny_start = ds.phase.coords[\"y\"][y0].item()\nx_end = ds.phase.coords[\"x\"][x0 + dx].item()\ny_end = ds.phase.coords[\"y\"][y0 + dy].item()\n\nrect = patches.Rectangle(\n    (x_start, y_start),\n    x_end - x_start,\n    y_end - y_start,\n    linewidth=1,\n    edgecolor=\"r\",\n    facecolor=\"red\",\n    alpha=0.5,\n    label=\"Subset\",\n)\n\n# Add the rectangle to the plot\naxs[2].add_patch(rect)\naxs[2].legend()\nplt.tight_layout()\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-8#unwrapping-on-a-subset","position":7},{"hierarchy":{"lvl1":"Phase Unwrapping","lvl3":"Unwrapping with coherence mask","lvl2":"Phase Unwrapping"},"type":"lvl3","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-8#unwrapping-with-coherence-mask","position":8},{"hierarchy":{"lvl1":"Phase Unwrapping","lvl3":"Unwrapping with coherence mask","lvl2":"Phase Unwrapping"},"content":"Additionally, can we try to calculate the unwrapped image, where we are excluding pixels, where the coherence values are lower than a certain threshold. This is done by masking the coherence image with the threshold value and then unwrapping the phase image with the masked coherence image.\n\nthreshold1 = 0.3\nsubset = unwrap_array(\n    subset,\n    coherence=subset.coh,\n    coh_low_threshold=threshold1,\n    complex_var=\"cmplx\",\n    ouput_var=\"unwrapped_coh\",\n)\n\nLetâ€™s compare the unwrapped image with and without the coherence mask.\n\nfig, axs = plt.subplots(1, 2, figsize=(13, 5))\n(\n    subset.unwrapped_coh.plot.imshow(\n        cmap=cmap_cyc, ax=axs[0], vmin=-80, vmax=80\n    ).axes.set_title(f\"Unwrapped Phase with Coherence Threshold {threshold1}\")\n)\n\n(\n    subset.unwrapped.plot.imshow(\n        cmap=cmap_cyc, ax=axs[1], vmin=-80, vmax=80\n    ).axes.set_title(\"Unwrapped Phase without Coherence Threshold\")\n)\n\nplt.show()\n\nLetâ€™s see if another threshold value for the coherence mask gives better results.\n\nthreshold2 = 0.5\nsubset = unwrap_array(\n    subset,\n    coherence=subset.coh,\n    coh_low_threshold=threshold2,\n    complex_var=\"cmplx\",\n    ouput_var=\"unwrapped_coh2\",\n)\n\nfig, axs = plt.subplots(1, 2, figsize=(13, 5))\n(\n    subset.unwrapped_coh2.plot.imshow(\n        cmap=cmap_cyc, ax=axs[0], vmin=-80, vmax=80\n    ).axes.set_title(\"Coherence Threshold 0.5\")\n)\n\n(\n    subset.unwrapped_coh.plot.imshow(\n        cmap=cmap_cyc, ax=axs[1], vmin=-80, vmax=80\n    ).axes.set_title(\"Coherence Threshold 0.3\")\n)\nplt.show()\n\nA higher coherence threshold means that only pixels with a coherence value greater than 0.5 will be used for phase unwrapping. This would result in an unwrapping process that is likely more stable, with reduced noise (invalid phase information in the proximity of the earthquake faults is discarded). However, an excessive coherence threshold might have significant gaps or missing information, especially in areas where motion or surface changes have occurred.\nThe choice of a coherence threshold depends on the balance you want to strike between the accuracy and coverage of the output unwrapped image.\n\nKeep in mind that in case of large displacements, such as the Ridgecrest earthquake, phase unwrapping can be problematic and lead to poor results: when the displacement is large, the phase difference becomes wrapped multiple times, leading to phase aliasing. In this case, the phase values become ambiguous, we cannot distinguish between multiple phase wraps, thus leading to incorrect results.","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-8#unwrapping-with-coherence-mask","position":9},{"hierarchy":{"lvl1":"Phase Unwrapping","lvl2":"Applying an Equation for the Displacement Map"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-8#applying-an-equation-for-the-displacement-map","position":10},{"hierarchy":{"lvl1":"Phase Unwrapping","lvl2":"Applying an Equation for the Displacement Map"},"content":"From the unwrapped phase image (we will use the phase masked with a coherence threshold of 0.3) we can calculate the displacement map using the following equation:\n\n\nd = - \\frac{\\lambda}{4 \\cdot \\pi} \\cdot \\Delta \\phi_d\n\nwhere:\n\n\\lambda = 0.056 for Sentinel-1\n\n\\Delta \\phi_d is the unwrapped image\n\nThis operation can be very useful for monitoring ground deformation.\n\ndef displacement(unw, lambda_val: float = 0.056) -> xr.DataArray:\n    \"\"\"\n    Calculates the displacement from the unwrapped phase\n\n    Parameters\n    ----------\n\n    unw: xarray DataArray with the unwrapped phase\n    unw: xr.DataArray\n    lambda_val: Wavelength of the radar signal\n    lambda_val: float\n\n    Returns\n    -------\n    xarray DataArray with the displacement\n    \"\"\"\n    disp = unw * -lambda_val / (4 * np.pi)\n    return disp\n\n\n# Calculate the displacement\ndisp_subset = displacement(subset.unwrapped_coh)\n\n# Plot the displacement map\n(\n    disp_subset.plot.imshow(\n        cmap=cmap_disp, cbar_kwargs={\"label\": \"Meters [m]\"}\n    ).axes.set_title(\"Displacement Map of the Subset\")\n)\nplt.show()\n\n","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-8#applying-an-equation-for-the-displacement-map","position":11},{"hierarchy":{"lvl1":"Phase Unwrapping","lvl2":"Coarsen Approach"},"type":"lvl2","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-8#coarsen-approach","position":12},{"hierarchy":{"lvl1":"Phase Unwrapping","lvl2":"Coarsen Approach"},"content":"As the whole data is too large and the processing time already exceeds 20 minutes when using an image with 4000x4000 pixels, we can coarsen the image so that we can unwrap and compute the displacement for the whole scene.\n\nkernel_size = 3\nlowres = ds.coarsen(x=kernel_size, y=kernel_size, boundary=\"trim\").median()\n\nlowres = unwrap_array(\n    lowres,\n    ntiles=(20, 30),\n    tile_overlap=10,\n    coherence=lowres.coh,\n    coh_low_threshold=0.3,\n    complex_var=\"cmplx\",\n    ouput_var=\"unwrapped\",\n)\n\nWe can now plot the unwrapped image of the low resolution image.\n\n# Plot the unwrapped phase\n(\n    lowres.unwrapped.plot.imshow(cmap=cmap_cyc).axes.set_title(\n        \"Unwrapped Phase entire scene (coarsened)\"\n    )\n)\nplt.show()\n\nWe can also now calculate the displacement map and compare them.\n\nlowres_disp = displacement(lowres.unwrapped)\n\n# Plot the displacement map\n(\n    lowres_disp.plot.imshow(\n        cmap=cmap_disp, cbar_kwargs={\"label\": \"Meters [m]\"}\n    ).axes.set_title(\"Displacement Map entire scene (coarse resolution)\")\n)\nplt.show()\n\nPlot a summary of the previous plots:\n\n# Plot summary of previous plots\nfig, axs = plt.subplots(2, 2, figsize=(12, 10))\nax = axs.ravel()\n\n(\n    subset.unwrapped_coh.plot.imshow(\n        cmap=cmap_cyc, ax=ax[0], vmin=-80, vmax=80\n    ).axes.set_title(\"Unwrapped Phase of the subset with Coherence Threshold 0.3\")\n)\n\n(\n    disp_subset.plot.imshow(\n        cmap=cmap_disp, ax=ax[1], cbar_kwargs={\"label\": \"Meters [m]\"}\n    ).axes.set_title(\"Displacement Map of the Subset\")\n)\n\n(\n    lowres.unwrapped.plot.imshow(cmap=cmap_cyc, ax=ax[2]).axes.set_title(\n        \"Unwrapped Phase of the entire scene with Coherence Threshold 0.3 (coarsened)\"\n    )\n)\n\n(\n    lowres_disp.plot.imshow(\n        cmap=cmap_disp, ax=ax[3], cbar_kwargs={\"label\": \"Meters [m]\"}\n    ).axes.set_title(\"Displacement Map entire scene (coarse resolution)\")\n)\n\nplt.tight_layout()\n\nIn the following animation, we can capture the 3D displacement caused by the Ridgecrest quake by observing the after and before elevation model.\n\n\nCredits: NASA","type":"content","url":"/notebooks/courses/microwave-remote-sensing/in-class-exercise-8#coarsen-approach","position":13},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1},{"hierarchy":{"lvl1":"References"},"type":"lvl1","url":"/notebooks/references","position":0},{"hierarchy":{"lvl1":"References"},"content":"","type":"content","url":"/notebooks/references","position":1},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery"},"type":"lvl1","url":"/notebooks/templates/classification","position":0},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery"},"content":"Finding forests with satellite imagery","type":"content","url":"/notebooks/templates/classification","position":1},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl2":"Data Acquisition"},"type":"lvl2","url":"/notebooks/templates/classification#data-acquisition","position":2},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl2":"Data Acquisition"},"content":"In this chapter, we will employ machine learning techniques to classify a scene using satellite imagery. Specifically, we will utilize scikit-learn to implement two distinct classifiers and subsequently compare their results. To begin, we need to import the following modules.\n\nfrom datetime import datetime, timedelta\n\nimport cmcrameri as cmc  # noqa: F401\nimport geopandas as gpd\nimport matplotlib.colors as colors\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport odc.stac\nimport pandas as pd\nimport pystac_client\nimport rioxarray  # noqa: F401\nimport xarray as xr\nfrom odc.geo.geobox import GeoBox\nfrom shapely.geometry import Polygon\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n# Scikit Learn\nfrom sklearn.naive_bayes import GaussianNB\n\nBefore we start, we need to load the data. We will use odc-stac to obtain data from Earth Search by Element 84. Here we define the area of interest and the time frame, aswell as the EPSG code and the resolution.","type":"content","url":"/notebooks/templates/classification#data-acquisition","position":3},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"Searching in the Catalog","lvl2":"Data Acquisition"},"type":"lvl3","url":"/notebooks/templates/classification#searching-in-the-catalog","position":4},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"Searching in the Catalog","lvl2":"Data Acquisition"},"content":"The module odc-stac provides access to free, open source satelite data. To retrieve the data, we must define  several parameters that specify the location and time period for the satellite data. Additionally, we must specify the data collection we wish to access, as multiple collections are available. In this example, we will use multispectral imagery from the Sentinel-2 satellite.\n\ndx = 0.0006  # 60m resolution\nepsg = 4326\n\n# Set Spatial extent\nlatmin, latmax = 47.86, 48.407\nlonmin, lonmax = 16.32, 16.9\nbounds = (lonmin, latmin, lonmax, latmax)\n\n\n# Set Temporal extent\nstart_date = datetime(year=2024, month=5, day=1)\nend_date = start_date + timedelta(days=10)\n\ntime_format = \"%Y-%m-%d\"\ndate_query = start_date.strftime(time_format) + \"/\" + end_date.strftime(time_format)\n\n# Search for Sentinel-2 data\nitems = (\n    pystac_client.Client.open(\"https://earth-search.aws.element84.com/v1\")\n    .search(\n        bbox=bounds,\n        collections=[\"sentinel-2-l2a\"],\n        datetime=date_query,\n        limit=100,\n    )\n    .item_collection()\n)\nprint(len(items), \"scenes found\")\n\nWe will now focus on the area south-east of Vienna, where the Nationalpark Donauauen is situated. The time frame we are interested in is the beginning of May 2024.\nAfter passing these parameters to the stac-catalog we have found 10 scenes that we can use for our analysis.","type":"content","url":"/notebooks/templates/classification#searching-in-the-catalog","position":5},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"Loading the Data","lvl2":"Data Acquisition"},"type":"lvl3","url":"/notebooks/templates/classification#loading-the-data","position":6},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"Loading the Data","lvl2":"Data Acquisition"},"content":"Now we will load the data directly into an xarray dataset, which we can use to perform computations on the data. xarray is a powerful library for working with multi-dimensional arrays, making it well-suited for handling satellite data.\n\nHereâ€™s how we can load the data using odc-stac and xarray:\n\n# define a geobox for my region\ngeobox = GeoBox.from_bbox(bounds, crs=f\"epsg:{epsg}\", resolution=dx)\n\n# lazily combine items into a datacube\ndc = odc.stac.load(\n    items,\n    bands=[\"scl\", \"red\", \"green\", \"blue\", \"nir\"],\n    chunks={\"time\": 5, \"x\": 600, \"y\": 600},\n    geobox=geobox,\n    resampling=\"bilinear\",\n)\ndc\n\n","type":"content","url":"/notebooks/templates/classification#loading-the-data","position":7},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl2":"Data Visualization"},"type":"lvl2","url":"/notebooks/templates/classification#data-visualization","position":8},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl2":"Data Visualization"},"content":"","type":"content","url":"/notebooks/templates/classification#data-visualization","position":9},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"RGB Image","lvl2":"Data Visualization"},"type":"lvl3","url":"/notebooks/templates/classification#rgb-image","position":10},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"RGB Image","lvl2":"Data Visualization"},"content":"With the image data now in our possession, we can proceed with computations and visualizations.\n\nFirst, we define a mask to exclude cloud cover and areas with missing data. Subsequently, we create a composite median image, where each pixel value represents the median value across all the scenes we have identified. This approach helps to eliminate clouds and outliers present in some of the images, thereby providing a clearer and more representative visualization of the scene.\n\n# define a mask for valid pixels (non-cloud)\n\n\ndef is_valid_pixel(data):\n    # include only vegetated, not_vegitated, water, and snow\n    return ((data > 3) & (data < 7)) | (data == 11)\n\n\ndc[\"valid\"] = is_valid_pixel(dc.scl)\n\n# compute the masked median\nrgb_median = (\n    dc[[\"red\", \"green\", \"blue\"]]\n    .where(dc.valid)\n    .to_dataarray(dim=\"band\")\n    .median(dim=\"time\")\n    .astype(int)\n)\n\n# plot the median composite\ntitle_rgb = (\n    \"RGB - Median Composite\"\n    + f\"\\n{start_date.strftime('%d.%m.%Y')} - {end_date.strftime('%d.%m.%Y')}\"\n)\nrgb_median.plot.imshow(robust=True).axes.set_title(title_rgb)\nplt.show()\n\n","type":"content","url":"/notebooks/templates/classification#rgb-image","position":11},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"False Color Image","lvl2":"Data Visualization"},"type":"lvl3","url":"/notebooks/templates/classification#false-color-image","position":12},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"False Color Image","lvl2":"Data Visualization"},"content":"In addition to the regular RGB Image, we can swap any of the bands from the visible spectrum with any other bands. In this specific case the red band has been changed to the near infrared band. This allows us to see vegetated areas more clearly, since they now appear in a bright red color. This is due to the fact that plants absorb regular red light while reflecting near infrared light \n\nNASA, 2020.\n\n# compute a false color image\n# near infrared instead of red\nfc_median = (\n    dc[[\"nir\", \"green\", \"blue\"]]\n    .where(dc.valid)\n    .to_dataarray(dim=\"band\")\n    .transpose(..., \"band\")\n    .median(dim=\"time\")\n    .astype(int)\n)\n\ntitle_fc = (\n    \"False color - Median Composite\"\n    + f\"\\n{start_date.strftime('%d.%m.%Y')} - {end_date.strftime('%d.%m.%Y')}\"\n)\nfc_median.plot.imshow(robust=True).axes.set_title(title_fc)\nplt.show()\n\n","type":"content","url":"/notebooks/templates/classification#false-color-image","position":13},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"NDVI Image","lvl2":"Data Visualization"},"type":"lvl3","url":"/notebooks/templates/classification#ndvi-image","position":14},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"NDVI Image","lvl2":"Data Visualization"},"content":"To get an first impression of the data, we can calculate the NDVI (Normalized Difference Vegetation Index) and plot it. The NDVI is calculated by useing the following formula. \n\nRouse et al., 1974NDVI = \\frac{NIR - Red}{NIR + Red}\n\nThis gives us a good overview of the vegetation in the area. The values can range from -1 to 1 where the following meanings are associated with these values:\n\n-1 to 0 indicate dead plants or inanimate objects\n\n0 to 0.33 are unhealthy plants\n\n0.33 to 0.66 are moderatly healthy plants\n\n0.66 to 1 are very healthy plants\n\n# Normalized Difference Vegetation Index (NDVI)\n\n\ndef normalized_difference(a, b):\n    return (a - b * 1.0) / (a + b)\n\n\nndvi = normalized_difference(dc.nir, dc.red)\nndvi.median(dim=\"time\").plot.imshow(cmap=\"cmc.cork\", vmin=-1, vmax=1).axes.set_title(\n    \"NDVI\"\n)\nplt.show()\n\n","type":"content","url":"/notebooks/templates/classification#ndvi-image","position":15},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl2":"Classification"},"type":"lvl2","url":"/notebooks/templates/classification#classification","position":16},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl2":"Classification"},"content":"In this chapter, we will classify the satellite data to identify forested areas within the scene. By using supervised machine learning techniques, we can train classifiers to distinguish between forested and non-forested regions based on the training data we provide. We will explore two different classifiers and compare their performance in accurately identifying forest areas.","type":"content","url":"/notebooks/templates/classification#classification","position":17},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"Regions of Interest","lvl2":"Classification"},"type":"lvl3","url":"/notebooks/templates/classification#regions-of-interest","position":18},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"Regions of Interest","lvl2":"Classification"},"content":"Since this is a supervised classification, we need to have some training data. Therefore we need to define areas or regions, which we are certain represent the feature which we are classifiying. In this case we are interested in forested areas and regions that are definitly not forested. These regions will be used to train our classifiers.\n\n# Define Polygons\nforest_areas = {\n    0: [\n        Polygon(\n            [\n                (16.482772, 47.901753),\n                (16.465133, 47.870124),\n                (16.510142, 47.874382),\n                (16.482772, 47.901753),\n            ]\n        )\n    ],\n    1: [\n        Polygon(\n            [\n                (16.594079, 47.938855),\n                (16.581914, 47.894454),\n                (16.620233, 47.910268),\n                (16.594079, 47.938855),\n            ]\n        )\n    ],\n    2: [\n        Polygon(\n            [\n                (16.67984, 47.978998),\n                (16.637263, 47.971091),\n                (16.660376, 47.929123),\n                (16.67984, 47.978998),\n            ]\n        )\n    ],\n    3: [\n        Polygon(\n            [\n                (16.756477, 48.000286),\n                (16.723024, 47.983256),\n                (16.739446, 47.972916),\n                (16.756477, 48.000286),\n            ]\n        )\n    ],\n    4: [\n        Polygon(\n            [\n                (16.80696, 48.135923),\n                (16.780806, 48.125583),\n                (16.798445, 48.115243),\n                (16.80696, 48.135923),\n            ]\n        )\n    ],\n    5: [\n        Polygon(\n            [\n                (16.684097, 48.144438),\n                (16.664634, 48.124366),\n                (16.690788, 48.118892),\n                (16.684097, 48.144438),\n            ]\n        )\n    ],\n    6: [\n        Polygon(\n            [\n                (16.550894, 48.169984),\n                (16.530822, 48.165118),\n                (16.558801, 48.137139),\n                (16.550894, 48.169984),\n            ]\n        )\n    ],\n    7: [\n        Polygon(\n            [\n                (16.588604, 48.402329),\n                (16.556976, 48.401112),\n                (16.580697, 48.382865),\n                (16.588604, 48.402329),\n            ]\n        )\n    ],\n}\n\nnonforest_areas = {\n    0: [\n        Polygon(\n            [\n                (16.674974, 48.269126),\n                (16.623882, 48.236281),\n                (16.682272, 48.213168),\n                (16.674974, 48.269126),\n            ]\n        )\n    ],\n    1: [\n        Polygon(\n            [\n                (16.375723, 48.228374),\n                (16.357476, 48.188839),\n                (16.399444, 48.185798),\n                (16.375723, 48.228374),\n            ]\n        )\n    ],\n    2: [\n        Polygon(\n            [\n                (16.457834, 48.26426),\n                (16.418907, 48.267301),\n                (16.440804, 48.23324),\n                (16.457834, 48.26426),\n            ]\n        )\n    ],\n    3: [\n        Polygon(\n            [\n                (16.519266, 48.101861),\n                (16.470607, 48.100645),\n                (16.500411, 48.07145),\n                (16.519266, 48.101861),\n            ]\n        )\n    ],\n    4: [\n        Polygon(\n            [\n                (16.453577, 48.051986),\n                (16.412217, 48.067192),\n                (16.425598, 48.012451),\n                (16.453577, 48.051986),\n            ]\n        )\n    ],\n}\n\n# Geoppandas Dataframe from Polygons\nforest_df = gpd.GeoDataFrame(\n    {\"geometry\": [poly[0] for poly in forest_areas.values()]}, crs=\"EPSG:4326\"\n)\nnonforest_df = gpd.GeoDataFrame(\n    {\"geometry\": [poly[0] for poly in nonforest_areas.values()]},\n    crs=\"EPSG:4326\",\n)\n\n\n# Plotting Regions of Interest\nfig, ax = plt.subplots()\nrgb_median.plot.imshow(ax=ax, robust=True)\nforest_df.plot(ax=ax, ec=\"C0\", fc=\"none\")\nnonforest_df.plot(ax=ax, ec=\"C1\", fc=\"none\")\nax.set_title(\"Regions of Interest\")\nax.set_aspect(\"equal\")\nplt.show()\n\n","type":"content","url":"/notebooks/templates/classification#regions-of-interest","position":19},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"Data Preparation","lvl2":"Classification"},"type":"lvl3","url":"/notebooks/templates/classification#data-preparation","position":20},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"Data Preparation","lvl2":"Classification"},"content":"In addition to the Regions of Interest we will extract the specific bands from the loaded dataset that we intend to use for the classification, which are the red, green, blue and near-infrared bands, although other bands can also be utilized. Using these bands, we will create both a training and a testing dataset. The training dataset will be used to train the classifier, while the testing dataset will be employed to evaluate its performance.\n\n# Classifiying dataset (only necessary bands)\nbands = [\"red\", \"green\", \"blue\", \"nir\"]\nds_class = dc[bands].where(dc.valid).median(dim=\"time\")\nds_class = ds_class.fillna(0)\n\n\ndef clip_array(ds: xr.Dataset, polygons):\n    clipped = ds.rio.clip(polygons, invert=False, all_touched=False, drop=True)\n    clipped_nan = clipped.where(clipped == ds)\n    return clipped_nan\n\n\n# Dictionaries with Dataarrays, each clipped by a Polygon\ndata_dict_feat = {\n    idx: clip_array(ds_class, polygon) for idx, polygon in forest_areas.items()\n}\ndata_dict_nonfeat = {\n    idx: clip_array(ds_class, polygon) for idx, polygon in nonforest_areas.items()\n}\n\n# Reshape the polygon dataarrays to get a tuple (one value per band) of pixel values\nfeat_data = [\n    xarray.to_array().values.reshape(len(bands), -1).T\n    for xarray in data_dict_feat.values()\n]  # replaced median_data_dict_feat with data_dict_feat\nnonfeat_data = [\n    xarray.to_array().values.reshape(len(bands), -1).T\n    for xarray in data_dict_nonfeat.values()\n]  # replaced median_data_dict_feat with data_dict_feat\n\n# The rows of the different polygons are concatenated to a single array for further processing\nfeat_values = np.concatenate(feat_data)\nnonfeat_values = np.concatenate(nonfeat_data)\n\n# Drop Nan Values\nX_feat_data = feat_values[~np.isnan(feat_values).any(axis=1)]\nX_nonfeat_data = nonfeat_values[~np.isnan(nonfeat_values).any(axis=1)]\n\n# Creating Output Vector (1 for pixel is features; 0 for pixel is not feature)\ny_feat_data = np.ones(X_feat_data.shape[0])\ny_nonfeat_data = np.zeros(X_nonfeat_data.shape[0])\n\n# Concatenate all Classes for training\nX = np.concatenate([X_feat_data, X_nonfeat_data])\ny = np.concatenate([y_feat_data, y_nonfeat_data])\n\n# Split into Training and Testing Data.\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=42\n)\n\nNow that we have prepared the training and testing data, we will create an image array of the actual scene that we intend to classify. This array will serve as the input for our classification algorithms, allowing us to apply the trained classifiers to the entire scene and identify the forested and non-forested areas accurately.\n\nimage_data = (\n    ds_class[bands].to_array(dim=\"band\").transpose(\"latitude\", \"longitude\", \"band\")\n)\n\n# Reshape the image data\nnum_of_pixels = ds_class.sizes[\"longitude\"] * ds_class.sizes[\"latitude\"]\nnum_of_bands = len(bands)\nX_image_data = image_data.values.reshape(num_of_pixels, num_of_bands)\n\n","type":"content","url":"/notebooks/templates/classification#data-preparation","position":21},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"Classifiying with Naive Bayes","lvl2":"Classification"},"type":"lvl3","url":"/notebooks/templates/classification#classifiying-with-naive-bayes","position":22},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"Classifiying with Naive Bayes","lvl2":"Classification"},"content":"Now that we have prepared all the needed data, we can begin the actual classification process.\n\nWe will start with a Naive Bayes classifier. First, we will train the classifier using our training dataset. Once trained, we will apply the classifier to the actual image to identify the forested and non-forested areas.\n\n# Naive Bayes initialization and training\nnb = GaussianNB()\nnb_test = nb.fit(X_train, y_train)\nnb_predict = nb.predict(X_test)\n\n# Prediction on image\nnb_predict_img = nb.predict(X_image_data)\nnb_predict_img = nb_predict_img.reshape(\n    ds_class.sizes[\"latitude\"], ds_class.sizes[\"longitude\"]\n)\n\n# Adding the Naive Bayes Prediction to the dataset\nds_class[\"NB-forest\"] = xr.DataArray(\n    nb_predict_img,\n    dims=[\"latitude\", \"longitude\"],\n    coords={\n        \"longitude\": ds_class[\"longitude\"],\n        \"latitude\": ds_class[\"latitude\"],\n    },\n)\n\nTo evaluate the effectiveness of the classification, we will plot the image predicted by the classifier. Additionally, we will examine the Classification Report and the Confusion Matrix to gain further insights into the classifierâ€™s performance.\n\n# Plot Naive Bayes\nalpha = 1\ncmap_green = colors.ListedColormap([(1, 1, 1, alpha), \"green\"])\n\nplot = ds_class[\"NB-forest\"].plot.imshow(\n    cmap=cmap_green, cbar_kwargs={\"ticks\": [0.25, 0.75]}\n)\ncbar = plot.colorbar\ncbar.set_ticklabels([\"non-forest\", \"forest\"])\nplot.axes.set_title(\"Naive Bayes Classification\")\nplt.show()\n\n# Print the Classification report\nprint(\"NAIVE BAYES: \\n \" + classification_report(y_test, nb_predict))\n\n# Print the confusion matrix\ncon_mat_nb = pd.DataFrame(\n    confusion_matrix(y_test, nb_predict),\n    index=[\"Actual Negative\", \"Actual Positive\"],\n    columns=[\"Predicted Negative\", \"Predicted Positive\"],\n)\ndisplay(con_mat_nb)\n\n","type":"content","url":"/notebooks/templates/classification#classifiying-with-naive-bayes","position":23},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"Classifiying with Random Forest","lvl2":"Classification"},"type":"lvl3","url":"/notebooks/templates/classification#classifiying-with-random-forest","position":24},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"Classifiying with Random Forest","lvl2":"Classification"},"content":"To ensure our results are robust, we will explore an additional classifier. In this section, we will use the Random Forest classifier. The procedure for using this classifier is the same as before: we will train the classifier using our training dataset and then apply it to the actual image to classify the scene.\n\n# Random Forest initialization and training\nrf = RandomForestClassifier(n_estimators=100)\nrf_test = rf.fit(X_train, y_train)\nrf_predict = rf.predict(X_test)\n\n# Prediction on image\nrf_predict_img = rf.predict(X_image_data)\nrf_predict_img = rf_predict_img.reshape(\n    ds_class.sizes[\"latitude\"], ds_class.sizes[\"longitude\"]\n)\n\n# Adding the Random Forest Prediction to the dataset\nds_class[\"RF-forest\"] = xr.DataArray(\n    rf_predict_img,\n    dims=[\"latitude\", \"longitude\"],\n    coords={\n        \"longitude\": ds_class[\"longitude\"],\n        \"latitude\": ds_class[\"latitude\"],\n    },\n)\n\nplot = ds_class[\"RF-forest\"].plot.imshow(\n    cmap=cmap_green, cbar_kwargs={\"ticks\": [0.25, 0.75]}\n)\ncbar = plot.colorbar\ncbar.set_ticklabels([\"non-forest\", \"forest\"])\nplot.axes.set_title(\"Random Forest Classification\")\nplt.show()\n\n# Print the Classification report\nprint(\"RANDOM FOREST: \\n \" + classification_report(y_test, rf_predict))\n\n# Print the confusion matrix\ncon_mat_rf = pd.DataFrame(\n    confusion_matrix(y_test, rf_predict),\n    index=[\"Actual Negative\", \"Actual Positive\"],\n    columns=[\"Predicted Negative\", \"Predicted Positive\"],\n)\ndisplay(con_mat_rf)\n\nWe can already see from the classification reports and the confusion matrices that the Random Forest classifier has outperformed the Naive Bayes classifier. This is particularly evident from the lower values in the secondary diagonal, indicating minimal False Positives and False Negatives. It appears that the Naive Bayes classifier is more sensitive to False Positives, resulting in a higher rate of incorrect classifications.","type":"content","url":"/notebooks/templates/classification#classifiying-with-random-forest","position":25},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"Comparison of the Classificators","lvl2":"Classification"},"type":"lvl3","url":"/notebooks/templates/classification#comparison-of-the-classificators","position":26},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl3":"Comparison of the Classificators","lvl2":"Classification"},"content":"To gain a more in-depth understanding of the classifiersâ€™ performance, we will compare their results. Specifically, we will identify the areas where both classifiers agree and the areas where they disagree. This comparison will provide valuable insights into the strengths and weaknesses of each classifier, allowing us to better assess their effectiveness in identifying forested and non-forested regions.\n\ncmap_trio = colors.ListedColormap([\"whitesmoke\", \"indianred\", \"goldenrod\", \"darkgreen\"])\n\n\ndouble_clf = ds_class[\"NB-forest\"] + 2 * ds_class[\"RF-forest\"]\n\nfig, ax = plt.subplots()\ncax = ax.imshow(double_clf, cmap=cmap_trio, interpolation=\"none\")\n\n# Add a colorbar with custom tick labels\ncbar = fig.colorbar(cax, ticks=[1 * 0.375, 3 * 0.375, 5 * 0.375, 7 * 0.375])\ncbar.ax.set_yticklabels([\"None\", \"Naive Bayes\", \"Random Forest\", \"Both\"])\nax.set_title(\"Classification Comparisson\")\nax.set_axis_off()\nplt.show()\n\nThe areas where both classifiers agree include the larger forested regions, such as the Nationalpark Donau-Auen and the Leithagebirge. Additionally, both classifiers accurately identified the urban areas of Vienna and correctly excluded them from being classified as forested.\n\n# Plot only one class, either None (0), Naive Bayes (1), Random Forest (2), or Both (3)\nfig, axs = plt.subplots(2, 2, figsize=(8, 8))\nax = axs.ravel()\n\nfor i in range(4):\n    ax[i].imshow(double_clf == i, cmap=\"cmc.oleron_r\", interpolation=\"none\")\n    category = [\n        \"by None\",\n        \"only by Naive Bayes\",\n        \"only by Random Forest\",\n        \"by Both\",\n    ][i]\n    title = \"Areas classified \" + category\n    ax[i].set_title(title)\n    ax[i].set_axis_off()\n\nplt.tight_layout()\n\nWhen plotting the classified areas individually, we observe that the Random Forest classifier mistakenly identified the Danube River as a forested area. Conversely, the Naive Bayes classifier erroneously classified a significant amount of cropland as forest.\n\nFinally, by analyzing the proportion of forested areas within the scene, we find that approximately 18% of the area is classified as forest, while around 66% is classified as non-forest. The remaining areas, which include water bodies and cropland, fall into less clearly defined categories.\n\nThe accompanying bar chart illustrates the distribution of these classifications, highlighting the percentage of forested areas, non-forested areas, and regions classified by only one of the two classifiers. This visual representation helps to quantify the areas of agreement and disagreement between the classifiers, providing a clearer picture of their performance.\n\ncounts = {}\nfor num in range(0, 4):\n    num_2_class = {0: \"None\", 1: \"Naive Bayes\", 2: \"Random Forest\", 3: \"Both\"}\n    counts[num_2_class[num]] = int((double_clf == num).sum().values)\n\nclass_counts_df = pd.DataFrame(list(counts.items()), columns=[\"Class\", \"Count\"])\nclass_counts_df[\"Percentage\"] = (\n    class_counts_df[\"Count\"] / class_counts_df[\"Count\"].sum()\n) * 100\nax = class_counts_df.plot.bar(\n    x=\"Class\",\n    y=\"Percentage\",\n    rot=0,\n    color=\"darkgreen\",\n    ylim=(0, 100),\n    title=\"Classified Areas per Classificator (%)\",\n)\n\n# Annotate the bars with the percentage values\nfor p in ax.patches:\n    ax.annotate(\n        f\"{p.get_height():.1f}%\",\n        (p.get_x() + p.get_width() / 2.0, p.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        xytext=(0, 9),\n        textcoords=\"offset points\",\n    )\n\n","type":"content","url":"/notebooks/templates/classification#comparison-of-the-classificators","position":27},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl2":"Conclusion"},"type":"lvl2","url":"/notebooks/templates/classification#conclusion","position":28},{"hierarchy":{"lvl1":"Classification of Sentinel-2 imagery","lvl2":"Conclusion"},"content":"In this chapter, we utilized machine learning to classify satellite imagery into forested and non-forested areas, comparing Naive Bayes and Random Forest classifiers. The Random Forest classifier generally outperformed Naive Bayes, with fewer errors in classification, although it misclassified the Danube River as forested, while Naive Bayes incorrectly identified cropland as forest. The analysis, supported by the bar chart, revealed that about 18% of the scene was classified as forest, 66% as non-forest, and the remainder included ambiguous categories. This comparison highlights the strengths and limitations of each classifier, underscoring the need for careful selection and evaluation of classification methods.","type":"content","url":"/notebooks/templates/classification#conclusion","position":29},{"hierarchy":{"lvl1":"Templates"},"type":"lvl1","url":"/notebooks/templates/prereqs-templates","position":0},{"hierarchy":{"lvl1":"Templates"},"content":"This section of the Cookbook covers a wide range of topics. The intent is to\ncreate templates to showcase workflows that can be used by students as a primer\nfor independent research projects.\n\nConcepts\n\nImportance\n\nNotes\n\nIntro to xarray\n\nNecessary\n\n\n\nDask Arrays\n\nNecessary\n\n\n\nDocumentation scikit-learn\n\nNeccesary\n\nMachine Learning in Python\n\nDocumentation Matplotlib\n\nHelpful\n\nPloting in Python\n\nDocumentation odc-stac\n\nHelpful\n\nData access\n\nTime to learn: 10 min","type":"content","url":"/notebooks/templates/prereqs-templates","position":1},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection"},"type":"lvl1","url":"/notebooks/tutorials/floodmapping","position":0},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection"},"content":"How an 275 year old idea helps map the extent of floods\n\nNote\n\nThis notebook contains interactive elements. The full interactive elements can only be viewed on Binder by clicking on the Binder badge or ðŸš€ button.\n\nThis notebook explains how microwave (\\sigma^0) backscattering can be used to map the extent of a flood. We replicate in this exercise the work of \n\nBauer-Marschallinger et al., 2022 on the TU Wien Bayesian-based flood mapping algorithm.\n\nimport datetime\n\nimport holoviews as hv\nimport hvplot.pandas\nimport hvplot.xarray\nimport numpy as np\nimport pandas as pd\nimport panel as pn\nimport pystac_client\nimport rioxarray  # noqa: F401\nimport xarray as xr\nfrom bokeh.models import FixedTicker\nfrom odc import stac as odc_stac\nfrom scipy.stats import norm\n\npn.extension()\nhv.extension(\"bokeh\")\n\n","type":"content","url":"/notebooks/tutorials/floodmapping","position":1},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl2":"Greece Flooding 2018"},"type":"lvl2","url":"/notebooks/tutorials/floodmapping#greece-flooding-2018","position":2},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl2":"Greece Flooding 2018"},"content":"In this exercise we will replicate the case study of the above mentioned paper, the February 2018 flooding of the Greek region of Thessaly.\n\ntime_range = \"2018-02-28T04:00:00Z/2018-02-28T05:00:00Z\"\nminlon, maxlon = 21.93, 22.23\nminlat, maxlat = 39.47, 39.64\nbounding_box = [minlon, minlat, maxlon, maxlat]\n\n","type":"content","url":"/notebooks/tutorials/floodmapping#greece-flooding-2018","position":3},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl2":"EODC STAC Catalog"},"type":"lvl2","url":"/notebooks/tutorials/floodmapping#eodc-stac-catalog","position":4},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl2":"EODC STAC Catalog"},"content":"The data required for TU Wien flood mapping algorithm consists of terrain corrected sigma naught backscatter data \\sigma^{0}, the projected local incidence angle (PLIA) values of those measurements, and the harmonic parameters (HPAR) of a model fit on the pixelâ€™s backscatter time series. The latter two datasets will needed to calculate the probability density functions over land and water for. We will be getting the required data from the EODC STAC Catalog. Specifically the collections: SENTINEL_SIG0_20M, SENTINEL1_MPLIA and SENTINEL1_HPAR. We use the pystac-client and odc_stac packages to, respectively, discover and fetch the data.\n\nDue to the way the data is acquired and stored, some items include â€œno dataâ€ areas. In our case, no data has the value -9999, but this can vary from data provider to data provider. This information can usually be found in the metadata. Furthermore, to save memory, data is often stored as integer (e.g. 25) and not in float (e.g. 2.5) format. For this reason, the backscatter values are often multiplied by a scale factor. Hence we define the function post_process_eodc_cube to correct for these factors as obtained from the STAC metadata.","type":"content","url":"/notebooks/tutorials/floodmapping#eodc-stac-catalog","position":5},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl3":"Sigma naught","lvl2":"EODC STAC Catalog"},"type":"lvl3","url":"/notebooks/tutorials/floodmapping#sigma-naught","position":6},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl3":"Sigma naught","lvl2":"EODC STAC Catalog"},"content":"\n\neodc_catalog = pystac_client.Client.open(\"https://stac.eodc.eu/api/v1\")\nsearch = eodc_catalog.search(\n    collections=\"SENTINEL1_SIG0_20M\",\n    bbox=bounding_box,\n    datetime=time_range,\n)\nitems_sig0 = search.item_collection()\n\n\ndef post_process_eodc_cube(dc, items, bands):\n    \"\"\"\n    Postprocessing of EODC data cubes.\n\n    Parameters\n    ----------\n    x : xarray.Dataset\n    items: pystac.item_collection.ItemCollection\n        STAC items that concern the Xarray Dataset\n    bands: array\n        Selected bands\n\n    Returns\n    -------\n    xarray.Dataset\n    \"\"\"\n    if not isinstance(bands, tuple):\n        bands = tuple([bands])\n    for i in bands:\n        dc[i] = post_process_eodc_cube_(dc[i], items, i)\n    return dc\n\n\ndef post_process_eodc_cube_(dc, items, band):\n    fields = items[0].assets[band].extra_fields\n    scale = fields.get(\"raster:bands\")[0][\"scale\"]\n    nodata = fields.get(\"raster:bands\")[0][\"nodata\"]\n    return dc.where(dc != nodata) / scale\n\n\nbands = \"VV\"\nsig0_dc = odc_stac.load(items_sig0, bands=bands, bbox=bounding_box)\nsig0_dc = (\n    post_process_eodc_cube(sig0_dc, items_sig0, bands)\n    .rename_vars({\"VV\": \"sig0\"})\n    .dropna(dim=\"time\", how=\"all\")\n    .median(\"time\")\n)\n\nsig0_dc\n\n","type":"content","url":"/notebooks/tutorials/floodmapping#sigma-naught","position":7},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl3":"Harmonic Parameters","lvl2":"EODC STAC Catalog"},"type":"lvl3","url":"/notebooks/tutorials/floodmapping#harmonic-parameters","position":8},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl3":"Harmonic Parameters","lvl2":"EODC STAC Catalog"},"content":"\n\nsearch = eodc_catalog.search(\n    collections=\"SENTINEL1_HPAR\",\n    bbox=bounding_box,\n    query=[\"sat:relative_orbit=80\"],\n)\n\nitems_hpar = search.item_collection()\nbands = (\"C1\", \"C2\", \"C3\", \"M0\", \"S1\", \"S2\", \"S3\", \"STD\")\nhpar_dc = odc_stac.load(\n    items_hpar,\n    bands=bands,\n    bbox=bounding_box,\n    groupby=None,\n)\nhpar_dc = post_process_eodc_cube(hpar_dc, items_hpar, bands).median(\"time\")\nhpar_dc\n\n","type":"content","url":"/notebooks/tutorials/floodmapping#harmonic-parameters","position":9},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl3":"Projected Local Incidence Angles","lvl2":"EODC STAC Catalog"},"type":"lvl3","url":"/notebooks/tutorials/floodmapping#projected-local-incidence-angles","position":10},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl3":"Projected Local Incidence Angles","lvl2":"EODC STAC Catalog"},"content":"\n\nsearch = eodc_catalog.search(\n    collections=\"SENTINEL1_MPLIA\",\n    bbox=bounding_box,\n    query=[\"sat:relative_orbit=80\"],\n)\n\nitems_plia = search.item_collection()\n\nbands = \"MPLIA\"\nplia_dc = odc_stac.load(\n    items_plia,\n    bands=bands,\n    bbox=bounding_box,\n)\n\nplia_dc = post_process_eodc_cube(plia_dc, items_plia, bands).median(\"time\")\nplia_dc\n\nFinally, we merged the datasets as one big dataset and reproject the data in EPSG 4326 for easier visualizing of the data.\n\nflood_dc = xr.merge([sig0_dc, plia_dc, hpar_dc])\nflood_dc = flood_dc.rio.reproject(\"EPSG:4326\").rio.write_crs(\"EPSG:4326\")\nflood_dc\n\n","type":"content","url":"/notebooks/tutorials/floodmapping#projected-local-incidence-angles","position":11},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl2":"From Backscattering to Flood Mapping"},"type":"lvl2","url":"/notebooks/tutorials/floodmapping#from-backscattering-to-flood-mapping","position":12},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl2":"From Backscattering to Flood Mapping"},"content":"In the following lines we create a map with microwave backscattering values.\n\nmrs_view = flood_dc.sig0.hvplot.image(\n    x=\"x\", y=\"y\", cmap=\"viridis\", geo=True, tiles=True\n).opts(frame_height=400)\nmrs_view\n\nFigureÂ 1:Area targeted for \\sigma^0 backscattering is the Greek region of Thessaly, which experienced a major flood in February of 2018.\n\n","type":"content","url":"/notebooks/tutorials/floodmapping#from-backscattering-to-flood-mapping","position":13},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl2":"Microwave Backscattering over Land and Water"},"type":"lvl2","url":"/notebooks/tutorials/floodmapping#microwave-backscattering-over-land-and-water","position":14},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl2":"Microwave Backscattering over Land and Water"},"content":"Reverend Bayes was concerned with two events, one (the hypothesis) occurring before the other (the evidence). If we know its cause, it is easy to logically deduce the probability of an effect. However, in this case we want to deduce the probability of a cause from an observed effect, also known as â€œreversed probabilityâ€. In the case of flood mapping, we have \\sigma^0 backscatter observations over land (the effect) and we want to deduce the probability of flooding (F) and non-flooding (NF).\n\nIn other words, we want to know the probability of flooding P(F) given a pixelâ€™s \\sigma^0:P(F|\\sigma^0)\n\nand the probability of a pixel being not flooded P(NF) given a certain \\sigma^0:P(NF|\\sigma^0).\n\nBayes showed that these can be deduced from the observation that forward and reversed probability are equal, so that:P(F|\\sigma^0)P(\\sigma^0) = P(\\sigma^0|F)P(F)\n\nandP(NF|\\sigma^0)P(\\sigma^0) = P(\\sigma^0|NF)P(NF).\n\nThe forward probability of \\sigma^0 given the occurrence of flooding (P(\\sigma^0|F)) and \\sigma^0 given no flooding (P(\\sigma^0|NF)) can be extracted from past information on backscattering over land and water surfaces. As seen in the sketch below , the characteristics of backscattering over land and water differ considerably.\n\n{#fig-sat}","type":"content","url":"/notebooks/tutorials/floodmapping#microwave-backscattering-over-land-and-water","position":15},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl2":"Likelihoods"},"type":"lvl2","url":"/notebooks/tutorials/floodmapping#likelihoods","position":16},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl2":"Likelihoods"},"content":"The so-called likelihoods of P(\\sigma^0|F) and P(\\sigma^0|NF) can thus be calculated from past backscattering information. In the following code chunk we define the functions calc_water_likelihood and calc_land_likelihood to calculate the water and land likelihoodâ€™s of a pixel, based on the Xarray datasets for the PLIA and HPAR, respectively.\n\ndef calc_water_likelihood(sigma, x=None, y=None):\n    \"\"\"\n    Calculate water likelihoods.\n\n    Parameters\n    ----------\n    sigma: float|array\n        Sigma naught value(s)\n    x: float|array\n        Longitude\n    y: float|array\n        Latitude\n\n    Returns\n    -------\n    numpy array\n    \"\"\"\n    point = flood_dc.sel(x=x, y=y, method=\"nearest\")\n    wbsc_mean = point.MPLIA * -0.394181 + -4.142015\n    wbsc_std = 2.754041\n    return norm.pdf(sigma, wbsc_mean.to_numpy(), wbsc_std)\n\n\ndef expected_land_backscatter(data, dtime_str):\n    w = np.pi * 2 / 365\n    dt = datetime.datetime.strptime(dtime_str, \"%Y-%m-%d\")\n    t = dt.timetuple().tm_yday\n    wt = w * t\n\n    M0 = data.M0\n    S1 = data.S1\n    S2 = data.S2\n    S3 = data.S3\n    C1 = data.C1\n    C2 = data.C2\n    C3 = data.C3\n    hm_c1 = (M0 + S1 * np.sin(wt)) + (C1 * np.cos(wt))\n    hm_c2 = (hm_c1 + S2 * np.sin(2 * wt)) + C2 * np.cos(2 * wt)\n    hm_c3 = (hm_c2 + S3 * np.sin(3 * wt)) + C3 * np.cos(3 * wt)\n    return hm_c3\n\n\ndef calc_land_likelihood(sigma, x=None, y=None):\n    \"\"\"\n    Calculate land likelihoods.\n\n    Parameters\n    ----------\n    sigma: float|array\n        Sigma naught value(s)\n    x: float|array\n        Longitude\n    y: float|array\n        Latitude\n\n    Returns\n    -------\n    numpy array\n    \"\"\"\n    point = flood_dc.sel(x=x, y=y, method=\"nearest\")\n    lbsc_mean = expected_land_backscatter(point, \"2018-02-01\")\n    lbsc_std = point.STD\n    return norm.pdf(sigma, lbsc_mean.to_numpy(), lbsc_std.to_numpy())\n\nWithout going into the details of how these likelihoods are calculated, you can hover over a pixel of the map to plot the likelihoods of \\sigma^0 being governed by land or water. For reference we model the water and land likelihoods (model_likelihoods) over a range of \\sigma^0 values.\n\ndef model_likelihoods(sigma=(-30, 0), x=None, y=None):\n    \"\"\"\n    Model likelihoods over a range of sigma naught.\n\n    Parameters\n    ----------\n    sigma: tuple\n        Minimum and maximum for range of sigma naught values\n    x: float|array\n        Longitude\n    y: float|array\n        Latitude\n\n    Returns\n    -------\n    Pandas Datafrane\n    \"\"\"\n    sigma = np.arange(sigma[0], sigma[1], 0.1)\n    land_likelihood = calc_land_likelihood(sigma=sigma, x=x, y=y)\n    water_likelihood = calc_water_likelihood(sigma=sigma, x=x, y=y)\n    point = flood_dc.sel(x=x, y=y, method=\"nearest\")\n    return pd.DataFrame(\n        {\n            \"sigma\": sigma,\n            \"water_likelihood\": water_likelihood,\n            \"land_likelihood\": land_likelihood,\n            \"observed\": np.repeat(point.sig0.values, len(land_likelihood)),\n        }\n    )\n\n\npointer = hv.streams.PointerXY(source=mrs_view.get(1), x=22.1, y=39.5)\n\nlikelihood_pdi = hvplot.bind(\n    model_likelihoods, x=pointer.param.x, y=pointer.param.y\n).interactive()\n\nview_likelihoods = (\n    likelihood_pdi.hvplot(\"sigma\", \"water_likelihood\", ylabel=\"likelihoods\").dmap()\n    * likelihood_pdi.hvplot(\"sigma\", \"land_likelihood\").dmap()\n    * likelihood_pdi.hvplot(\"observed\", \"land_likelihood\").dmap()\n).opts(frame_height=200, frame_width=300)\n\nview_likelihoods + mrs_view.get(1)\n\nFigureÂ 2:Likelihoods for \\sigma^0 being associated with land or water for 1 pixel in the Greek area of Thessaly. Likelihoods are calculated over a range of \\sigma^0. The pixelâ€™s observed \\sigma^0 is given with a vertical line. Hover on the map to re-calculate and update this figure for another pixel in the study area.\n\n","type":"content","url":"/notebooks/tutorials/floodmapping#likelihoods","position":17},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl2":"Posteriors"},"type":"lvl2","url":"/notebooks/tutorials/floodmapping#posteriors","position":18},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl2":"Posteriors"},"content":"Having calculated the likelihoods, we can now move on to calculate the probability of (non-)flooding given a pixelâ€™s \\sigma^0. These so-called posteriors need one more piece of information, as can be seen in the equation above. We need the probability that a pixel is flooded P(F) or not flooded P(NF). Of course, these are the figures weâ€™ve been trying to find this whole time. We donâ€™t actually have them yet, so what can we do? In Bayesian statistics, we can just start with our best guess. These guesses are called our â€œpriorsâ€, because they are the beliefs we hold prior to looking at the data. This subjective prior belief is the foundation Bayesian statistics, and we use the likelihoods we just calculated to update our belief in this particular hypothesis. This updated belief is called the â€œposteriorâ€.\n\nLetâ€™s say that our best estimate for the chance of flooding versus non-flooding of a pixel is 50-50: a coin flip.  We now can also calculate the probability of backscattering P(\\sigma^0), as the weighted average of the water and land likelihoods, ensuring that our posteriors range between 0 to 1.\n\nThe following code block shows how we calculate the priors.\n\ndef calc_posteriors(sigma, x=None, y=None):\n    \"\"\"\n    Calculate posterior probability.\n\n    Parameters\n    ----------\n    sigma: float|array\n        Sigma naught value(s)\n    x: float|array\n        Longitude\n    y: float|array\n        Latitude\n\n    Returns\n    -------\n    Tuple of two Numpy arrays\n    \"\"\"\n    land_likelihood = calc_land_likelihood(sigma=sigma, x=x, y=y)\n    water_likelihood = calc_water_likelihood(sigma=sigma, x=x, y=y)\n    evidence = (water_likelihood * 0.5) + (land_likelihood * 0.5)\n    water_posterior = (water_likelihood * 0.5) / evidence\n    land_posterior = (land_likelihood * 0.5) / evidence\n    return water_posterior, land_posterior\n\nWe can plot the posterior probabilities of flooding and non-flooding again and compare these to pixelâ€™s measured \\sigma^0. For reference we model the flood and non-flood posteriors (model_posteriors) over a range of \\sigma^0 values. Hover on a pixel to calculate the posterior probability.\n\ndef model_posteriors(sigma=(-30, 0), x=None, y=None):\n    \"\"\"\n    Model posterior probabilities over a range of sigma naught.\n\n    Parameters\n    ----------\n    sigma: tuple\n        Minimum and maximum for range of sigma naught values\n    x: float|array\n        Longitude\n    y: float|array\n        Latitude\n\n    Returns\n    -------\n    Pandas Datafrane\n    \"\"\"\n    bays_pd = model_likelihoods(sigma=sigma, x=x, y=y)\n    sigma = np.arange(sigma[0], sigma[1], 0.1)\n    bays_pd[\"f_post_prob\"], bays_pd[\"nf_post_prob\"] = calc_posteriors(\n        sigma=sigma, x=x, y=y\n    )\n    return bays_pd\n\n\nposterior_pdi = hvplot.bind(\n    model_posteriors, x=pointer.param.x, y=pointer.param.y\n).interactive()\n\nview_posteriors = (\n    posterior_pdi.hvplot(\"sigma\", \"f_post_prob\", ylabel=\"posteriors\").dmap()\n    * posterior_pdi.hvplot(\"sigma\", \"nf_post_prob\").dmap()\n    * posterior_pdi.hvplot(\"observed\", \"nf_post_prob\").dmap()\n).opts(frame_height=200, frame_width=300)\n\n(view_likelihoods + view_posteriors).cols(1) + mrs_view.get(1)\n\nFigureÂ 3:Posterior probabilities for \\sigma^0 of 1 pixel being associated with land for water in the Greek area of Thessaly. Hover on the map to re-calculate and update this figure for another pixel in the study area.\n\n","type":"content","url":"/notebooks/tutorials/floodmapping#posteriors","position":19},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl2":"Flood Classification"},"type":"lvl2","url":"/notebooks/tutorials/floodmapping#flood-classification","position":20},{"hierarchy":{"lvl1":"Reverend Bayes updates our Belief in Flood Detection","lvl2":"Flood Classification"},"content":"We are now ready to combine all this information and classify the pixels according to the probability of flooding given the backscatter value of each pixel. Here we just look whether the probability of flooding is higher than non-flooding:\n\ndef bayesian_flood_decision(sigma, x=None, y=None):\n    \"\"\"\n    Bayesian decision.\n\n    Parameters\n    ----------\n    sigma: float|array\n        Sigma naught value(s)\n    x: float|array\n        Longitude\n    y: float|array\n        Latitude\n\n    Returns\n    -------\n    Xarray DataArray\n    \"\"\"\n    f_post_prob, nf_post_prob = calc_posteriors(sigma=sigma, x=x, y=y)\n    return xr.where(\n        np.isnan(f_post_prob) | np.isnan(nf_post_prob),\n        np.nan,\n        np.greater(f_post_prob, nf_post_prob),\n    )\n\nHover on a point in the below map to see the likelihoods and posterior distributions (in the left-hand subplots).\n\nflood_dc[\"decision\"] = (\n    (\"y\", \"x\"),\n    bayesian_flood_decision(flood_dc.sig0, flood_dc.x, flood_dc.y),\n)\n\ncolorbar_opts = {\n    \"major_label_overrides\": {\n        0: \"non-flood\",\n        1: \"flood\",\n    },\n    \"ticker\": FixedTicker(ticks=[0, 1]),\n}\nflood_view = flood_dc.decision.hvplot.image(\n    x=\"x\", y=\"y\", rasterize=True, geo=True, cmap=[\"rgba(0, 0, 1, 0.1)\", \"darkred\"]\n).opts(frame_height=400, colorbar_opts={**colorbar_opts})\nmrs_view.get(0) * flood_view\n\nFigureÂ 4:Flood extent of the Greek region of Thessaly based on Bayesian probabilities are shown on the map superimposed on an open street map. Hover over a pixel to generate the pointâ€™s water and land likelihoods as well as the posterior probabilities.","type":"content","url":"/notebooks/tutorials/floodmapping#flood-classification","position":21},{"hierarchy":{"lvl1":"Tutorials"},"type":"lvl1","url":"/notebooks/tutorials/prereqs-tutorials","position":0},{"hierarchy":{"lvl1":"Tutorials"},"content":"This section of the Cookbook covers a wide range of topics. They showcase the\ncreation and usage of data products developed by TU Wien and EODC.\n\nConcepts\n\nImportance\n\nNotes\n\nIntro to xarray\n\nNecessary\n\n\n\nDask Arrays\n\nNecessary\n\n\n\nDocumentation hvPlot\n\nHelpful\n\nInteractive plotting\n\nDocumentation odc-stac\n\nHelpful\n\nData access\n\nTime to learn: 10 min","type":"content","url":"/notebooks/tutorials/prereqs-tutorials","position":1}]}