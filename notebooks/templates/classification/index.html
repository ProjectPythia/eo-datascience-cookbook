<!DOCTYPE html><html lang="en" class="" style="scroll-padding:60px"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>Classification of Sentinel-2 imagery - Earth Observation Data Science Cookbook</title><meta property="og:title" content="Classification of Sentinel-2 imagery - Earth Observation Data Science Cookbook"/><meta name="generator" content="mystmd"/><meta name="keywords" content=""/><meta name="image" content="/eo-datascience-cookbook/build/tuw-geo_eodc_logo_ho-a768683616f0967cd254152a2de90c57.png"/><meta property="og:image" content="/eo-datascience-cookbook/build/tuw-geo_eodc_logo_ho-a768683616f0967cd254152a2de90c57.png"/><link rel="stylesheet" href="/eo-datascience-cookbook/build/_assets/app-5WKS5EPQ.css"/><link rel="stylesheet" href="/eo-datascience-cookbook/build/_assets/thebe-core-VKVHG5VY.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jupyter-matplotlib@0.11.3/css/mpl_widget.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-T52X8HNYE8"></script><script>window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-T52X8HNYE8');</script><link rel="icon" href="/eo-datascience-cookbook/favicon.ico"/><link rel="stylesheet" href="/eo-datascience-cookbook/myst-theme.css"/><script>
  const savedTheme = localStorage.getItem("myst:theme");
  const theme = window.matchMedia("(prefers-color-scheme: light)").matches ? 'light' : 'dark';
  const classes = document.documentElement.classList;
  const hasAnyTheme = classes.contains('light') || classes.contains('dark');
  if (!hasAnyTheme) classes.add(savedTheme ?? theme);
</script></head><body class="m-0 transition-colors duration-500 bg-white dark:bg-stone-900"><div class="fixed top-1 left-1 h-[0px] w-[0px] focus-within:z-40 focus-within:h-auto focus-within:w-auto bg-white overflow-hidden focus-within:p-2 focus-within:ring-1" aria-label="skip to content options"><a href="#skip-to-frontmatter" class="block px-2 py-1 text-black underline">Skip to article frontmatter</a><a href="#skip-to-article" class="block px-2 py-1 text-black underline">Skip to article content</a></div><div class="bg-white/80 backdrop-blur dark:bg-stone-900/80 shadow dark:shadow-stone-700 p-3 md:px-8 sticky w-screen top-0 z-30 h-[60px]"><nav class="flex items-center justify-between flex-nowrap max-w-[1440px] mx-auto"><div class="flex flex-row xl:min-w-[19.5rem] mr-2 sm:mr-7 justify-start items-center shrink-0"><div class="block xl:hidden"><button class="flex items-center border-stone-400 text-stone-800 hover:text-stone-900 dark:text-stone-200 hover:dark:text-stone-100"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="2rem" height="2rem" class="m-1"><path fill-rule="evenodd" d="M3 6.75A.75.75 0 0 1 3.75 6h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 6.75ZM3 12a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 12Zm0 5.25a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75a.75.75 0 0 1-.75-.75Z" clip-rule="evenodd"></path></svg><span class="sr-only">Open Menu</span></button></div><a class="flex items-center ml-3 dark:text-white w-fit md:ml-5 xl:ml-7" href="https://projectpythia.org"><div class="p-1 mr-3"><img src="/eo-datascience-cookbook/build/pythia_logo-white-rt-d6cf0cb8860aa9e06ebcb5bd22b50c47.svg" class="h-9 dark:hidden" height="2.25rem"/><img src="/eo-datascience-cookbook/build/config-item-84d6d338-2fe2b3d43bd778d5d6159fd2427ee26f.svg" class="hidden h-9 dark:block" height="2.25rem"/></div><span class="text-md sm:text-xl tracking-tight sm:mr-5 sr-only">Made with MyST</span></a></div><div class="flex items-center flex-grow w-auto"><div class="flex-grow hidden text-md lg:block"><div class="relative inline-block mx-2 grow-0"><a class="inline-flex items-center justify-center w-full mx-2 py-1 text-md font-medium dark:text-white focus:outline-none focus-visible:ring-2 focus-visible:ring-white focus-visible:ring-opacity-75" href="/eo-datascience-cookbookhttps://projectpythia.org">Home</a></div><div class="relative inline-block mx-2 grow-0"><a class="inline-flex items-center justify-center w-full mx-2 py-1 text-md font-medium dark:text-white focus:outline-none focus-visible:ring-2 focus-visible:ring-white focus-visible:ring-opacity-75" href="/eo-datascience-cookbookhttps://foundations.projectpythia.org">Foundations</a></div><div class="relative inline-block mx-2 grow-0"><a class="inline-flex items-center justify-center w-full mx-2 py-1 text-md font-medium dark:text-white focus:outline-none focus-visible:ring-2 focus-visible:ring-white focus-visible:ring-opacity-75" href="/eo-datascience-cookbookhttps://cookbooks.projectpythia.org/">Cookbooks</a></div><div class="relative inline-block mx-2 grow-0"><a class="inline-flex items-center justify-center w-full mx-2 py-1 text-md font-medium dark:text-white focus:outline-none focus-visible:ring-2 focus-visible:ring-white focus-visible:ring-opacity-75" href="/eo-datascience-cookbookhttps://projectpythia.org/resource-gallery/">Resources</a></div><div class="relative inline-block mx-2 grow-0"><a class="inline-flex items-center justify-center w-full mx-2 py-1 text-md font-medium dark:text-white focus:outline-none focus-visible:ring-2 focus-visible:ring-white focus-visible:ring-opacity-75" href="/eo-datascience-cookbookhttps://projectpythia.org/#join-us">Community</a></div></div><div class="flex-grow block"></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R74op:" data-state="closed" class="flex items-center h-10 aspect-square sm:w-64 text-left text-gray-400 border border-gray-300 dark:border-gray-600 rounded-lg bg-gray-50 dark:bg-gray-700 hover:ring-blue-500 dark:hover:ring-blue-500 hover:border-blue-500 dark:hover:border-blue-500"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="p-2.5 h-10 w-10 aspect-square"><path fill-rule="evenodd" d="M10.5 3.75a6.75 6.75 0 1 0 0 13.5 6.75 6.75 0 0 0 0-13.5ZM2.25 10.5a8.25 8.25 0 1 1 14.59 5.28l4.69 4.69a.75.75 0 1 1-1.06 1.06l-4.69-4.69A8.25 8.25 0 0 1 2.25 10.5Z" clip-rule="evenodd"></path></svg><span class="hidden sm:block grow">Search</span><div aria-hidden="true" class="items-center hidden mx-1 font-mono text-sm text-gray-400 sm:flex gap-x-1"><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none hide-mac">CTRL</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none show-mac">âŒ˜</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none ">K</kbd><script>
;(() => {
const script = document.currentScript;
const root = script.parentElement;

const isMac = /mac/i.test(
      window.navigator.userAgentData?.platform ?? window.navigator.userAgent,
    );
root.querySelectorAll(".hide-mac").forEach(node => {node.classList.add(isMac ? "hidden" : "block")});
root.querySelectorAll(".show-mac").forEach(node => {node.classList.add(!isMac ? "hidden" : "block")});
})()</script></div></button><button class="theme rounded-full aspect-square border border-stone-700 dark:border-white hover:bg-neutral-100 border-solid overflow-hidden text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 w-8 h-8 mx-3" title="Toggle theme between light and dark mode" aria-label="Toggle theme between light and dark mode"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 hidden dark:block"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 0 1 .162.819A8.97 8.97 0 0 0 9 6a9 9 0 0 0 9 9 8.97 8.97 0 0 0 3.463-.69.75.75 0 0 1 .981.98 10.503 10.503 0 0 1-9.694 6.46c-5.799 0-10.5-4.7-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 0 1 .818.162Z" clip-rule="evenodd"></path></svg><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 dark:hidden"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v2.25m6.364.386-1.591 1.591M21 12h-2.25m-.386 6.364-1.591-1.591M12 18.75V21m-4.773-4.227-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 1 1-7.5 0 3.75 3.75 0 0 1 7.5 0Z"></path></svg></button><div class="block sm:hidden"><div class="relative" data-headlessui-state=""><div><button class="flex text-sm bg-transparent rounded-full focus:outline-none" id="headlessui-menu-button-:Rr4op:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open Menu</span><div class="flex items-center text-stone-200 hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="2rem" height="2rem" class="p-1"><path fill-rule="evenodd" d="M10.5 6a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Zm0 6a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Zm0 6a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Z" clip-rule="evenodd"></path></svg></div></button></div></div></div><div class="hidden sm:block"><a href="https://projectpythia.org" target="_blank" rel="noopener noreferrer" class="inline-block px-4 py-2 mx-1 mt-0 leading-none border rounded text-md border-stone-700 dark:border-white text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 hover:bg-neutral-100">Project Pythia</a></div></div></nav></div><div class="fixed xl:article-grid grid-gap xl:w-screen xl:pointer-events-none overflow-auto max-xl:min-w-[300px] hidden z-10" style="top:60px"><div class="pointer-events-auto xl:col-margin-left flex-col overflow-hidden hidden xl:flex"><div class="flex-grow py-6 overflow-y-auto primary-scrollbar"><nav aria-label="Navigation" class="overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px] lg:hidden"><div class="w-full px-1 dark:text-white font-medium"><a href="https://projectpythia.org" target="_blank" rel="noopener noreferrer" class="p-2 my-1 rounded-lg hover:bg-slate-300/30 block break-words focus:outline outline-blue-200 outline-2 rounded">Home</a><a href="https://foundations.projectpythia.org" target="_blank" rel="noopener noreferrer" class="p-2 my-1 rounded-lg hover:bg-slate-300/30 block break-words focus:outline outline-blue-200 outline-2 rounded">Foundations</a><a href="https://cookbooks.projectpythia.org/" target="_blank" rel="noopener noreferrer" class="p-2 my-1 rounded-lg hover:bg-slate-300/30 block break-words focus:outline outline-blue-200 outline-2 rounded">Cookbooks</a><a href="https://projectpythia.org/resource-gallery/" target="_blank" rel="noopener noreferrer" class="p-2 my-1 rounded-lg hover:bg-slate-300/30 block break-words focus:outline outline-blue-200 outline-2 rounded">Resources</a><a href="https://projectpythia.org/#join-us" target="_blank" rel="noopener noreferrer" class="p-2 my-1 rounded-lg hover:bg-slate-300/30 block break-words focus:outline outline-blue-200 outline-2 rounded">Community</a></div></nav><div class="my-3 border-b-2 lg:hidden"></div><nav aria-label="Table of Contents" class="flex-grow overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px]"><div class="w-full px-1 dark:text-white"><a title="Earth Observation Data Science Cookbook" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30 font-bold" href="/eo-datascience-cookbook/">Earth Observation Data Science Cookbook</a><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Preamble" class="block break-words rounded py-2 grow cursor-pointer">Preamble</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rmp8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:Rmp8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Courses" class="block break-words rounded py-2 grow cursor-pointer">Courses</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rup8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:Rup8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Templates" class="block break-words rounded py-2 grow cursor-pointer">Templates</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R16p8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R16p8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Tutorials" class="block break-words rounded py-2 grow cursor-pointer">Tutorials</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1ep8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1ep8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="References" class="block break-words rounded py-2 grow cursor-pointer">References</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1mp8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1mp8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div></div></nav></div><div class="flex-none py-6 transition-all duration-700 translate-y-6 opacity-0"><a class="flex mx-auto text-gray-700 w-fit hover:text-blue-700 dark:text-gray-200 dark:hover:text-blue-400" href="https://mystmd.org/made-with-myst" target="_blank" rel="noreferrer"><svg style="width:24px;height:24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" stroke="none"><g id="icon"><path fill="currentColor" d="M23.8,54.8v-3.6l4.7-0.8V17.5l-4.7-0.8V13H36l13.4,31.7h0.2l13-31.7h12.6v3.6l-4.7,0.8v32.9l4.7,0.8v3.6h-15
          v-3.6l4.9-0.8V20.8H65L51.4,53.3h-3.8l-14-32.5h-0.1l0.2,17.4v12.1l5,0.8v3.6H23.8z"></path><path fill="#F37726" d="M47,86.9c0-5.9-3.4-8.8-10.1-8.8h-8.4c-5.2,0-9.4-1.3-12.5-3.8c-3.1-2.5-5.4-6.2-6.8-11l4.8-1.6
          c1.8,5.6,6.4,8.6,13.8,8.8h9.2c6.4,0,10.8,2.5,13.1,7.5c2.3-5,6.7-7.5,13.1-7.5h8.4c7.8,0,12.7-2.9,14.6-8.7l4.8,1.6
          c-1.4,4.9-3.6,8.6-6.8,11.1c-3.1,2.5-7.3,3.7-12.4,3.8H63c-6.7,0-10,2.9-10,8.8"></path></g></svg><span class="self-center ml-2 text-sm">Made with MyST</span></a></div></div></div><main class="article-grid grid-gap"><article class="article-grid subgrid-gap col-screen article content"><div class="hidden"></div><div id="skip-to-frontmatter" aria-label="article frontmatter" class="mb-8 pt-9"><div class="flex items-center h-6 mb-5 text-sm font-light"><div class="flex-grow"></div><a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener noreferrer" class="opacity-50 hover:opacity-100 text-inherit hover:text-inherit" aria-label="Content License: Creative Commons Attribution 4.0 International (CC-BY-4.0)"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block mx-1"><title>Content License: Creative Commons Attribution 4.0 International (CC-BY-4.0)</title><path d="M12 2.2c2.7 0 5 1 7 2.9.9.9 1.6 2 2.1 3.1.5 1.2.7 2.4.7 3.8 0 1.3-.2 2.6-.7 3.8-.5 1.2-1.2 2.2-2.1 3.1-1 .9-2 1.7-3.2 2.2-1.2.5-2.5.7-3.7.7s-2.6-.3-3.8-.8c-1.2-.5-2.2-1.2-3.2-2.1s-1.6-2-2.1-3.2-.8-2.4-.8-3.7c0-1.3.2-2.5.7-3.7S4.2 6 5.1 5.1C7 3.2 9.3 2.2 12 2.2zM12 4c-2.2 0-4.1.8-5.6 2.3C5.6 7.1 5 8 4.6 9c-.4 1-.6 2-.6 3s.2 2.1.6 3c.4 1 1 1.8 1.8 2.6S8 19 9 19.4c1 .4 2 .6 3 .6s2.1-.2 3-.6c1-.4 1.9-1 2.7-1.8 1.5-1.5 2.3-3.3 2.3-5.6 0-1.1-.2-2.1-.6-3.1-.4-1-1-1.8-1.7-2.6C16.1 4.8 14.2 4 12 4zm-.1 6.4l-1.3.7c-.1-.3-.3-.5-.5-.6-.2-.1-.4-.2-.6-.2-.9 0-1.3.6-1.3 1.7 0 .5.1.9.3 1.3.2.3.5.5 1 .5.6 0 1-.3 1.2-.8l1.2.6c-.3.5-.6.9-1.1 1.1-.5.3-1 .4-1.5.4-.9 0-1.6-.3-2.1-.8-.5-.6-.8-1.3-.8-2.3 0-.9.3-1.7.8-2.2.6-.6 1.3-.8 2.1-.8 1.2 0 2.1.4 2.6 1.4zm5.6 0l-1.3.7c-.1-.3-.3-.5-.5-.6-.2-.1-.4-.2-.6-.2-.9 0-1.3.6-1.3 1.7 0 .5.1.9.3 1.3.2.3.5.5 1 .5.6 0 1-.3 1.2-.8l1.2.6c-.3.5-.6.9-1.1 1.1-.4.2-.9.3-1.4.3-.9 0-1.6-.3-2.1-.8s-.8-1.3-.8-2.2c0-.9.3-1.7.8-2.2.5-.5 1.2-.8 2-.8 1.2 0 2.1.4 2.6 1.4z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block mr-1"><title>Credit must be given to the creator</title><path d="M12 2.2c2.7 0 5 .9 6.9 2.8 1.9 1.9 2.8 4.2 2.8 6.9s-.9 5-2.8 6.8c-2 1.9-4.3 2.9-7 2.9-2.6 0-4.9-1-6.9-2.9-1.8-1.7-2.8-4-2.8-6.7s1-5 2.9-6.9C7 3.2 9.3 2.2 12 2.2zM12 4c-2.2 0-4.1.8-5.6 2.3C4.8 8 4 9.9 4 12c0 2.2.8 4 2.4 5.6C8 19.2 9.8 20 12 20c2.2 0 4.1-.8 5.7-2.4 1.5-1.5 2.3-3.3 2.3-5.6 0-2.2-.8-4.1-2.3-5.7C16.1 4.8 14.2 4 12 4zm2.6 5.6v4h-1.1v4.7h-3v-4.7H9.4v-4c0-.2.1-.3.2-.4.1-.2.2-.2.4-.2h4c.2 0 .3.1.4.2.2.1.2.2.2.4zm-4-2.5c0-.9.5-1.4 1.4-1.4s1.4.5 1.4 1.4c0 .9-.5 1.4-1.4 1.4s-1.4-.5-1.4-1.4z"></path></svg></a><a href="https://opensource.org/licenses/Apache-2.0" target="_blank" rel="noopener noreferrer" title="Code License: Apache License 2.0 (Apache-2.0)" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="mx-1 inline-block opacity-60 hover:opacity-100 hover:text-[#599F46]"><path d="M13.2 15.6c1.4-.5 2.1-1.6 2.1-3.3S13.8 8.9 12 8.9c-1.9 0-3.3 1.6-3.3 3.3 0 1.8.8 3 2.2 3.4l-2.3 5.9c-3.1-.8-6.3-4.6-6.3-9.3 0-5.5 4.3-10 9.7-10s9.8 4.5 9.8 10c0 4.7-3.1 8.5-6.3 9.3l-2.3-5.9z"></path></svg></a><a href="https://en.wikipedia.org/wiki/Open_access" target="_blank" rel="noopener noreferrer" title="Open Access" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="mr-1 inline-block opacity-60 hover:opacity-100 hover:text-[#E18435]"><path d="M17.1 12.6h-2V7.5c0-1.7-1.4-3.1-3-3.1-.8 0-1.6.3-2.2.9-.6.5-.9 1.3-.9 2.2v.7H7v-.7c0-1.4.5-2.7 1.5-3.7s2.2-1.5 3.6-1.5 2.6.5 3.6 1.5 1.5 2.3 1.5 3.7v5.1z"></path><path d="M12 21.8c-.8 0-1.6-.2-2.3-.5-.7-.3-1.4-.8-1.9-1.3-.6-.6-1-1.2-1.3-2-.3-.8-.5-1.6-.5-2.4s.2-1.6.5-2.4c.3-.7.7-1.4 1.3-2s1.2-1 1.9-1.3c.7-.3 1.5-.5 2.3-.5.8 0 1.6.2 2.3.5.7.3 1.4.8 1.9 1.3.6.6 1 1.2 1.3 2 .3.8.5 1.6.5 2.4s-.2 1.6-.5 2.4c-.3.7-.7 1.4-1.3 2-.6.6-1.2 1-1.9 1.3-.7.3-1.5.5-2.3.5zm0-10.3c-2.2 0-4 1.8-4 4.1s1.8 4.1 4 4.1 4-1.8 4-4.1-1.8-4.1-4-4.1z"></path><circle cx="12" cy="15.6" r="1.7"></circle></svg></a><div class="inline-block mr-1"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block"><title>Jupyter Notebook</title><path d="M20.2 1.7c0 .8-.5 1.4-1.3 1.5-.8 0-1.4-.5-1.5-1.3 0-.8.5-1.4 1.3-1.5.8-.1 1.5.5 1.5 1.3zM12 17.9c-3.7 0-7-1.3-8.7-3.3 1.8 4.8 7.1 7.3 11.9 5.5 2.5-.9 4.5-2.9 5.5-5.5-1.7 2-4.9 3.3-8.7 3.3zM12 5.1c3.7 0 7 1.3 8.7 3.3-1.8-4.8-7.1-7.3-11.9-5.5-2.5.9-4.5 2.9-5.5 5.5 1.7-2 5-3.3 8.7-3.3zM6.9 21.8c.1 1-.7 1.8-1.7 1.9-1 .1-1.8-.7-1.9-1.7 0-1 .7-1.8 1.7-1.9 1-.1 1.8.7 1.9 1.7zM3.7 4.6c-.6 0-1-.4-1-1s.4-1 1-1 1 .4 1 1c0 .5-.4 1-1 1z"></path></svg></div><div class="relative flex inline-block mx-1 grow-0" data-headlessui-state=""><button class="relative ml-2 -mr-1" id="headlessui-menu-button-:Rs8top:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Downloads</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem"><title>Download</title><path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75V16.5M16.5 12 12 16.5m0 0L7.5 12m4.5 4.5V3"></path></svg></button></div><button class="inline-flex size-[24px] hover:text-[#E18435] items-center justify-center" aria-label="Launch in external computing interface" title="Launch in external computing interface" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Re8top:" data-state="closed"><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.85357 3.85355L7.65355 3.05353C8.2981 2.40901 9.42858 1.96172 10.552 1.80125C11.1056 1.72217 11.6291 1.71725 12.0564 1.78124C12.4987 1.84748 12.7698 1.97696 12.8965 2.10357C13.0231 2.23018 13.1526 2.50125 13.2188 2.94357C13.2828 3.37086 13.2779 3.89439 13.1988 4.44801C13.0383 5.57139 12.591 6.70188 11.9464 7.34645L7.49999 11.7929L6.35354 10.6465C6.15827 10.4512 5.84169 10.4512 5.64643 10.6465C5.45117 10.8417 5.45117 11.1583 5.64643 11.3536L7.14644 12.8536C7.34171 13.0488 7.65829 13.0488 7.85355 12.8536L8.40073 12.3064L9.57124 14.2572C9.65046 14.3893 9.78608 14.4774 9.9389 14.4963C10.0917 14.5151 10.2447 14.4624 10.3535 14.3536L12.3535 12.3536C12.4648 12.2423 12.5172 12.0851 12.495 11.9293L12.0303 8.67679L12.6536 8.05355C13.509 7.19808 14.0117 5.82855 14.1887 4.58943C14.2784 3.9618 14.2891 3.33847 14.2078 2.79546C14.1287 2.26748 13.9519 1.74482 13.6035 1.39645C13.2552 1.04809 12.7325 0.871332 12.2045 0.792264C11.6615 0.710945 11.0382 0.721644 10.4105 0.8113C9.17143 0.988306 7.80189 1.491 6.94644 2.34642L6.32322 2.96968L3.07071 2.50504C2.91492 2.48278 2.75773 2.53517 2.64645 2.64646L0.646451 4.64645C0.537579 4.75533 0.484938 4.90829 0.50375 5.0611C0.522563 5.21391 0.61073 5.34954 0.742757 5.42876L2.69364 6.59928L2.14646 7.14645C2.0527 7.24022 2.00002 7.3674 2.00002 7.50001C2.00002 7.63261 2.0527 7.75979 2.14646 7.85356L3.64647 9.35356C3.84173 9.54883 4.15831 9.54883 4.35357 9.35356C4.54884 9.1583 4.54884 8.84172 4.35357 8.64646L3.20712 7.50001L3.85357 6.85356L6.85357 3.85355ZM10.0993 13.1936L9.12959 11.5775L11.1464 9.56067L11.4697 11.8232L10.0993 13.1936ZM3.42251 5.87041L5.43935 3.85356L3.17678 3.53034L1.80638 4.90074L3.42251 5.87041ZM2.35356 10.3535C2.54882 10.1583 2.54882 9.8417 2.35356 9.64644C2.1583 9.45118 1.84171 9.45118 1.64645 9.64644L0.646451 10.6464C0.451188 10.8417 0.451188 11.1583 0.646451 11.3535C0.841713 11.5488 1.1583 11.5488 1.35356 11.3535L2.35356 10.3535ZM3.85358 11.8536C4.04884 11.6583 4.04885 11.3417 3.85359 11.1465C3.65833 10.9512 3.34175 10.9512 3.14648 11.1465L1.14645 13.1464C0.95119 13.3417 0.951187 13.6583 1.14645 13.8535C1.34171 14.0488 1.65829 14.0488 1.85355 13.8536L3.85358 11.8536ZM5.35356 13.3535C5.54882 13.1583 5.54882 12.8417 5.35356 12.6464C5.1583 12.4512 4.84171 12.4512 4.64645 12.6464L3.64645 13.6464C3.45119 13.8417 3.45119 14.1583 3.64645 14.3535C3.84171 14.5488 4.1583 14.5488 4.35356 14.3535L5.35356 13.3535ZM9.49997 6.74881C10.1897 6.74881 10.7488 6.1897 10.7488 5.5C10.7488 4.8103 10.1897 4.25118 9.49997 4.25118C8.81026 4.25118 8.25115 4.8103 8.25115 5.5C8.25115 6.1897 8.81026 6.74881 9.49997 6.74881Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></button></div><h1 class="mb-0">Classification of Sentinel-2 imagery</h1><header class="mt-4 not-prose"><div><span class="font-semibold text-sm inline-block text-comma"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Rj8top:" data-state="closed">Wolfgang Wagner</button></span><span class="font-semibold text-sm inline-block text-comma"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Rl8top:" data-state="closed">Martin Schobben</button></span><span class="font-semibold text-sm inline-block text-comma"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Rn8top:" data-state="closed">Nikolas Pikall</button></span><span class="font-semibold text-sm inline-block text-comma"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Rp8top:" data-state="closed">Joseph Wagner</button></span><span class="font-semibold text-sm inline-block text-comma"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Rr8top:" data-state="closed">Davide Festa</button></span><span class="font-semibold text-sm inline-block text-comma"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Rt8top:" data-state="closed">Felix David ReuÃŸ</button></span><span class="font-semibold text-sm inline-block"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Rv8top:" data-state="closed">Luka JoviÄ‡</button></span></div></header></div><div class="block my-10 lg:sticky lg:z-10 lg:h-0 lg:pt-0 lg:my-0 lg:ml-10 lg:col-margin-right" style="top:60px"><nav></nav></div><div class="sticky top-[60px] pb-[14px] flex justify-end w-full z-20 pointer-events-none"><div class="flex p-1 m-1 space-x-1 border rounded-full shadow pointer-events-auto border-stone-300 bg-white/80 dark:bg-stone-900/80 backdrop-blur"><div class="rounded"><button class="flex text-center rounded-full cursor-pointer text-stone-800 dark:text-white hover:opacity-100 opacity-60" aria-label="start compute environment"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="inline-block w-6 h-6 align-top"><title>Launch kernel</title><path stroke-linecap="round" stroke-linejoin="round" d="M5.636 5.636a9 9 0 1 0 12.728 0M12 3v9"></path></svg></button></div></div></div><div id="skip-to-article"></div><div id="MZwhQhdhuD" class="relative group/block"><p><strong>Finding forests with satellite imagery</strong></p><h2 id="data-acquisition" class="relative group"><span class="heading-text">Data Acquisition</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#data-acquisition" title="Link to this Section" aria-label="Link to this Section">Â¶</a></h2><p>In this chapter, we will employ machine learning techniques to classify a scene using satellite imagery. Specifically, we will utilize <code>scikit-learn</code> to implement two distinct classifiers and subsequently compare their results. To begin, we need to import the following modules.</p></div><div id="tGgkSbxA1s" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">from datetime import datetime, timedelta

import cmcrameri as cmc  # noqa: F401
import geopandas as gpd
import matplotlib.colors as colors
import matplotlib.pyplot as plt
import numpy as np
import odc.stac
import pandas as pd
import pystac_client
import rioxarray  # noqa: F401
import xarray as xr
from odc.geo.geobox import GeoBox
from shapely.geometry import Polygon
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="sfNPsHIY_Vhtl3YWZnfFK" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="NR2K4vSw06" class="relative group/block"><p>Before we start, we need to load the data. We will use <code>odc-stac</code> to obtain data from Earth Search by Element 84. Here we define the area of interest and the time frame, aswell as the EPSG code and the resolution.</p><h3 id="searching-in-the-catalog" class="relative group"><span class="heading-text">Searching in the Catalog</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#searching-in-the-catalog" title="Link to this Section" aria-label="Link to this Section">Â¶</a></h3><p>The module <code>odc-stac</code> provides access to free, open source satelite data. To retrieve the data, we must define  several parameters that specify the location and time period for the satellite data. Additionally, we must specify the data collection we wish to access, as multiple collections are available. In this example, we will use multispectral imagery from the Sentinel-2 satellite.</p></div><div id="PeR3RRCQNL" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">dx = 0.0006  # 60m resolution
epsg = 4326

# Set Spatial extent
latmin, latmax = 47.86, 48.407
lonmin, lonmax = 16.32, 16.9
bounds = (lonmin, latmin, lonmax, latmax)


# Set Temporal extent
start_date = datetime(year=2024, month=5, day=1)
end_date = start_date + timedelta(days=10)

time_format = &quot;%Y-%m-%d&quot;
date_query = start_date.strftime(time_format) + &quot;/&quot; + end_date.strftime(time_format)

# Search for Sentinel-2 data
items = (
    pystac_client.Client.open(&quot;https://earth-search.aws.element84.com/v1&quot;)
    .search(
        bbox=bounds,
        collections=[&quot;sentinel-2-l2a&quot;],
        datetime=date_query,
        limit=100,
    )
    .item_collection()
)
print(len(items), &quot;scenes found&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="phlI_n2OwMjXv2ZXkHCOs" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><pre class="text-sm font-thin font-system"><code><span>10 scenes found
</span></code></pre></div></div></div><div id="m4qErEizzl" class="relative group/block"><p>We will now focus on the area south-east of Vienna, where the Nationalpark <em>Donauauen</em> is situated. The time frame we are interested in is the beginning of May 2024.
After passing these parameters to the <code>stac-catalog</code> we have found <strong>10 scenes</strong> that we can use for our analysis.</p><h3 id="loading-the-data" class="relative group"><span class="heading-text">Loading the Data</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#loading-the-data" title="Link to this Section" aria-label="Link to this Section">Â¶</a></h3><p>Now we will load the data directly into an <code>xarray</code> dataset, which we can use to perform computations on the data. <code>xarray</code> is a powerful library for working with multi-dimensional arrays, making it well-suited for handling satellite data.</p><p>Hereâ€™s how we can load the data using odc-stac and xarray:</p></div><div id="Ur3Ukh27ow" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># define a geobox for my region
geobox = GeoBox.from_bbox(bounds, crs=f&quot;epsg:{epsg}&quot;, resolution=dx)

# lazily combine items into a datacube
dc = odc.stac.load(
    items,
    bands=[&quot;scl&quot;, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;nir&quot;],
    chunks={&quot;time&quot;: 5, &quot;x&quot;: 600, &quot;y&quot;: 600},
    geobox=geobox,
    resampling=&quot;bilinear&quot;,
)
dc</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="s9yUqcKVo1DqU6nrPnLgh" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><div class="p-2.5">Loading...</div></div></div></div><div id="KB2ydSBYLW" class="relative group/block"><h2 id="data-visualization" class="relative group"><span class="heading-text">Data Visualization</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#data-visualization" title="Link to this Section" aria-label="Link to this Section">Â¶</a></h2><h3 id="rgb-image" class="relative group"><span class="heading-text">RGB Image</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#rgb-image" title="Link to this Section" aria-label="Link to this Section">Â¶</a></h3><p>With the image data now in our possession, we can proceed with computations and visualizations.</p><p>First, we define a mask to exclude cloud cover and areas with missing data. Subsequently, we create a composite median image, where each pixel value represents the median value across all the scenes we have identified. This approach helps to eliminate clouds and outliers present in some of the images, thereby providing a clearer and more representative visualization of the scene.</p></div><div id="mZw83x8uHk" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># define a mask for valid pixels (non-cloud)


def is_valid_pixel(data):
    # include only vegetated, not_vegitated, water, and snow
    return ((data &gt; 3) &amp; (data &lt; 7)) | (data == 11)


dc[&quot;valid&quot;] = is_valid_pixel(dc.scl)

# compute the masked median
rgb_median = (
    dc[[&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;]]
    .where(dc.valid)
    .to_dataarray(dim=&quot;band&quot;)
    .median(dim=&quot;time&quot;)
    .astype(int)
)

# plot the median composite
title_rgb = (
    &quot;RGB - Median Composite&quot;
    + f&quot;\n{start_date.strftime(&#x27;%d.%m.%Y&#x27;)} - {end_date.strftime(&#x27;%d.%m.%Y&#x27;)}&quot;
)
rgb_median.plot.imshow(robust=True).axes.set_title(title_rgb)
plt.show()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="ys3Nd_vWAawrnQVF4TYh7" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><pre class="text-sm font-thin font-system"><code><span>/home/runner/micromamba/envs/eo-datascience-cookbook/lib/python3.13/site-packages/rasterio/warp.py:387: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.
  dest = _reproject(
</span></code></pre></div><img src="/eo-datascience-cookbook/build/d2b8a6ad242e0dbd6835dd72229ccdee.png" alt="&lt;Figure size 640x480 with 1 Axes&gt;"/></div></div><div id="vKIrUQRX6r" class="relative group/block"><h3 id="false-color-image" class="relative group"><span class="heading-text">False Color Image</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#false-color-image" title="Link to this Section" aria-label="Link to this Section">Â¶</a></h3><p>In addition to the regular RGB Image, we can swap any of the bands from the visible spectrum with any other bands. In this specific case the red band has been changed to the near infrared band. This allows us to see vegetated areas more clearly, since they now appear in a bright red color. This is due to the fact that plants absorb regular red light while reflecting near infrared light <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://earthobservatory.nasa.gov/features/MeasuringVegetation/measuring_vegetation_2.php" target="_blank" rel="noreferrer" class="hover-link">NASA, 2020</a></cite></span>.</p></div><div id="YSLbh5fTUO" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># compute a false color image
# near infrared instead of red
fc_median = (
    dc[[&quot;nir&quot;, &quot;green&quot;, &quot;blue&quot;]]
    .where(dc.valid)
    .to_dataarray(dim=&quot;band&quot;)
    .transpose(..., &quot;band&quot;)
    .median(dim=&quot;time&quot;)
    .astype(int)
)

title_fc = (
    &quot;False color - Median Composite&quot;
    + f&quot;\n{start_date.strftime(&#x27;%d.%m.%Y&#x27;)} - {end_date.strftime(&#x27;%d.%m.%Y&#x27;)}&quot;
)
fc_median.plot.imshow(robust=True).axes.set_title(title_fc)
plt.show()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="t3ogpATY-WQ2pVZJBiblL" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><img src="/eo-datascience-cookbook/build/a7bc4c92461db3b34c015e7915808c89.png" alt="&lt;Figure size 640x480 with 1 Axes&gt;"/></div></div><div id="DajkxgmduW" class="relative group/block"><h3 id="ndvi-image" class="relative group"><span class="heading-text">NDVI Image</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#ndvi-image" title="Link to this Section" aria-label="Link to this Section">Â¶</a></h3><p>To get an first impression of the data, we can calculate the NDVI (Normalized Difference Vegetation Index) and plot it. The NDVI is calculated by useing the following formula. <span class="cite-group parenthetical"><cite class="" data-state="closed"><span class="hover-link">Rouse <em>et al.</em>, 1974</span></cite></span></p><div id="VFvtv1D5FX" class="flex my-5 group"><div class="flex-grow overflow-x-auto overflow-y-hidden"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>N</mi><mi>D</mi><mi>V</mi><mi>I</mi><mo>=</mo><mfrac><mrow><mi>N</mi><mi>I</mi><mi>R</mi><mo>âˆ’</mo><mi>R</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>N</mi><mi>I</mi><mi>R</mi><mo>+</mo><mi>R</mi><mi>e</mi><mi>d</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">NDVI = \frac{NIR - Red}{NIR + Red}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.1408em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><div class="relative self-center flex-none pl-2 m-0 text-right select-none"><a class="no-underline text-inherit hover:text-inherit text-inherit hover:text-inherit select-none hover:underline" href="#VFvtv1D5FX" title="Link to this Equation" aria-label="Link to this Equation">(<!-- -->1<!-- -->)</a></div></div><p>This gives us a good overview of the vegetation in the area. The values can range from -1 to 1 where the following meanings are associated with these values:</p><ul><li><p>-1 to 0 indicate dead plants or inanimate objects</p></li><li><p>0 to 0.33 are unhealthy plants</p></li><li><p>0.33 to 0.66 are moderatly healthy plants</p></li><li><p>0.66 to 1 are very healthy plants</p></li></ul></div><div id="fXUVDFOhrf" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Normalized Difference Vegetation Index (NDVI)


def normalized_difference(a, b):
    return (a - b * 1.0) / (a + b)


ndvi = normalized_difference(dc.nir, dc.red)
ndvi.median(dim=&quot;time&quot;).plot.imshow(cmap=&quot;cmc.cork&quot;, vmin=-1, vmax=1).axes.set_title(
    &quot;NDVI&quot;
)
plt.show()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="A8nCM33RFrMbqvj-8vGP7" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><img src="/eo-datascience-cookbook/build/072ad65d1f5dd354e9dccc6b39027ef3.png" alt="&lt;Figure size 640x480 with 2 Axes&gt;"/></div></div><div id="uGivCKzMu1" class="relative group/block"><h2 id="classification" class="relative group"><span class="heading-text">Classification</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#classification" title="Link to this Section" aria-label="Link to this Section">Â¶</a></h2><p>In this chapter, we will classify the satellite data to identify forested areas within the scene. By using supervised machine learning techniques, we can train classifiers to distinguish between forested and non-forested regions based on the training data we provide. We will explore two different classifiers and compare their performance in accurately identifying forest areas.</p><h3 id="regions-of-interest" class="relative group"><span class="heading-text">Regions of Interest</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#regions-of-interest" title="Link to this Section" aria-label="Link to this Section">Â¶</a></h3><p>Since this is a supervised classification, we need to have some training data. Therefore we need to define areas or regions, which we are certain represent the feature which we are classifiying. In this case we are interested in forested areas and regions that are definitly not forested. These regions will be used to train our classifiers.</p></div><div id="lOSE481rMk" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Define Polygons
forest_areas = {
    0: [
        Polygon(
            [
                (16.482772, 47.901753),
                (16.465133, 47.870124),
                (16.510142, 47.874382),
                (16.482772, 47.901753),
            ]
        )
    ],
    1: [
        Polygon(
            [
                (16.594079, 47.938855),
                (16.581914, 47.894454),
                (16.620233, 47.910268),
                (16.594079, 47.938855),
            ]
        )
    ],
    2: [
        Polygon(
            [
                (16.67984, 47.978998),
                (16.637263, 47.971091),
                (16.660376, 47.929123),
                (16.67984, 47.978998),
            ]
        )
    ],
    3: [
        Polygon(
            [
                (16.756477, 48.000286),
                (16.723024, 47.983256),
                (16.739446, 47.972916),
                (16.756477, 48.000286),
            ]
        )
    ],
    4: [
        Polygon(
            [
                (16.80696, 48.135923),
                (16.780806, 48.125583),
                (16.798445, 48.115243),
                (16.80696, 48.135923),
            ]
        )
    ],
    5: [
        Polygon(
            [
                (16.684097, 48.144438),
                (16.664634, 48.124366),
                (16.690788, 48.118892),
                (16.684097, 48.144438),
            ]
        )
    ],
    6: [
        Polygon(
            [
                (16.550894, 48.169984),
                (16.530822, 48.165118),
                (16.558801, 48.137139),
                (16.550894, 48.169984),
            ]
        )
    ],
    7: [
        Polygon(
            [
                (16.588604, 48.402329),
                (16.556976, 48.401112),
                (16.580697, 48.382865),
                (16.588604, 48.402329),
            ]
        )
    ],
}

nonforest_areas = {
    0: [
        Polygon(
            [
                (16.674974, 48.269126),
                (16.623882, 48.236281),
                (16.682272, 48.213168),
                (16.674974, 48.269126),
            ]
        )
    ],
    1: [
        Polygon(
            [
                (16.375723, 48.228374),
                (16.357476, 48.188839),
                (16.399444, 48.185798),
                (16.375723, 48.228374),
            ]
        )
    ],
    2: [
        Polygon(
            [
                (16.457834, 48.26426),
                (16.418907, 48.267301),
                (16.440804, 48.23324),
                (16.457834, 48.26426),
            ]
        )
    ],
    3: [
        Polygon(
            [
                (16.519266, 48.101861),
                (16.470607, 48.100645),
                (16.500411, 48.07145),
                (16.519266, 48.101861),
            ]
        )
    ],
    4: [
        Polygon(
            [
                (16.453577, 48.051986),
                (16.412217, 48.067192),
                (16.425598, 48.012451),
                (16.453577, 48.051986),
            ]
        )
    ],
}</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="7MWhY68ebDdVT8sUb27_4" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="pLzkhgUMBl" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Geoppandas Dataframe from Polygons
forest_df = gpd.GeoDataFrame(
    {&quot;geometry&quot;: [poly[0] for poly in forest_areas.values()]}, crs=&quot;EPSG:4326&quot;
)
nonforest_df = gpd.GeoDataFrame(
    {&quot;geometry&quot;: [poly[0] for poly in nonforest_areas.values()]},
    crs=&quot;EPSG:4326&quot;,
)


# Plotting Regions of Interest
fig, ax = plt.subplots()
rgb_median.plot.imshow(ax=ax, robust=True)
forest_df.plot(ax=ax, ec=&quot;C0&quot;, fc=&quot;none&quot;)
nonforest_df.plot(ax=ax, ec=&quot;C1&quot;, fc=&quot;none&quot;)
ax.set_title(&quot;Regions of Interest&quot;)
ax.set_aspect(&quot;equal&quot;)
plt.show()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="6yl7_MF05DkhTcu71r92z" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><img src="/eo-datascience-cookbook/build/282153f05ea9e2fc0212624576005f67.png" alt="&lt;Figure size 640x480 with 1 Axes&gt;"/></div></div><div id="xR26VqT1zn" class="relative group/block"><h3 id="data-preparation" class="relative group"><span class="heading-text">Data Preparation</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#data-preparation" title="Link to this Section" aria-label="Link to this Section">Â¶</a></h3><p>In addition to the Regions of Interest we will extract the specific bands from the loaded dataset that we intend to use for the classification, which are the <code>red, green, blue</code> and <code>near-infrared</code> bands, although other bands can also be utilized. Using these bands, we will create both a training and a testing dataset. The training dataset will be used to train the classifier, while the testing dataset will be employed to evaluate its performance.</p></div><div id="ajtY98WFEt" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Classifiying dataset (only necessary bands)
bands = [&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;nir&quot;]
ds_class = dc[bands].where(dc.valid).median(dim=&quot;time&quot;)
ds_class = ds_class.fillna(0)


def clip_array(ds: xr.Dataset, polygons):
    clipped = ds.rio.clip(polygons, invert=False, all_touched=False, drop=True)
    clipped_nan = clipped.where(clipped == ds)
    return clipped_nan


# Dictionaries with Dataarrays, each clipped by a Polygon
data_dict_feat = {
    idx: clip_array(ds_class, polygon) for idx, polygon in forest_areas.items()
}
data_dict_nonfeat = {
    idx: clip_array(ds_class, polygon) for idx, polygon in nonforest_areas.items()
}</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="NTdkGb0h4MjCooGW_5ROd" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="V4MsQQ4Wfu" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Reshape the polygon dataarrays to get a tuple (one value per band) of pixel values
feat_data = [
    xarray.to_array().values.reshape(len(bands), -1).T
    for xarray in data_dict_feat.values()
]  # replaced median_data_dict_feat with data_dict_feat
nonfeat_data = [
    xarray.to_array().values.reshape(len(bands), -1).T
    for xarray in data_dict_nonfeat.values()
]  # replaced median_data_dict_feat with data_dict_feat

# The rows of the different polygons are concatenated to a single array for further processing
feat_values = np.concatenate(feat_data)
nonfeat_values = np.concatenate(nonfeat_data)

# Drop Nan Values
X_feat_data = feat_values[~np.isnan(feat_values).any(axis=1)]
X_nonfeat_data = nonfeat_values[~np.isnan(nonfeat_values).any(axis=1)]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="bA9ObuvOldJr3_7zJCmvc" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="M3jJpdz4J9" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Creating Output Vector (1 for pixel is features; 0 for pixel is not feature)
y_feat_data = np.ones(X_feat_data.shape[0])
y_nonfeat_data = np.zeros(X_nonfeat_data.shape[0])

# Concatenate all Classes for training
X = np.concatenate([X_feat_data, X_nonfeat_data])
y = np.concatenate([y_feat_data, y_nonfeat_data])

# Split into Training and Testing Data.
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.5, random_state=42
)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="KrBjtETc7ek0yW6xHnEaE" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="b44cDvBY1n" class="relative group/block"><p>Now that we have prepared the training and testing data, we will create an image array of the actual scene that we intend to classify. This array will serve as the input for our classification algorithms, allowing us to apply the trained classifiers to the entire scene and identify the forested and non-forested areas accurately.</p></div><div id="GJv2zWPkVE" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">image_data = (
    ds_class[bands].to_array(dim=&quot;band&quot;).transpose(&quot;latitude&quot;, &quot;longitude&quot;, &quot;band&quot;)
)

# Reshape the image data
num_of_pixels = ds_class.sizes[&quot;longitude&quot;] * ds_class.sizes[&quot;latitude&quot;]
num_of_bands = len(bands)
X_image_data = image_data.values.reshape(num_of_pixels, num_of_bands)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="n-UBJZL-i3FJVjnZGTJm4" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="jQEnOWp6nr" class="relative group/block"><h3 id="classifiying-with-naive-bayes" class="relative group"><span class="heading-text">Classifiying with Naive Bayes</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#classifiying-with-naive-bayes" title="Link to this Section" aria-label="Link to this Section">Â¶</a></h3><p>Now that we have prepared all the needed data, we can begin the actual classification process.</p><p>We will start with a <em>Naive Bayes</em> classifier. First, we will train the classifier using our training dataset. Once trained, we will apply the classifier to the actual image to identify the forested and non-forested areas.</p></div><div id="l4s8tJxkJX" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Naive Bayes initialization and training
nb = GaussianNB()
nb_test = nb.fit(X_train, y_train)
nb_predict = nb.predict(X_test)

# Prediction on image
nb_predict_img = nb.predict(X_image_data)
nb_predict_img = nb_predict_img.reshape(
    ds_class.sizes[&quot;latitude&quot;], ds_class.sizes[&quot;longitude&quot;]
)

# Adding the Naive Bayes Prediction to the dataset
ds_class[&quot;NB-forest&quot;] = xr.DataArray(
    nb_predict_img,
    dims=[&quot;latitude&quot;, &quot;longitude&quot;],
    coords={
        &quot;longitude&quot;: ds_class[&quot;longitude&quot;],
        &quot;latitude&quot;: ds_class[&quot;latitude&quot;],
    },
)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="aUjYrhHq1eeI9VRbJGkse" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="yKxkxBzsy5" class="relative group/block"><p>To evaluate the effectiveness of the classification, we will plot the image predicted by the classifier. Additionally, we will examine the <code>Classification Report</code> and the <code>Confusion Matrix</code> to gain further insights into the classifierâ€™s performance.</p></div><div id="VGxSRTyYoI" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Plot Naive Bayes
alpha = 1
cmap_green = colors.ListedColormap([(1, 1, 1, alpha), &quot;green&quot;])

plot = ds_class[&quot;NB-forest&quot;].plot.imshow(
    cmap=cmap_green, cbar_kwargs={&quot;ticks&quot;: [0.25, 0.75]}
)
cbar = plot.colorbar
cbar.set_ticklabels([&quot;non-forest&quot;, &quot;forest&quot;])
plot.axes.set_title(&quot;Naive Bayes Classification&quot;)
plt.show()

# Print the Classification report
print(&quot;NAIVE BAYES: \n &quot; + classification_report(y_test, nb_predict))

# Print the confusion matrix
con_mat_nb = pd.DataFrame(
    confusion_matrix(y_test, nb_predict),
    index=[&quot;Actual Negative&quot;, &quot;Actual Positive&quot;],
    columns=[&quot;Predicted Negative&quot;, &quot;Predicted Positive&quot;],
)
display(con_mat_nb)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="4JhsVFB6662DABQHFVAcj" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><div class="p-2.5">Loading...</div></div></div></div><div id="M5F0kfLgNZ" class="relative group/block"><h3 id="classifiying-with-random-forest" class="relative group"><span class="heading-text">Classifiying with Random Forest</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#classifiying-with-random-forest" title="Link to this Section" aria-label="Link to this Section">Â¶</a></h3><p>To ensure our results are robust, we will explore an additional classifier. In this section, we will use the Random Forest classifier. The procedure for using this classifier is the same as before: we will train the classifier using our training dataset and then apply it to the actual image to classify the scene.</p></div><div id="Ra9juWBZAZ" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Random Forest initialization and training
rf = RandomForestClassifier(n_estimators=100)
rf_test = rf.fit(X_train, y_train)
rf_predict = rf.predict(X_test)

# Prediction on image
rf_predict_img = rf.predict(X_image_data)
rf_predict_img = rf_predict_img.reshape(
    ds_class.sizes[&quot;latitude&quot;], ds_class.sizes[&quot;longitude&quot;]
)

# Adding the Random Forest Prediction to the dataset
ds_class[&quot;RF-forest&quot;] = xr.DataArray(
    rf_predict_img,
    dims=[&quot;latitude&quot;, &quot;longitude&quot;],
    coords={
        &quot;longitude&quot;: ds_class[&quot;longitude&quot;],
        &quot;latitude&quot;: ds_class[&quot;latitude&quot;],
    },
)

plot = ds_class[&quot;RF-forest&quot;].plot.imshow(
    cmap=cmap_green, cbar_kwargs={&quot;ticks&quot;: [0.25, 0.75]}
)
cbar = plot.colorbar
cbar.set_ticklabels([&quot;non-forest&quot;, &quot;forest&quot;])
plot.axes.set_title(&quot;Random Forest Classification&quot;)
plt.show()

# Print the Classification report
print(&quot;RANDOM FOREST: \n &quot; + classification_report(y_test, rf_predict))

# Print the confusion matrix
con_mat_rf = pd.DataFrame(
    confusion_matrix(y_test, rf_predict),
    index=[&quot;Actual Negative&quot;, &quot;Actual Positive&quot;],
    columns=[&quot;Predicted Negative&quot;, &quot;Predicted Positive&quot;],
)
display(con_mat_rf)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="_l2aRggLVt6KpaQMh378p" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><div class="p-2.5">Loading...</div></div></div></div><div id="nNrZi1OX0i" class="relative group/block"><p>We can already see from the <code>classification reports</code> and the <code>confusion matrices</code> that the Random Forest classifier has outperformed the Naive Bayes classifier. This is particularly evident from the lower values in the secondary diagonal, indicating minimal False Positives and False Negatives. It appears that the Naive Bayes classifier is more sensitive to False Positives, resulting in a higher rate of incorrect classifications.</p><h3 id="comparison-of-the-classificators" class="relative group"><span class="heading-text">Comparison of the Classificators</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#comparison-of-the-classificators" title="Link to this Section" aria-label="Link to this Section">Â¶</a></h3><p>To gain a more in-depth understanding of the classifiersâ€™ performance, we will compare their results. Specifically, we will identify the areas where both classifiers agree and the areas where they disagree. This comparison will provide valuable insights into the strengths and weaknesses of each classifier, allowing us to better assess their effectiveness in identifying forested and non-forested regions.</p></div><div id="f622e3u8G7" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">cmap_trio = colors.ListedColormap([&quot;whitesmoke&quot;, &quot;indianred&quot;, &quot;goldenrod&quot;, &quot;darkgreen&quot;])


double_clf = ds_class[&quot;NB-forest&quot;] + 2 * ds_class[&quot;RF-forest&quot;]

fig, ax = plt.subplots()
cax = ax.imshow(double_clf, cmap=cmap_trio, interpolation=&quot;none&quot;)

# Add a colorbar with custom tick labels
cbar = fig.colorbar(cax, ticks=[1 * 0.375, 3 * 0.375, 5 * 0.375, 7 * 0.375])
cbar.ax.set_yticklabels([&quot;None&quot;, &quot;Naive Bayes&quot;, &quot;Random Forest&quot;, &quot;Both&quot;])
ax.set_title(&quot;Classification Comparisson&quot;)
ax.set_axis_off()
plt.show()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="qWxuT0uXSLO0JSGiKF4W0" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><img src="/eo-datascience-cookbook/build/69d520b35c61c9e23db18a405a7f523b.png" alt="&lt;Figure size 640x480 with 2 Axes&gt;"/></div></div><div id="h7PlBFlveC" class="relative group/block"><p>The areas where both classifiers agree include the larger forested regions, such as the <em>Nationalpark Donau-Auen</em> and the <em>Leithagebirge</em>. Additionally, both classifiers accurately identified the urban areas of Vienna and correctly excluded them from being classified as forested.</p></div><div id="LG1HoS2Itb" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Plot only one class, either None (0), Naive Bayes (1), Random Forest (2), or Both (3)
fig, axs = plt.subplots(2, 2, figsize=(8, 8))
ax = axs.ravel()

for i in range(4):
    ax[i].imshow(double_clf == i, cmap=&quot;cmc.oleron_r&quot;, interpolation=&quot;none&quot;)
    category = [
        &quot;by None&quot;,
        &quot;only by Naive Bayes&quot;,
        &quot;only by Random Forest&quot;,
        &quot;by Both&quot;,
    ][i]
    title = &quot;Areas classified &quot; + category
    ax[i].set_title(title)
    ax[i].set_axis_off()

plt.tight_layout()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="kmxsDfhJoJW-LPgzV6fL_" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><img src="/eo-datascience-cookbook/build/6dbfca6f0574eff57a107c1c5ff53380.png" alt="&lt;Figure size 800x800 with 4 Axes&gt;"/></div></div><div id="FppimaNwll" class="relative group/block"><p>When plotting the classified areas individually, we observe that the Random Forest classifier mistakenly identified the Danube River as a forested area. Conversely, the Naive Bayes classifier erroneously classified a significant amount of cropland as forest.</p><p>Finally, by analyzing the proportion of forested areas within the scene, we find that approximately 18% of the area is classified as forest, while around 66% is classified as non-forest. The remaining areas, which include water bodies and cropland, fall into less clearly defined categories.</p><p>The accompanying bar chart illustrates the distribution of these classifications, highlighting the percentage of forested areas, non-forested areas, and regions classified by only one of the two classifiers. This visual representation helps to quantify the areas of agreement and disagreement between the classifiers, providing a clearer picture of their performance.</p></div><div id="ACuSwHPXK1" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">counts = {}
for num in range(0, 4):
    num_2_class = {0: &quot;None&quot;, 1: &quot;Naive Bayes&quot;, 2: &quot;Random Forest&quot;, 3: &quot;Both&quot;}
    counts[num_2_class[num]] = int((double_clf == num).sum().values)

class_counts_df = pd.DataFrame(list(counts.items()), columns=[&quot;Class&quot;, &quot;Count&quot;])
class_counts_df[&quot;Percentage&quot;] = (
    class_counts_df[&quot;Count&quot;] / class_counts_df[&quot;Count&quot;].sum()
) * 100
ax = class_counts_df.plot.bar(
    x=&quot;Class&quot;,
    y=&quot;Percentage&quot;,
    rot=0,
    color=&quot;darkgreen&quot;,
    ylim=(0, 100),
    title=&quot;Classified Areas per Classificator (%)&quot;,
)

# Annotate the bars with the percentage values
for p in ax.patches:
    ax.annotate(
        f&quot;{p.get_height():.1f}%&quot;,
        (p.get_x() + p.get_width() / 2.0, p.get_height()),
        ha=&quot;center&quot;,
        va=&quot;center&quot;,
        xytext=(0, 9),
        textcoords=&quot;offset points&quot;,
    )</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="DJtCeCAwuC2oZfbV7AaZ4" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><img src="/eo-datascience-cookbook/build/23fb6b31812e99fd48291d5a6ea41fb5.png" alt="&lt;Figure size 640x480 with 1 Axes&gt;"/></div></div><div id="Nq6YpNyvdp" class="relative group/block"><h2 id="conclusion" class="relative group"><span class="heading-text">Conclusion</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#conclusion" title="Link to this Section" aria-label="Link to this Section">Â¶</a></h2><p>In this chapter, we utilized machine learning to classify satellite imagery into forested and non-forested areas, comparing Naive Bayes and Random Forest classifiers. The Random Forest classifier generally outperformed Naive Bayes, with fewer errors in classification, although it misclassified the Danube River as forested, while Naive Bayes incorrectly identified cropland as forest. The analysis, supported by the bar chart, revealed that about 18% of the scene was classified as forest, 66% as non-forest, and the remainder included ambiguous categories. This comparison highlights the strengths and limitations of each classifier, underscoring the need for careful selection and evaluation of classification methods.</p></div><div></div><section id="references" class="article-grid subgrid-gap col-screen"><div><header class="text-lg font-semibold text-stone-900 dark:text-white group">References<a class="no-underline text-inherit hover:text-inherit ml-2 select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#references" title="Link to References" aria-label="Link to References">Â¶</a></header></div><div class="pl-3 mb-8 text-xs text-stone-500 dark:text-stone-300"><ol><li class="break-words" id="cite-nasa2020">NASA. (2020). <i>Earth Observatory</i>. <a target="_blank" rel="noreferrer" href="https://earthobservatory.nasa.gov/features/MeasuringVegetation/measuring_vegetation_2.php">https://earthobservatory.nasa.gov/features/MeasuringVegetation/measuring_vegetation_2.php</a></li><li class="break-words" id="cite-rouse1974monitoring">Rouse, J. W., Haas, R. H., Schell, J. A., Deering, D. W., & others. (1974). Monitoring vegetation systems in the Great Plains with ERTS. <i>NASA Spec. Publ</i>, <i>351</i>(1), 309.</li></ol></div></section><div class="flex pt-10 mb-10 space-x-4"><a class="flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700" href="/eo-datascience-cookbook/notebooks/templates/prereqs-templates"><div class="flex h-full align-middle"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="self-center transition-transform group-hover:-translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M10.5 19.5 3 12m0 0 7.5-7.5M3 12h18"></path></svg><div class="flex-grow text-right"><div class="text-xs text-gray-500 dark:text-gray-400">Templates</div>Templates</div></div></a><a class="flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700" href="/eo-datascience-cookbook/notebooks/tutorials/prereqs-tutorials"><div class="flex h-full align-middle"><div class="flex-grow"><div class="text-xs text-gray-500 dark:text-gray-400">Tutorials</div>Tutorials</div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="self-center transition-transform group-hover:translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 4.5 21 12m0 0-7.5 7.5M21 12H3"></path></svg></div></a></div></article></main><script>((a,d)=>{if(!window.history.state||!window.history.state.key){let h=Math.random().toString(32).slice(2);window.history.replaceState({key:h},"")}try{let f=JSON.parse(sessionStorage.getItem(a)||"{}")[d||window.history.state.key];typeof f=="number"&&window.scrollTo(0,f)}catch(h){console.error(h),sessionStorage.removeItem(a)}})("positions", null)</script><link rel="modulepreload" href="/eo-datascience-cookbook/build/entry.client-UNPC4GT3.js"/><link rel="modulepreload" href="/eo-datascience-cookbook/build/_shared/chunk-OCTKKCIL.js"/><link rel="modulepreload" href="/eo-datascience-cookbook/build/_shared/chunk-UAI5KRM7.js"/><link rel="modulepreload" href="/eo-datascience-cookbook/build/_shared/chunk-2NH4LW52.js"/><link rel="modulepreload" href="/eo-datascience-cookbook/build/_shared/chunk-XWET6RJ7.js"/><link rel="modulepreload" href="/eo-datascience-cookbook/build/_shared/chunk-HBJK6BW3.js"/><link rel="modulepreload" href="/eo-datascience-cookbook/build/_shared/chunk-HYMQ7M2K.js"/><link rel="modulepreload" href="/eo-datascience-cookbook/build/_shared/chunk-OHOXABTA.js"/><link rel="modulepreload" href="/eo-datascience-cookbook/build/_shared/chunk-OCWQY3HK.js"/><link rel="modulepreload" href="/eo-datascience-cookbook/build/_shared/chunk-C7FW3E47.js"/><link rel="modulepreload" href="/eo-datascience-cookbook/build/_shared/chunk-3CVK3PYF.js"/><link rel="modulepreload" href="/eo-datascience-cookbook/build/_shared/chunk-J6FHCSRC.js"/><link rel="modulepreload" href="/eo-datascience-cookbook/build/_shared/chunk-ND43KHSX.js"/><link rel="modulepreload" href="/eo-datascience-cookbook/build/_shared/chunk-GUCIBHGO.js"/><link rel="modulepreload" href="/eo-datascience-cookbook/build/root-ZJOPFBMV.js"/><link rel="modulepreload" href="/eo-datascience-cookbook/build/_shared/chunk-NFM2H3KQ.js"/><link rel="modulepreload" href="/eo-datascience-cookbook/build/routes/$-CQPS5IOR.js"/><script>window.__remixContext = {"url":"/notebooks/templates/classification","state":{"loaderData":{"root":{"config":{"version":2,"myst":"1.6.2","options":{"favicon":"/eo-datascience-cookbook/build/config-item-9b3afca6-7e48599c2f08ee7c06a7e73f2679efc6.ico","logo":"/eo-datascience-cookbook/build/pythia_logo-white-rt-d6cf0cb8860aa9e06ebcb5bd22b50c47.svg","logo_dark":"/eo-datascience-cookbook/build/config-item-84d6d338-2fe2b3d43bd778d5d6159fd2427ee26f.svg","logo_url":"https://projectpythia.org","analytics_google":"G-T52X8HNYE8","folders":true},"parts":{},"nav":[{"title":"Home","url":"https://projectpythia.org"},{"title":"Foundations","url":"https://foundations.projectpythia.org"},{"title":"Cookbooks","url":"https://cookbooks.projectpythia.org/"},{"title":"Resources","url":"https://projectpythia.org/resource-gallery/"},{"title":"Community","url":"https://projectpythia.org/#join-us"}],"actions":[{"title":"Project Pythia","url":"https://projectpythia.org","internal":false,"static":false}],"projects":[{"open_access":true,"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"Apache-2.0","url":"https://opensource.org/licenses/Apache-2.0","name":"Apache License 2.0","free":true,"osi":true}},"title":"Earth Observation Data Science Cookbook","authors":[{"nameParsed":{"literal":"Wolfgang Wagner","given":"Wolfgang","family":"Wagner"},"name":"Wolfgang Wagner","id":"contributors-myst-generated-uid-0"},{"nameParsed":{"literal":"Martin Schobben","given":"Martin","family":"Schobben"},"name":"Martin Schobben","id":"contributors-myst-generated-uid-1"},{"nameParsed":{"literal":"Nikolas Pikall","given":"Nikolas","family":"Pikall"},"name":"Nikolas Pikall","id":"contributors-myst-generated-uid-2"},{"nameParsed":{"literal":"Joseph Wagner","given":"Joseph","family":"Wagner"},"name":"Joseph Wagner","id":"contributors-myst-generated-uid-3"},{"nameParsed":{"literal":"Davide Festa","given":"Davide","family":"Festa"},"name":"Davide Festa","id":"contributors-myst-generated-uid-4"},{"nameParsed":{"literal":"Felix David ReuÃŸ","given":"Felix David","family":"ReuÃŸ"},"name":"Felix David ReuÃŸ","id":"contributors-myst-generated-uid-5"},{"nameParsed":{"literal":"Luka JoviÄ‡","given":"Luka","family":"JoviÄ‡"},"name":"Luka JoviÄ‡","id":"contributors-myst-generated-uid-6"}],"contributors":[{"id":"brian-rose","nameParsed":{"literal":"Brian E. J. Rose","given":"Brian E. J.","family":"Rose"},"name":"Brian E. J. Rose","orcid":"0000-0002-9961-3821","email":"brose@albany.edu","affiliations":["UAlbany"],"url":"https://brian-rose.github.io","github":"brian-rose","bluesky":"@brianejrose.bsky.social","linkedin":"https://www.linkedin.com/in/brian-rose-37bb55106/"},{"id":"clyne","nameParsed":{"literal":"John Clyne","given":"John","family":"Clyne"},"name":"John Clyne","orcid":"0000-0003-2788-9017","affiliations":["CISL"],"github":"clyne"},{"id":"jukent","nameParsed":{"literal":"Julia Kent","given":"Julia","family":"Kent"},"name":"Julia Kent","orcid":"0000-0002-5611-8986","affiliations":["CISL"],"github":"jukent"},{"id":"ktyle","nameParsed":{"literal":"Kevin Tyle","given":"Kevin","family":"Tyle"},"name":"Kevin Tyle","orcid":"0000-0001-5249-9665","affiliations":["UAlbany"],"github":"ktyle"},{"id":"andersy005","nameParsed":{"literal":"Anderson Banihirwe","given":"Anderson","family":"Banihirwe"},"name":"Anderson Banihirwe","orcid":"0000-0001-6583-571X","affiliations":["CarbonPlan"],"github":"andersy005"},{"id":"dcamron","nameParsed":{"literal":"Drew Camron","given":"Drew","family":"Camron"},"name":"Drew Camron","orcid":"0000-0001-7246-6502","affiliations":["Unidata"],"github":"dcamron"},{"id":"dopplershift","nameParsed":{"literal":"Ryan May","given":"Ryan","family":"May"},"name":"Ryan May","orcid":"0000-0003-2907-038X","affiliations":["Unidata"],"github":"dopplershift"},{"id":"mgrover1","nameParsed":{"literal":"Maxwell Grover","given":"Maxwell","family":"Grover"},"name":"Maxwell Grover","orcid":"0000-0002-0370-8974","affiliations":["Argonne"],"github":"mgrover1"},{"id":"r-ford","nameParsed":{"literal":"Robert R. Ford","given":"Robert R.","family":"Ford"},"name":"Robert R. Ford","orcid":"0000-0001-5483-4965","affiliations":["UAlbany"],"github":"r-ford"},{"id":"kmpaul","nameParsed":{"literal":"Kevin Paul","given":"Kevin","family":"Paul"},"name":"Kevin Paul","orcid":"0000-0001-8155-8038","affiliations":["NVIDIA"],"github":"kmpaul"},{"id":"jnmorley","nameParsed":{"literal":"James Morley","given":"James","family":"Morley"},"name":"James Morley","orcid":"0009-0005-5193-7981","github":"jnmorley"},{"id":"erogluorhan","nameParsed":{"literal":"Orhan Eroglu","given":"Orhan","family":"Eroglu"},"name":"Orhan Eroglu","orcid":"0000-0003-3099-8775","affiliations":["CISL"],"github":"erogluorhan"},{"id":"lkailynncar","nameParsed":{"literal":"Lily Kailyn","given":"Lily","family":"Kailyn"},"name":"Lily Kailyn","orcid":"0009-0002-0125-5091","affiliations":["CISL"],"github":"lkailynncar"},{"id":"anissa111","nameParsed":{"literal":"Anissa Zacharias","given":"Anissa","family":"Zacharias"},"name":"Anissa Zacharias","orcid":"0000-0002-2666-8493","affiliations":["CISL"],"github":"anissa111"},{"id":"kafitzgerald","nameParsed":{"literal":"Katelyn FitzGerald","given":"Katelyn","family":"FitzGerald"},"name":"Katelyn FitzGerald","orcid":"0000-0003-4184-1917","affiliations":["CISL"],"github":"kafitzgerald"}],"affiliations":[{"id":"UAlbany","name":"University at Albany (SUNY)","department":"Atmospheric and Environmental Sciences","url":"https://www.albany.edu/daes"},{"id":"CISL","name":"NSF National Center for Atmospheric Research","department":"Computational and Information Systems Lab","url":"https://www.cisl.ucar.edu"},{"id":"Unidata","name":"NSF Unidata","url":"https://www.unidata.ucar.edu"},{"id":"Argonne","name":"Argonne National Laboratory","department":"Environmental Science Division","url":"https://www.anl.gov/evs"},{"id":"CarbonPlan","name":"CarbonPlan","url":"https://carbonplan.org"},{"id":"NVIDIA","name":"NVIDIA Corporation","url":"https://www.nvidia.com/"}],"copyright":"2024","references":{"foundations":{"url":"https://foundations.projectpythia.org"},"xarray":{"url":"https://docs.xarray.dev/en/stable"},"matplotlib":{"url":"https://matplotlib.org/stable"}},"thebe":{"binder":{"url":"https://mybinder.org/","provider":"github","repo":"projectpythia/eo-datascience-cookbook","ref":"HEAD"}},"toc":[{"file":"README.md"},{"children":[{"file":"notebooks/how-to-cite.md"}],"title":"Preamble"},{"children":[{"children":[{"file":"notebooks/courses/microwave-remote-sensing/unit_01/01_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_01/02_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_01/03_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_02/04_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_02/05_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_02/06_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_03/07_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_03/08_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_03/09_in_class_exercise.ipynb"}],"file":"notebooks/courses/microwave-remote-sensing.ipynb"},{"children":[{"file":"notebooks/courses/environmental-remote-sensing/unit_01/01_handout_drought.ipynb"},{"file":"notebooks/courses/environmental-remote-sensing/unit_01/02_handout_drought.ipynb"},{"file":"notebooks/courses/environmental-remote-sensing/unit_01/03_handout_drought.ipynb"},{"file":"notebooks/courses/environmental-remote-sensing/unit_01/04_handout_drought.ipynb"}],"file":"notebooks/courses/environmental-remote-sensing.ipynb"}],"title":"Courses"},{"children":[{"children":[{"file":"notebooks/templates/classification.ipynb"}],"file":"notebooks/templates/prereqs-templates.ipynb"}],"title":"Templates"},{"children":[{"children":[{"file":"notebooks/tutorials/floodmapping.ipynb"}],"file":"notebooks/tutorials/prereqs-tutorials.ipynb"}],"title":"Tutorials"},{"children":[{"file":"notebooks/references.ipynb"}],"title":"References"}],"thumbnail":"/eo-datascience-cookbook/build/tuw-geo_eodc_logo_ho-a768683616f0967cd254152a2de90c57.png","exports":[],"bibliography":[],"index":"index","pages":[{"level":1,"title":"Preamble"},{"slug":"notebooks.how-to-cite","title":"How to Cite This Cookbook","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/940d5ffd64b58a1e369604298a5c4034.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Courses"},{"slug":"notebooks.courses.microwave-remote-sensing","title":"Microwave Remote Sensing","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"notebooks.courses.microwave-remote-sensing.unit-01.in-class-exercise","title":"Discover and Read SAR Data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-01.in-class-exercise-1","title":"Unit Conversion","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-01.in-class-exercise-2","title":"Backscattering Coefficients","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/side_looking_image_d-09f49644d88e802a414d9dd462acaaac.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-02.in-class-exercise","title":"Datacubes","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/dbe80838a9862c93c705efd1a8edd024.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-02.in-class-exercise-1","title":"Wavelength and Polarization","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-02.in-class-exercise-2","title":"Dielectric Properties","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-03.in-class-exercise","title":"Speckle Statistics","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/speckle_effect-9973bd7f075fdf526b747812edcc7f04.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-03.in-class-exercise-1","title":"Interferograms","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/8494624449702f8b89e05532470631cf.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-03.in-class-exercise-2","title":"Phase Unwrapping","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/3f26b4039a027763e4d44920b3792e29.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.environmental-remote-sensing","title":"Environmental Remote Sensing","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"notebooks.courses.environmental-remote-sensing.unit-01.handout-drought","title":"Remotely Sensed Droughts in Mozambique","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/0e0633405e49fe13238fa11c831f4f0f.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.environmental-remote-sensing.unit-01.handout-drought-1","title":"Evaluating the Reliability of Remotely Sensed Soil Moisture","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/7334c1f7c83b0b6a3bd7d11ff35388d8.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.environmental-remote-sensing.unit-01.handout-drought-2","title":"Evaluating the Reliability of Remotely Sensed Droughts","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/cfde475a9031e72eedf656b180d652ed.webp","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.environmental-remote-sensing.unit-01.handout-drought-3","title":"Leads and lags in Drought Dynamics","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/f20c6145ef56baa28c62ffb1caf68f93.webp","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"level":1,"title":"Templates"},{"slug":"notebooks.templates.prereqs-templates","title":"Templates","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"notebooks.templates.classification","title":"Classification of Sentinel-2 imagery","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"level":1,"title":"Tutorials"},{"slug":"notebooks.tutorials.prereqs-tutorials","title":"Tutorials","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"notebooks.tutorials.floodmapping","title":"Reverend Bayes updates our Belief in Flood Detection","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/1ab490d3ba1d22952fb4c45d205afb33.gif","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"level":1,"title":"References"},{"slug":"notebooks.references","title":"References","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"CONTENT_CDN_PORT":"3100","MODE":"static","BASE_URL":"/eo-datascience-cookbook"},"routes/$":{"config":{"version":2,"myst":"1.6.2","options":{"favicon":"/eo-datascience-cookbook/build/config-item-9b3afca6-7e48599c2f08ee7c06a7e73f2679efc6.ico","logo":"/eo-datascience-cookbook/build/pythia_logo-white-rt-d6cf0cb8860aa9e06ebcb5bd22b50c47.svg","logo_dark":"/eo-datascience-cookbook/build/config-item-84d6d338-2fe2b3d43bd778d5d6159fd2427ee26f.svg","logo_url":"https://projectpythia.org","analytics_google":"G-T52X8HNYE8","folders":true},"parts":{},"nav":[{"title":"Home","url":"https://projectpythia.org"},{"title":"Foundations","url":"https://foundations.projectpythia.org"},{"title":"Cookbooks","url":"https://cookbooks.projectpythia.org/"},{"title":"Resources","url":"https://projectpythia.org/resource-gallery/"},{"title":"Community","url":"https://projectpythia.org/#join-us"}],"actions":[{"title":"Project Pythia","url":"https://projectpythia.org","internal":false,"static":false}],"projects":[{"open_access":true,"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"Apache-2.0","url":"https://opensource.org/licenses/Apache-2.0","name":"Apache License 2.0","free":true,"osi":true}},"title":"Earth Observation Data Science Cookbook","authors":[{"nameParsed":{"literal":"Wolfgang Wagner","given":"Wolfgang","family":"Wagner"},"name":"Wolfgang Wagner","id":"contributors-myst-generated-uid-0"},{"nameParsed":{"literal":"Martin Schobben","given":"Martin","family":"Schobben"},"name":"Martin Schobben","id":"contributors-myst-generated-uid-1"},{"nameParsed":{"literal":"Nikolas Pikall","given":"Nikolas","family":"Pikall"},"name":"Nikolas Pikall","id":"contributors-myst-generated-uid-2"},{"nameParsed":{"literal":"Joseph Wagner","given":"Joseph","family":"Wagner"},"name":"Joseph Wagner","id":"contributors-myst-generated-uid-3"},{"nameParsed":{"literal":"Davide Festa","given":"Davide","family":"Festa"},"name":"Davide Festa","id":"contributors-myst-generated-uid-4"},{"nameParsed":{"literal":"Felix David ReuÃŸ","given":"Felix David","family":"ReuÃŸ"},"name":"Felix David ReuÃŸ","id":"contributors-myst-generated-uid-5"},{"nameParsed":{"literal":"Luka JoviÄ‡","given":"Luka","family":"JoviÄ‡"},"name":"Luka JoviÄ‡","id":"contributors-myst-generated-uid-6"}],"contributors":[{"id":"brian-rose","nameParsed":{"literal":"Brian E. J. Rose","given":"Brian E. J.","family":"Rose"},"name":"Brian E. J. Rose","orcid":"0000-0002-9961-3821","email":"brose@albany.edu","affiliations":["UAlbany"],"url":"https://brian-rose.github.io","github":"brian-rose","bluesky":"@brianejrose.bsky.social","linkedin":"https://www.linkedin.com/in/brian-rose-37bb55106/"},{"id":"clyne","nameParsed":{"literal":"John Clyne","given":"John","family":"Clyne"},"name":"John Clyne","orcid":"0000-0003-2788-9017","affiliations":["CISL"],"github":"clyne"},{"id":"jukent","nameParsed":{"literal":"Julia Kent","given":"Julia","family":"Kent"},"name":"Julia Kent","orcid":"0000-0002-5611-8986","affiliations":["CISL"],"github":"jukent"},{"id":"ktyle","nameParsed":{"literal":"Kevin Tyle","given":"Kevin","family":"Tyle"},"name":"Kevin Tyle","orcid":"0000-0001-5249-9665","affiliations":["UAlbany"],"github":"ktyle"},{"id":"andersy005","nameParsed":{"literal":"Anderson Banihirwe","given":"Anderson","family":"Banihirwe"},"name":"Anderson Banihirwe","orcid":"0000-0001-6583-571X","affiliations":["CarbonPlan"],"github":"andersy005"},{"id":"dcamron","nameParsed":{"literal":"Drew Camron","given":"Drew","family":"Camron"},"name":"Drew Camron","orcid":"0000-0001-7246-6502","affiliations":["Unidata"],"github":"dcamron"},{"id":"dopplershift","nameParsed":{"literal":"Ryan May","given":"Ryan","family":"May"},"name":"Ryan May","orcid":"0000-0003-2907-038X","affiliations":["Unidata"],"github":"dopplershift"},{"id":"mgrover1","nameParsed":{"literal":"Maxwell Grover","given":"Maxwell","family":"Grover"},"name":"Maxwell Grover","orcid":"0000-0002-0370-8974","affiliations":["Argonne"],"github":"mgrover1"},{"id":"r-ford","nameParsed":{"literal":"Robert R. Ford","given":"Robert R.","family":"Ford"},"name":"Robert R. Ford","orcid":"0000-0001-5483-4965","affiliations":["UAlbany"],"github":"r-ford"},{"id":"kmpaul","nameParsed":{"literal":"Kevin Paul","given":"Kevin","family":"Paul"},"name":"Kevin Paul","orcid":"0000-0001-8155-8038","affiliations":["NVIDIA"],"github":"kmpaul"},{"id":"jnmorley","nameParsed":{"literal":"James Morley","given":"James","family":"Morley"},"name":"James Morley","orcid":"0009-0005-5193-7981","github":"jnmorley"},{"id":"erogluorhan","nameParsed":{"literal":"Orhan Eroglu","given":"Orhan","family":"Eroglu"},"name":"Orhan Eroglu","orcid":"0000-0003-3099-8775","affiliations":["CISL"],"github":"erogluorhan"},{"id":"lkailynncar","nameParsed":{"literal":"Lily Kailyn","given":"Lily","family":"Kailyn"},"name":"Lily Kailyn","orcid":"0009-0002-0125-5091","affiliations":["CISL"],"github":"lkailynncar"},{"id":"anissa111","nameParsed":{"literal":"Anissa Zacharias","given":"Anissa","family":"Zacharias"},"name":"Anissa Zacharias","orcid":"0000-0002-2666-8493","affiliations":["CISL"],"github":"anissa111"},{"id":"kafitzgerald","nameParsed":{"literal":"Katelyn FitzGerald","given":"Katelyn","family":"FitzGerald"},"name":"Katelyn FitzGerald","orcid":"0000-0003-4184-1917","affiliations":["CISL"],"github":"kafitzgerald"}],"affiliations":[{"id":"UAlbany","name":"University at Albany (SUNY)","department":"Atmospheric and Environmental Sciences","url":"https://www.albany.edu/daes"},{"id":"CISL","name":"NSF National Center for Atmospheric Research","department":"Computational and Information Systems Lab","url":"https://www.cisl.ucar.edu"},{"id":"Unidata","name":"NSF Unidata","url":"https://www.unidata.ucar.edu"},{"id":"Argonne","name":"Argonne National Laboratory","department":"Environmental Science Division","url":"https://www.anl.gov/evs"},{"id":"CarbonPlan","name":"CarbonPlan","url":"https://carbonplan.org"},{"id":"NVIDIA","name":"NVIDIA Corporation","url":"https://www.nvidia.com/"}],"copyright":"2024","references":{"foundations":{"url":"https://foundations.projectpythia.org"},"xarray":{"url":"https://docs.xarray.dev/en/stable"},"matplotlib":{"url":"https://matplotlib.org/stable"}},"thebe":{"binder":{"url":"https://mybinder.org/","provider":"github","repo":"projectpythia/eo-datascience-cookbook","ref":"HEAD"}},"toc":[{"file":"README.md"},{"children":[{"file":"notebooks/how-to-cite.md"}],"title":"Preamble"},{"children":[{"children":[{"file":"notebooks/courses/microwave-remote-sensing/unit_01/01_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_01/02_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_01/03_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_02/04_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_02/05_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_02/06_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_03/07_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_03/08_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_03/09_in_class_exercise.ipynb"}],"file":"notebooks/courses/microwave-remote-sensing.ipynb"},{"children":[{"file":"notebooks/courses/environmental-remote-sensing/unit_01/01_handout_drought.ipynb"},{"file":"notebooks/courses/environmental-remote-sensing/unit_01/02_handout_drought.ipynb"},{"file":"notebooks/courses/environmental-remote-sensing/unit_01/03_handout_drought.ipynb"},{"file":"notebooks/courses/environmental-remote-sensing/unit_01/04_handout_drought.ipynb"}],"file":"notebooks/courses/environmental-remote-sensing.ipynb"}],"title":"Courses"},{"children":[{"children":[{"file":"notebooks/templates/classification.ipynb"}],"file":"notebooks/templates/prereqs-templates.ipynb"}],"title":"Templates"},{"children":[{"children":[{"file":"notebooks/tutorials/floodmapping.ipynb"}],"file":"notebooks/tutorials/prereqs-tutorials.ipynb"}],"title":"Tutorials"},{"children":[{"file":"notebooks/references.ipynb"}],"title":"References"}],"thumbnail":"/eo-datascience-cookbook/build/tuw-geo_eodc_logo_ho-a768683616f0967cd254152a2de90c57.png","exports":[],"bibliography":[],"index":"index","pages":[{"level":1,"title":"Preamble"},{"slug":"notebooks.how-to-cite","title":"How to Cite This Cookbook","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/940d5ffd64b58a1e369604298a5c4034.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Courses"},{"slug":"notebooks.courses.microwave-remote-sensing","title":"Microwave Remote Sensing","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"notebooks.courses.microwave-remote-sensing.unit-01.in-class-exercise","title":"Discover and Read SAR Data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-01.in-class-exercise-1","title":"Unit Conversion","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-01.in-class-exercise-2","title":"Backscattering Coefficients","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/side_looking_image_d-09f49644d88e802a414d9dd462acaaac.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-02.in-class-exercise","title":"Datacubes","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/dbe80838a9862c93c705efd1a8edd024.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-02.in-class-exercise-1","title":"Wavelength and Polarization","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-02.in-class-exercise-2","title":"Dielectric Properties","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-03.in-class-exercise","title":"Speckle Statistics","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/speckle_effect-9973bd7f075fdf526b747812edcc7f04.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-03.in-class-exercise-1","title":"Interferograms","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/8494624449702f8b89e05532470631cf.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-03.in-class-exercise-2","title":"Phase Unwrapping","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/3f26b4039a027763e4d44920b3792e29.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.environmental-remote-sensing","title":"Environmental Remote Sensing","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"notebooks.courses.environmental-remote-sensing.unit-01.handout-drought","title":"Remotely Sensed Droughts in Mozambique","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/0e0633405e49fe13238fa11c831f4f0f.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.environmental-remote-sensing.unit-01.handout-drought-1","title":"Evaluating the Reliability of Remotely Sensed Soil Moisture","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/7334c1f7c83b0b6a3bd7d11ff35388d8.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.environmental-remote-sensing.unit-01.handout-drought-2","title":"Evaluating the Reliability of Remotely Sensed Droughts","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/cfde475a9031e72eedf656b180d652ed.webp","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.environmental-remote-sensing.unit-01.handout-drought-3","title":"Leads and lags in Drought Dynamics","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/f20c6145ef56baa28c62ffb1caf68f93.webp","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"level":1,"title":"Templates"},{"slug":"notebooks.templates.prereqs-templates","title":"Templates","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"notebooks.templates.classification","title":"Classification of Sentinel-2 imagery","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"level":1,"title":"Tutorials"},{"slug":"notebooks.tutorials.prereqs-tutorials","title":"Tutorials","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"notebooks.tutorials.floodmapping","title":"Reverend Bayes updates our Belief in Flood Detection","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/1ab490d3ba1d22952fb4c45d205afb33.gif","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"level":1,"title":"References"},{"slug":"notebooks.references","title":"References","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"page":{"version":2,"kind":"Notebook","sha256":"2101497368618ac0549a0c6e36b2996b558676dca082bd4be95fddf7103cc6f6","slug":"notebooks.templates.classification","location":"/notebooks/templates/classification.ipynb","dependencies":[],"frontmatter":{"title":"Classification of Sentinel-2 imagery","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"authors":[{"nameParsed":{"literal":"Wolfgang Wagner","given":"Wolfgang","family":"Wagner"},"name":"Wolfgang Wagner","id":"contributors-myst-generated-uid-0"},{"nameParsed":{"literal":"Martin Schobben","given":"Martin","family":"Schobben"},"name":"Martin Schobben","id":"contributors-myst-generated-uid-1"},{"nameParsed":{"literal":"Nikolas Pikall","given":"Nikolas","family":"Pikall"},"name":"Nikolas Pikall","id":"contributors-myst-generated-uid-2"},{"nameParsed":{"literal":"Joseph Wagner","given":"Joseph","family":"Wagner"},"name":"Joseph Wagner","id":"contributors-myst-generated-uid-3"},{"nameParsed":{"literal":"Davide Festa","given":"Davide","family":"Festa"},"name":"Davide Festa","id":"contributors-myst-generated-uid-4"},{"nameParsed":{"literal":"Felix David ReuÃŸ","given":"Felix David","family":"ReuÃŸ"},"name":"Felix David ReuÃŸ","id":"contributors-myst-generated-uid-5"},{"nameParsed":{"literal":"Luka JoviÄ‡","given":"Luka","family":"JoviÄ‡"},"name":"Luka JoviÄ‡","id":"contributors-myst-generated-uid-6"}],"open_access":true,"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"Apache-2.0","url":"https://opensource.org/licenses/Apache-2.0","name":"Apache License 2.0","free":true,"osi":true}},"copyright":"2024","affiliations":[{"id":"UAlbany","name":"University at Albany (SUNY)","department":"Atmospheric and Environmental Sciences","url":"https://www.albany.edu/daes"},{"id":"CISL","name":"NSF National Center for Atmospheric Research","department":"Computational and Information Systems Lab","url":"https://www.cisl.ucar.edu"},{"id":"Unidata","name":"NSF Unidata","url":"https://www.unidata.ucar.edu"},{"id":"Argonne","name":"Argonne National Laboratory","department":"Environmental Science Division","url":"https://www.anl.gov/evs"},{"id":"CarbonPlan","name":"CarbonPlan","url":"https://carbonplan.org"},{"id":"NVIDIA","name":"NVIDIA Corporation","url":"https://www.nvidia.com/"}],"numbering":{"title":{"offset":2}},"exports":[{"format":"ipynb","filename":"classification.ipynb","url":"/eo-datascience-cookbook/build/classification-97a61d256b8388cb603acaef3a044bb9.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"strong","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Finding forests with satellite imagery","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Jlo3EAlJBz"}],"key":"RdnXR42IK7"}],"key":"OtMe8Cv8p2"},{"type":"heading","depth":2,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Data Acquisition","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ot0bS1SnQA"}],"identifier":"data-acquisition","label":"Data Acquisition","html_id":"data-acquisition","implicit":true,"key":"t84sRR8uMf"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"In this chapter, we will employ machine learning techniques to classify a scene using satellite imagery. Specifically, we will utilize ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"v6lLFNLmyJ"},{"type":"inlineCode","value":"scikit-learn","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"BI2k4TzTpX"},{"type":"text","value":" to implement two distinct classifiers and subsequently compare their results. To begin, we need to import the following modules.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"pySN0zWjPQ"}],"key":"Uv1mTcx5B0"}],"key":"MZwhQhdhuD"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from datetime import datetime, timedelta\n\nimport cmcrameri as cmc  # noqa: F401\nimport geopandas as gpd\nimport matplotlib.colors as colors\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport odc.stac\nimport pandas as pd\nimport pystac_client\nimport rioxarray  # noqa: F401\nimport xarray as xr\nfrom odc.geo.geobox import GeoBox\nfrom shapely.geometry import Polygon\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB","key":"k2yckW2ESz"},{"type":"output","id":"sfNPsHIY_Vhtl3YWZnfFK","data":[],"key":"kPsEIsdyxS"}],"key":"tGgkSbxA1s"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Before we start, we need to load the data. We will use ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UHNcu7JSeD"},{"type":"inlineCode","value":"odc-stac","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"t62agldctQ"},{"type":"text","value":" to obtain data from Earth Search by Element 84. Here we define the area of interest and the time frame, aswell as the EPSG code and the resolution.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"swSdefVpSQ"}],"key":"mZhcTfT3tE"},{"type":"heading","depth":3,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Searching in the Catalog","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"RtI8RvoKdt"}],"identifier":"searching-in-the-catalog","label":"Searching in the Catalog","html_id":"searching-in-the-catalog","implicit":true,"key":"fzDFDAut5q"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"The module ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"kdxEGHwRWz"},{"type":"inlineCode","value":"odc-stac","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"TlpBKpWDBU"},{"type":"text","value":" provides access to free, open source satelite data. To retrieve the data, we must define  several parameters that specify the location and time period for the satellite data. Additionally, we must specify the data collection we wish to access, as multiple collections are available. In this example, we will use multispectral imagery from the Sentinel-2 satellite.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"dPlpCR4JZY"}],"key":"CydVJpRgPh"}],"key":"NR2K4vSw06"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"dx = 0.0006  # 60m resolution\nepsg = 4326\n\n# Set Spatial extent\nlatmin, latmax = 47.86, 48.407\nlonmin, lonmax = 16.32, 16.9\nbounds = (lonmin, latmin, lonmax, latmax)\n\n\n# Set Temporal extent\nstart_date = datetime(year=2024, month=5, day=1)\nend_date = start_date + timedelta(days=10)\n\ntime_format = \"%Y-%m-%d\"\ndate_query = start_date.strftime(time_format) + \"/\" + end_date.strftime(time_format)\n\n# Search for Sentinel-2 data\nitems = (\n    pystac_client.Client.open(\"https://earth-search.aws.element84.com/v1\")\n    .search(\n        bbox=bounds,\n        collections=[\"sentinel-2-l2a\"],\n        datetime=date_query,\n        limit=100,\n    )\n    .item_collection()\n)\nprint(len(items), \"scenes found\")","key":"jovpQLRD6L"},{"type":"output","id":"phlI_n2OwMjXv2ZXkHCOs","data":[{"output_type":"stream","name":"stdout","text":"10 scenes found\n"}],"key":"qPl2Bh0g6P"}],"key":"PeR3RRCQNL"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"We will now focus on the area south-east of Vienna, where the Nationalpark ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dM9TW8bWhr"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Donauauen","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xSTVSGVjyD"}],"key":"RP1f4jtOb0"},{"type":"text","value":" is situated. The time frame we are interested in is the beginning of May 2024.\nAfter passing these parameters to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"k4CusUOVUH"},{"type":"inlineCode","value":"stac-catalog","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Jl7x4J4HWv"},{"type":"text","value":" we have found ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"r01VL6fNaB"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"10 scenes","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qKGqakgHmr"}],"key":"RPM47Oqf8N"},{"type":"text","value":" that we can use for our analysis.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qmmC9FWOE3"}],"key":"i2e4jdR8Q8"},{"type":"heading","depth":3,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Loading the Data","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"GV85nYdmjo"}],"identifier":"loading-the-data","label":"Loading the Data","html_id":"loading-the-data","implicit":true,"key":"MJHL5mAS7g"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Now we will load the data directly into an ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"cJZS0ngp0o"},{"type":"inlineCode","value":"xarray","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"szfheNEj3f"},{"type":"text","value":" dataset, which we can use to perform computations on the data. ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"TYTuY3WZP6"},{"type":"inlineCode","value":"xarray","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"AOCLKCgyur"},{"type":"text","value":" is a powerful library for working with multi-dimensional arrays, making it well-suited for handling satellite data.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"SMoSjQCqEM"}],"key":"nHI79MeP7r"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Hereâ€™s how we can load the data using odc-stac and xarray:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Wx0ARQVggQ"}],"key":"bycz8X3ql7"}],"key":"m4qErEizzl"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# define a geobox for my region\ngeobox = GeoBox.from_bbox(bounds, crs=f\"epsg:{epsg}\", resolution=dx)\n\n# lazily combine items into a datacube\ndc = odc.stac.load(\n    items,\n    bands=[\"scl\", \"red\", \"green\", \"blue\", \"nir\"],\n    chunks={\"time\": 5, \"x\": 600, \"y\": 600},\n    geobox=geobox,\n    resampling=\"bilinear\",\n)\ndc","key":"s1AZYG8kl1"},{"type":"output","id":"s9yUqcKVo1DqU6nrPnLgh","data":[{"output_type":"execute_result","execution_count":3,"metadata":{},"data":{"text/plain":{"content":"\u003cxarray.Dataset\u003e Size: 79MB\nDimensions:      (latitude: 913, longitude: 967, time: 10)\nCoordinates:\n  * latitude     (latitude) float64 7kB 48.41 48.41 48.41 ... 47.86 47.86 47.86\n  * longitude    (longitude) float64 8kB 16.32 16.32 16.32 ... 16.9 16.9 16.9\n  * time         (time) datetime64[ns] 80B 2024-05-01T09:57:21.858000 ... 202...\n    spatial_ref  int32 4B 4326\nData variables:\n    scl          (time, latitude, longitude) uint8 9MB dask.array\u003cchunksize=(5, 600, 600), meta=np.ndarray\u003e\n    red          (time, latitude, longitude) uint16 18MB dask.array\u003cchunksize=(5, 600, 600), meta=np.ndarray\u003e\n    green        (time, latitude, longitude) uint16 18MB dask.array\u003cchunksize=(5, 600, 600), meta=np.ndarray\u003e\n    blue         (time, latitude, longitude) uint16 18MB dask.array\u003cchunksize=(5, 600, 600), meta=np.ndarray\u003e\n    nir          (time, latitude, longitude) uint16 18MB dask.array\u003cchunksize=(5, 600, 600), meta=np.ndarray\u003e","content_type":"text/plain"},"text/html":{"content_type":"text/html","hash":"5e2d613a68954c9045aa5fdbff79beee","path":"/eo-datascience-cookbook/build/5e2d613a68954c9045aa5fdbff79beee.html"}}}],"key":"dIC8uJgfKD"}],"key":"Ur3Ukh27ow"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Data Visualization","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EUSDZvncKv"}],"identifier":"data-visualization","label":"Data Visualization","html_id":"data-visualization","implicit":true,"key":"wRfGjYOVZU"},{"type":"heading","depth":3,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"RGB Image","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"cmTsgBCuHL"}],"identifier":"rgb-image","label":"RGB Image","html_id":"rgb-image","implicit":true,"key":"OJO9sykvOe"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"With the image data now in our possession, we can proceed with computations and visualizations.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"JztQMIriE6"}],"key":"onkBBCtNVs"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"First, we define a mask to exclude cloud cover and areas with missing data. Subsequently, we create a composite median image, where each pixel value represents the median value across all the scenes we have identified. This approach helps to eliminate clouds and outliers present in some of the images, thereby providing a clearer and more representative visualization of the scene.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Sl1XB9Ci5P"}],"key":"fzCXRYgAZS"}],"key":"KB2ydSBYLW"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# define a mask for valid pixels (non-cloud)\n\n\ndef is_valid_pixel(data):\n    # include only vegetated, not_vegitated, water, and snow\n    return ((data \u003e 3) \u0026 (data \u003c 7)) | (data == 11)\n\n\ndc[\"valid\"] = is_valid_pixel(dc.scl)\n\n# compute the masked median\nrgb_median = (\n    dc[[\"red\", \"green\", \"blue\"]]\n    .where(dc.valid)\n    .to_dataarray(dim=\"band\")\n    .median(dim=\"time\")\n    .astype(int)\n)\n\n# plot the median composite\ntitle_rgb = (\n    \"RGB - Median Composite\"\n    + f\"\\n{start_date.strftime('%d.%m.%Y')} - {end_date.strftime('%d.%m.%Y')}\"\n)\nrgb_median.plot.imshow(robust=True).axes.set_title(title_rgb)\nplt.show()","key":"RQxw47XDlg"},{"type":"output","id":"ys3Nd_vWAawrnQVF4TYh7","data":[{"output_type":"stream","name":"stderr","text":"/home/runner/micromamba/envs/eo-datascience-cookbook/lib/python3.13/site-packages/rasterio/warp.py:387: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n  dest = _reproject(\n"},{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 640x480 with 1 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"d2b8a6ad242e0dbd6835dd72229ccdee","path":"/eo-datascience-cookbook/build/d2b8a6ad242e0dbd6835dd72229ccdee.png"}}}],"key":"PY2uMAiATG"}],"key":"mZw83x8uHk"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"False Color Image","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lnarTWnzkN"}],"identifier":"false-color-image","label":"False Color Image","html_id":"false-color-image","implicit":true,"key":"aGNZ8DY7kr"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"In addition to the regular RGB Image, we can swap any of the bands from the visible spectrum with any other bands. In this specific case the red band has been changed to the near infrared band. This allows us to see vegetated areas more clearly, since they now appear in a bright red color. This is due to the fact that plants absorb regular red light while reflecting near infrared light ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"up8ZUPL22Z"},{"type":"citeGroup","kind":"parenthetical","children":[{"type":"cite","kind":"parenthetical","label":"nasa2020","identifier":"nasa2020","children":[{"type":"text","value":"NASA, 2020","key":"fMbZ5fEPJa"}],"enumerator":"1","key":"RKQ39bCvn5"}],"key":"uRPuBNQOSw"},{"type":"text","value":".","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"cUFR2kEpv1"}],"key":"TkpcIAFBYM"}],"key":"vKIrUQRX6r"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# compute a false color image\n# near infrared instead of red\nfc_median = (\n    dc[[\"nir\", \"green\", \"blue\"]]\n    .where(dc.valid)\n    .to_dataarray(dim=\"band\")\n    .transpose(..., \"band\")\n    .median(dim=\"time\")\n    .astype(int)\n)\n\ntitle_fc = (\n    \"False color - Median Composite\"\n    + f\"\\n{start_date.strftime('%d.%m.%Y')} - {end_date.strftime('%d.%m.%Y')}\"\n)\nfc_median.plot.imshow(robust=True).axes.set_title(title_fc)\nplt.show()","key":"Zc7LSGKVMV"},{"type":"output","id":"t3ogpATY-WQ2pVZJBiblL","data":[{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 640x480 with 1 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"a7bc4c92461db3b34c015e7915808c89","path":"/eo-datascience-cookbook/build/a7bc4c92461db3b34c015e7915808c89.png"}}}],"key":"eU5hD2fGYo"}],"key":"YSLbh5fTUO"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"NDVI Image","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pfnDrxZo2K"}],"identifier":"ndvi-image","label":"NDVI Image","html_id":"ndvi-image","implicit":true,"key":"EE322dUhTl"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"To get an first impression of the data, we can calculate the NDVI (Normalized Difference Vegetation Index) and plot it. The NDVI is calculated by useing the following formula. ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"PInZ2V9Q8G"},{"type":"citeGroup","kind":"parenthetical","children":[{"type":"cite","kind":"parenthetical","label":"rouse1974monitoring","identifier":"rouse1974monitoring","children":[{"type":"text","value":"Rouse ","key":"JvZdT02Skh"},{"type":"emphasis","children":[{"type":"text","value":"et al.","key":"cIhMrU7IYr"}],"key":"RGvEfyBSlf"},{"type":"text","value":", 1974","key":"AKWJFSPpiK"}],"enumerator":"2","key":"YJyGx58THc"}],"key":"F0nI2X3yJR"}],"key":"TTFKOrg3zB"},{"type":"math","value":"NDVI = \\frac{NIR - Red}{NIR + Red}","position":{"start":{"line":4,"column":1},"end":{"line":6,"column":1}},"html":"\u003cspan class=\"katex-display\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eN\u003c/mi\u003e\u003cmi\u003eD\u003c/mi\u003e\u003cmi\u003eV\u003c/mi\u003e\u003cmi\u003eI\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmi\u003eN\u003c/mi\u003e\u003cmi\u003eI\u003c/mi\u003e\u003cmi\u003eR\u003c/mi\u003e\u003cmo\u003eâˆ’\u003c/mo\u003e\u003cmi\u003eR\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003ed\u003c/mi\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmi\u003eN\u003c/mi\u003e\u003cmi\u003eI\u003c/mi\u003e\u003cmi\u003eR\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eR\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003ed\u003c/mi\u003e\u003c/mrow\u003e\u003c/mfrac\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eNDVI = \\frac{NIR - Red}{NIR + Red}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\"\u003eN\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eD\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\"\u003eV\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eI\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:2.1408em;vertical-align:-0.7693em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mopen nulldelimiter\"\u003e\u003c/span\u003e\u003cspan class=\"mfrac\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:1.3714em;\"\u003e\u003cspan style=\"top:-2.314em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\"\u003eN\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eI\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.00773em;\"\u003eR\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.00773em;\"\u003eR\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ee\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ed\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.23em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"frac-line\" style=\"border-bottom-width:0.04em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.677em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\"\u003eN\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eI\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.00773em;\"\u003eR\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003eâˆ’\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.00773em;\"\u003eR\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ee\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ed\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003eâ€‹\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.7693em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose nulldelimiter\"\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","enumerator":"1","key":"VFvtv1D5FX"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"This gives us a good overview of the vegetation in the area. The values can range from -1 to 1 where the following meanings are associated with these values:","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"WrHYgMWss2"}],"key":"XQ8GlQzXIk"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"-1 to 0 indicate dead plants or inanimate objects","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"WUVjtVRYWt"}],"key":"b6x1BPSGh0"}],"key":"tv3FIpUDT5"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"0 to 0.33 are unhealthy plants","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"bNgyl41VWb"}],"key":"E7gq34LelM"}],"key":"Tt2yohHXma"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"0.33 to 0.66 are moderatly healthy plants","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"OAw706IfzU"}],"key":"wPO1WEugpg"}],"key":"ZXJpQYrqDO"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"0.66 to 1 are very healthy plants","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"qNtQF3wEDs"}],"key":"RP0vGGfUMF"}],"key":"raN4X0SkzK"}],"key":"flt0pJL7wL"}],"key":"DajkxgmduW"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Normalized Difference Vegetation Index (NDVI)\n\n\ndef normalized_difference(a, b):\n    return (a - b * 1.0) / (a + b)\n\n\nndvi = normalized_difference(dc.nir, dc.red)\nndvi.median(dim=\"time\").plot.imshow(cmap=\"cmc.cork\", vmin=-1, vmax=1).axes.set_title(\n    \"NDVI\"\n)\nplt.show()","key":"WvnCGJGxsj"},{"type":"output","id":"A8nCM33RFrMbqvj-8vGP7","data":[{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 640x480 with 2 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"072ad65d1f5dd354e9dccc6b39027ef3","path":"/eo-datascience-cookbook/build/072ad65d1f5dd354e9dccc6b39027ef3.png"}}}],"key":"FOohtmQosJ"}],"key":"fXUVDFOhrf"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Classification","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oFj1bypTPG"}],"identifier":"classification","label":"Classification","html_id":"classification","implicit":true,"key":"DDTX66qxHA"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"In this chapter, we will classify the satellite data to identify forested areas within the scene. By using supervised machine learning techniques, we can train classifiers to distinguish between forested and non-forested regions based on the training data we provide. We will explore two different classifiers and compare their performance in accurately identifying forest areas.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"kXlHEwgbbD"}],"key":"fvraI7wkhf"},{"type":"heading","depth":3,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Regions of Interest","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"QX7HXKAuLM"}],"identifier":"regions-of-interest","label":"Regions of Interest","html_id":"regions-of-interest","implicit":true,"key":"QItLhJ9drn"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Since this is a supervised classification, we need to have some training data. Therefore we need to define areas or regions, which we are certain represent the feature which we are classifiying. In this case we are interested in forested areas and regions that are definitly not forested. These regions will be used to train our classifiers.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"rFsAMYCqOC"}],"key":"mI1GJcjYL8"}],"key":"uGivCKzMu1"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Define Polygons\nforest_areas = {\n    0: [\n        Polygon(\n            [\n                (16.482772, 47.901753),\n                (16.465133, 47.870124),\n                (16.510142, 47.874382),\n                (16.482772, 47.901753),\n            ]\n        )\n    ],\n    1: [\n        Polygon(\n            [\n                (16.594079, 47.938855),\n                (16.581914, 47.894454),\n                (16.620233, 47.910268),\n                (16.594079, 47.938855),\n            ]\n        )\n    ],\n    2: [\n        Polygon(\n            [\n                (16.67984, 47.978998),\n                (16.637263, 47.971091),\n                (16.660376, 47.929123),\n                (16.67984, 47.978998),\n            ]\n        )\n    ],\n    3: [\n        Polygon(\n            [\n                (16.756477, 48.000286),\n                (16.723024, 47.983256),\n                (16.739446, 47.972916),\n                (16.756477, 48.000286),\n            ]\n        )\n    ],\n    4: [\n        Polygon(\n            [\n                (16.80696, 48.135923),\n                (16.780806, 48.125583),\n                (16.798445, 48.115243),\n                (16.80696, 48.135923),\n            ]\n        )\n    ],\n    5: [\n        Polygon(\n            [\n                (16.684097, 48.144438),\n                (16.664634, 48.124366),\n                (16.690788, 48.118892),\n                (16.684097, 48.144438),\n            ]\n        )\n    ],\n    6: [\n        Polygon(\n            [\n                (16.550894, 48.169984),\n                (16.530822, 48.165118),\n                (16.558801, 48.137139),\n                (16.550894, 48.169984),\n            ]\n        )\n    ],\n    7: [\n        Polygon(\n            [\n                (16.588604, 48.402329),\n                (16.556976, 48.401112),\n                (16.580697, 48.382865),\n                (16.588604, 48.402329),\n            ]\n        )\n    ],\n}\n\nnonforest_areas = {\n    0: [\n        Polygon(\n            [\n                (16.674974, 48.269126),\n                (16.623882, 48.236281),\n                (16.682272, 48.213168),\n                (16.674974, 48.269126),\n            ]\n        )\n    ],\n    1: [\n        Polygon(\n            [\n                (16.375723, 48.228374),\n                (16.357476, 48.188839),\n                (16.399444, 48.185798),\n                (16.375723, 48.228374),\n            ]\n        )\n    ],\n    2: [\n        Polygon(\n            [\n                (16.457834, 48.26426),\n                (16.418907, 48.267301),\n                (16.440804, 48.23324),\n                (16.457834, 48.26426),\n            ]\n        )\n    ],\n    3: [\n        Polygon(\n            [\n                (16.519266, 48.101861),\n                (16.470607, 48.100645),\n                (16.500411, 48.07145),\n                (16.519266, 48.101861),\n            ]\n        )\n    ],\n    4: [\n        Polygon(\n            [\n                (16.453577, 48.051986),\n                (16.412217, 48.067192),\n                (16.425598, 48.012451),\n                (16.453577, 48.051986),\n            ]\n        )\n    ],\n}","key":"j5SfHUBbry"},{"type":"output","id":"7MWhY68ebDdVT8sUb27_4","data":[],"key":"YkOExCmZT7"}],"key":"lOSE481rMk"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Geoppandas Dataframe from Polygons\nforest_df = gpd.GeoDataFrame(\n    {\"geometry\": [poly[0] for poly in forest_areas.values()]}, crs=\"EPSG:4326\"\n)\nnonforest_df = gpd.GeoDataFrame(\n    {\"geometry\": [poly[0] for poly in nonforest_areas.values()]},\n    crs=\"EPSG:4326\",\n)\n\n\n# Plotting Regions of Interest\nfig, ax = plt.subplots()\nrgb_median.plot.imshow(ax=ax, robust=True)\nforest_df.plot(ax=ax, ec=\"C0\", fc=\"none\")\nnonforest_df.plot(ax=ax, ec=\"C1\", fc=\"none\")\nax.set_title(\"Regions of Interest\")\nax.set_aspect(\"equal\")\nplt.show()","key":"HEy7ReOtom"},{"type":"output","id":"6yl7_MF05DkhTcu71r92z","data":[{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 640x480 with 1 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"282153f05ea9e2fc0212624576005f67","path":"/eo-datascience-cookbook/build/282153f05ea9e2fc0212624576005f67.png"}}}],"key":"POLm2PW1ft"}],"key":"pLzkhgUMBl"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Data Preparation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"R2wYN8qyKj"}],"identifier":"data-preparation","label":"Data Preparation","html_id":"data-preparation","implicit":true,"key":"Oy0xI9GKaR"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"In addition to the Regions of Interest we will extract the specific bands from the loaded dataset that we intend to use for the classification, which are the ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"yzO0iSkM6H"},{"type":"inlineCode","value":"red, green, blue","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"jXc6YZrh0E"},{"type":"text","value":" and ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"wyPawzp5zu"},{"type":"inlineCode","value":"near-infrared","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"wyj8Acmayd"},{"type":"text","value":" bands, although other bands can also be utilized. Using these bands, we will create both a training and a testing dataset. The training dataset will be used to train the classifier, while the testing dataset will be employed to evaluate its performance.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"g697bOLMwN"}],"key":"s3l5If6i2f"}],"key":"xR26VqT1zn"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Classifiying dataset (only necessary bands)\nbands = [\"red\", \"green\", \"blue\", \"nir\"]\nds_class = dc[bands].where(dc.valid).median(dim=\"time\")\nds_class = ds_class.fillna(0)\n\n\ndef clip_array(ds: xr.Dataset, polygons):\n    clipped = ds.rio.clip(polygons, invert=False, all_touched=False, drop=True)\n    clipped_nan = clipped.where(clipped == ds)\n    return clipped_nan\n\n\n# Dictionaries with Dataarrays, each clipped by a Polygon\ndata_dict_feat = {\n    idx: clip_array(ds_class, polygon) for idx, polygon in forest_areas.items()\n}\ndata_dict_nonfeat = {\n    idx: clip_array(ds_class, polygon) for idx, polygon in nonforest_areas.items()\n}","key":"L3uWDNapQq"},{"type":"output","id":"NTdkGb0h4MjCooGW_5ROd","data":[],"key":"WmACsVyHH9"}],"key":"ajtY98WFEt"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Reshape the polygon dataarrays to get a tuple (one value per band) of pixel values\nfeat_data = [\n    xarray.to_array().values.reshape(len(bands), -1).T\n    for xarray in data_dict_feat.values()\n]  # replaced median_data_dict_feat with data_dict_feat\nnonfeat_data = [\n    xarray.to_array().values.reshape(len(bands), -1).T\n    for xarray in data_dict_nonfeat.values()\n]  # replaced median_data_dict_feat with data_dict_feat\n\n# The rows of the different polygons are concatenated to a single array for further processing\nfeat_values = np.concatenate(feat_data)\nnonfeat_values = np.concatenate(nonfeat_data)\n\n# Drop Nan Values\nX_feat_data = feat_values[~np.isnan(feat_values).any(axis=1)]\nX_nonfeat_data = nonfeat_values[~np.isnan(nonfeat_values).any(axis=1)]","key":"mxXJa0IHIF"},{"type":"output","id":"bA9ObuvOldJr3_7zJCmvc","data":[],"key":"ee0bPWKRkV"}],"key":"V4MsQQ4Wfu"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Creating Output Vector (1 for pixel is features; 0 for pixel is not feature)\ny_feat_data = np.ones(X_feat_data.shape[0])\ny_nonfeat_data = np.zeros(X_nonfeat_data.shape[0])\n\n# Concatenate all Classes for training\nX = np.concatenate([X_feat_data, X_nonfeat_data])\ny = np.concatenate([y_feat_data, y_nonfeat_data])\n\n# Split into Training and Testing Data.\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=42\n)","key":"V86CzDkG0Z"},{"type":"output","id":"KrBjtETc7ek0yW6xHnEaE","data":[],"key":"qis1H1n3L8"}],"key":"M3jJpdz4J9"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now that we have prepared the training and testing data, we will create an image array of the actual scene that we intend to classify. This array will serve as the input for our classification algorithms, allowing us to apply the trained classifiers to the entire scene and identify the forested and non-forested areas accurately.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"U3VbPSj5Y1"}],"key":"DtGgQZopiT"}],"key":"b44cDvBY1n"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"image_data = (\n    ds_class[bands].to_array(dim=\"band\").transpose(\"latitude\", \"longitude\", \"band\")\n)\n\n# Reshape the image data\nnum_of_pixels = ds_class.sizes[\"longitude\"] * ds_class.sizes[\"latitude\"]\nnum_of_bands = len(bands)\nX_image_data = image_data.values.reshape(num_of_pixels, num_of_bands)","key":"QA6uSOjz9V"},{"type":"output","id":"n-UBJZL-i3FJVjnZGTJm4","data":[],"key":"LEEAmsmcuH"}],"key":"GJv2zWPkVE"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Classifiying with Naive Bayes","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Y6rrx4rDAi"}],"identifier":"classifiying-with-naive-bayes","label":"Classifiying with Naive Bayes","html_id":"classifiying-with-naive-bayes","implicit":true,"key":"rGYWCBhHoq"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Now that we have prepared all the needed data, we can begin the actual classification process.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"OFiaO3P6vB"}],"key":"fzLSSpl062"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"We will start with a ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"lt4WI0UpeL"},{"type":"emphasis","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Naive Bayes","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"ib0j0Hl3vE"}],"key":"KBZlZpoE25"},{"type":"text","value":" classifier. First, we will train the classifier using our training dataset. Once trained, we will apply the classifier to the actual image to identify the forested and non-forested areas.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"c8At0IB1rZ"}],"key":"xaHwCukxdh"}],"key":"jQEnOWp6nr"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Naive Bayes initialization and training\nnb = GaussianNB()\nnb_test = nb.fit(X_train, y_train)\nnb_predict = nb.predict(X_test)\n\n# Prediction on image\nnb_predict_img = nb.predict(X_image_data)\nnb_predict_img = nb_predict_img.reshape(\n    ds_class.sizes[\"latitude\"], ds_class.sizes[\"longitude\"]\n)\n\n# Adding the Naive Bayes Prediction to the dataset\nds_class[\"NB-forest\"] = xr.DataArray(\n    nb_predict_img,\n    dims=[\"latitude\", \"longitude\"],\n    coords={\n        \"longitude\": ds_class[\"longitude\"],\n        \"latitude\": ds_class[\"latitude\"],\n    },\n)","key":"Fxx1dRFfe6"},{"type":"output","id":"aUjYrhHq1eeI9VRbJGkse","data":[],"key":"HdpzyCr52u"}],"key":"l4s8tJxkJX"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"To evaluate the effectiveness of the classification, we will plot the image predicted by the classifier. Additionally, we will examine the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qTymZAHxuT"},{"type":"inlineCode","value":"Classification Report","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WWuJYxA5M8"},{"type":"text","value":" and the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"N49pcFKEvU"},{"type":"inlineCode","value":"Confusion Matrix","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W6ZQM49pWF"},{"type":"text","value":" to gain further insights into the classifierâ€™s performance.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sAB5XEujF1"}],"key":"MqAQGtFcy3"}],"key":"yKxkxBzsy5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Plot Naive Bayes\nalpha = 1\ncmap_green = colors.ListedColormap([(1, 1, 1, alpha), \"green\"])\n\nplot = ds_class[\"NB-forest\"].plot.imshow(\n    cmap=cmap_green, cbar_kwargs={\"ticks\": [0.25, 0.75]}\n)\ncbar = plot.colorbar\ncbar.set_ticklabels([\"non-forest\", \"forest\"])\nplot.axes.set_title(\"Naive Bayes Classification\")\nplt.show()\n\n# Print the Classification report\nprint(\"NAIVE BAYES: \\n \" + classification_report(y_test, nb_predict))\n\n# Print the confusion matrix\ncon_mat_nb = pd.DataFrame(\n    confusion_matrix(y_test, nb_predict),\n    index=[\"Actual Negative\", \"Actual Positive\"],\n    columns=[\"Predicted Negative\", \"Predicted Positive\"],\n)\ndisplay(con_mat_nb)","key":"NdmpSs4WU4"},{"type":"output","id":"4JhsVFB6662DABQHFVAcj","data":[{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 640x480 with 2 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"2cb04934af0e750924f60ba4e4353f11","path":"/eo-datascience-cookbook/build/2cb04934af0e750924f60ba4e4353f11.png"}}},{"output_type":"stream","name":"stdout","text":"NAIVE BAYES: \n               precision    recall  f1-score   support\n\n         0.0       0.95      0.82      0.88      6618\n         1.0       0.81      0.95      0.88      5487\n\n    accuracy                           0.88     12105\n   macro avg       0.88      0.88      0.88     12105\nweighted avg       0.89      0.88      0.88     12105\n\n"},{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"                 Predicted Negative  Predicted Positive\nActual Negative                5406                1212\nActual Positive                 276                5211","content_type":"text/plain"},"text/html":{"content":"\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003ePredicted Negative\u003c/th\u003e\n      \u003cth\u003ePredicted Positive\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003eActual Negative\u003c/th\u003e\n      \u003ctd\u003e5406\u003c/td\u003e\n      \u003ctd\u003e1212\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003eActual Positive\u003c/th\u003e\n      \u003ctd\u003e276\u003c/td\u003e\n      \u003ctd\u003e5211\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e","content_type":"text/html"}}}],"key":"Ot1q9pQHt6"}],"key":"VGxSRTyYoI"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Classifiying with Random Forest","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"n6WmN1At8v"}],"identifier":"classifiying-with-random-forest","label":"Classifiying with Random Forest","html_id":"classifiying-with-random-forest","implicit":true,"key":"dzc5zkDgX6"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"To ensure our results are robust, we will explore an additional classifier. In this section, we will use the Random Forest classifier. The procedure for using this classifier is the same as before: we will train the classifier using our training dataset and then apply it to the actual image to classify the scene.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"bKoDcET2ZX"}],"key":"LDsEVLMbkT"}],"key":"M5F0kfLgNZ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Random Forest initialization and training\nrf = RandomForestClassifier(n_estimators=100)\nrf_test = rf.fit(X_train, y_train)\nrf_predict = rf.predict(X_test)\n\n# Prediction on image\nrf_predict_img = rf.predict(X_image_data)\nrf_predict_img = rf_predict_img.reshape(\n    ds_class.sizes[\"latitude\"], ds_class.sizes[\"longitude\"]\n)\n\n# Adding the Random Forest Prediction to the dataset\nds_class[\"RF-forest\"] = xr.DataArray(\n    rf_predict_img,\n    dims=[\"latitude\", \"longitude\"],\n    coords={\n        \"longitude\": ds_class[\"longitude\"],\n        \"latitude\": ds_class[\"latitude\"],\n    },\n)\n\nplot = ds_class[\"RF-forest\"].plot.imshow(\n    cmap=cmap_green, cbar_kwargs={\"ticks\": [0.25, 0.75]}\n)\ncbar = plot.colorbar\ncbar.set_ticklabels([\"non-forest\", \"forest\"])\nplot.axes.set_title(\"Random Forest Classification\")\nplt.show()\n\n# Print the Classification report\nprint(\"RANDOM FOREST: \\n \" + classification_report(y_test, rf_predict))\n\n# Print the confusion matrix\ncon_mat_rf = pd.DataFrame(\n    confusion_matrix(y_test, rf_predict),\n    index=[\"Actual Negative\", \"Actual Positive\"],\n    columns=[\"Predicted Negative\", \"Predicted Positive\"],\n)\ndisplay(con_mat_rf)","key":"Y6VrdRxf2H"},{"type":"output","id":"_l2aRggLVt6KpaQMh378p","data":[{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 640x480 with 2 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"31dd48d4e262089885c6d079d4d47c34","path":"/eo-datascience-cookbook/build/31dd48d4e262089885c6d079d4d47c34.png"}}},{"output_type":"stream","name":"stdout","text":"RANDOM FOREST: \n               precision    recall  f1-score   support\n\n         0.0       0.96      0.95      0.95      6618\n         1.0       0.94      0.95      0.94      5487\n\n    accuracy                           0.95     12105\n   macro avg       0.95      0.95      0.95     12105\nweighted avg       0.95      0.95      0.95     12105\n\n"},{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"                 Predicted Negative  Predicted Positive\nActual Negative                6294                 324\nActual Positive                 283                5204","content_type":"text/plain"},"text/html":{"content":"\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003ePredicted Negative\u003c/th\u003e\n      \u003cth\u003ePredicted Positive\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003eActual Negative\u003c/th\u003e\n      \u003ctd\u003e6294\u003c/td\u003e\n      \u003ctd\u003e324\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003eActual Positive\u003c/th\u003e\n      \u003ctd\u003e283\u003c/td\u003e\n      \u003ctd\u003e5204\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e","content_type":"text/html"}}}],"key":"cPamkb4pcQ"}],"key":"Ra9juWBZAZ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We can already see from the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VIBe5i73u0"},{"type":"inlineCode","value":"classification reports","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WQNwh41BtW"},{"type":"text","value":" and the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i5gBQVpIyf"},{"type":"inlineCode","value":"confusion matrices","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YMeKqOEGQb"},{"type":"text","value":" that the Random Forest classifier has outperformed the Naive Bayes classifier. This is particularly evident from the lower values in the secondary diagonal, indicating minimal False Positives and False Negatives. It appears that the Naive Bayes classifier is more sensitive to False Positives, resulting in a higher rate of incorrect classifications.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hAbICSYSz2"}],"key":"ZeWYeSY79a"},{"type":"heading","depth":3,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Comparison of the Classificators","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"X6wb4r8QnG"}],"identifier":"comparison-of-the-classificators","label":"Comparison of the Classificators","html_id":"comparison-of-the-classificators","implicit":true,"key":"fJBY6XZ8O0"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"To gain a more in-depth understanding of the classifiersâ€™ performance, we will compare their results. Specifically, we will identify the areas where both classifiers agree and the areas where they disagree. This comparison will provide valuable insights into the strengths and weaknesses of each classifier, allowing us to better assess their effectiveness in identifying forested and non-forested regions.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Am5IhGefoi"}],"key":"K1G5CnwCYQ"}],"key":"nNrZi1OX0i"},{"type":"block","kind":"notebook-code","data":{"code-fold":true},"children":[{"type":"code","lang":"python","executable":true,"value":"cmap_trio = colors.ListedColormap([\"whitesmoke\", \"indianred\", \"goldenrod\", \"darkgreen\"])\n\n\ndouble_clf = ds_class[\"NB-forest\"] + 2 * ds_class[\"RF-forest\"]\n\nfig, ax = plt.subplots()\ncax = ax.imshow(double_clf, cmap=cmap_trio, interpolation=\"none\")\n\n# Add a colorbar with custom tick labels\ncbar = fig.colorbar(cax, ticks=[1 * 0.375, 3 * 0.375, 5 * 0.375, 7 * 0.375])\ncbar.ax.set_yticklabels([\"None\", \"Naive Bayes\", \"Random Forest\", \"Both\"])\nax.set_title(\"Classification Comparisson\")\nax.set_axis_off()\nplt.show()","key":"jF5vTxiGu0"},{"type":"output","id":"qWxuT0uXSLO0JSGiKF4W0","data":[{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 640x480 with 2 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"69d520b35c61c9e23db18a405a7f523b","path":"/eo-datascience-cookbook/build/69d520b35c61c9e23db18a405a7f523b.png"}}}],"key":"IhbqL7Fmew"}],"key":"f622e3u8G7"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The areas where both classifiers agree include the larger forested regions, such as the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Lhl4a7CF5z"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Nationalpark Donau-Auen","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pXzVstUMeW"}],"key":"uIOP3sRWre"},{"type":"text","value":" and the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SrgFOHEDqh"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Leithagebirge","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fkis9bgSap"}],"key":"mcyDkGwUel"},{"type":"text","value":". Additionally, both classifiers accurately identified the urban areas of Vienna and correctly excluded them from being classified as forested.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OcAfFOkNHN"}],"key":"ncqP4N06II"}],"key":"h7PlBFlveC"},{"type":"block","kind":"notebook-code","data":{"code-fold":true},"children":[{"type":"code","lang":"python","executable":true,"value":"# Plot only one class, either None (0), Naive Bayes (1), Random Forest (2), or Both (3)\nfig, axs = plt.subplots(2, 2, figsize=(8, 8))\nax = axs.ravel()\n\nfor i in range(4):\n    ax[i].imshow(double_clf == i, cmap=\"cmc.oleron_r\", interpolation=\"none\")\n    category = [\n        \"by None\",\n        \"only by Naive Bayes\",\n        \"only by Random Forest\",\n        \"by Both\",\n    ][i]\n    title = \"Areas classified \" + category\n    ax[i].set_title(title)\n    ax[i].set_axis_off()\n\nplt.tight_layout()","key":"M61YOGSznU"},{"type":"output","id":"kmxsDfhJoJW-LPgzV6fL_","data":[{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 800x800 with 4 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"6dbfca6f0574eff57a107c1c5ff53380","path":"/eo-datascience-cookbook/build/6dbfca6f0574eff57a107c1c5ff53380.png"}}}],"key":"z68ZlVoWlU"}],"key":"LG1HoS2Itb"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"When plotting the classified areas individually, we observe that the Random Forest classifier mistakenly identified the Danube River as a forested area. Conversely, the Naive Bayes classifier erroneously classified a significant amount of cropland as forest.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TFNPu20MuG"}],"key":"EkrmPXpx6N"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Finally, by analyzing the proportion of forested areas within the scene, we find that approximately 18% of the area is classified as forest, while around 66% is classified as non-forest. The remaining areas, which include water bodies and cropland, fall into less clearly defined categories.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"FbiL5ePsYr"}],"key":"YOK3RjhSag"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"The accompanying bar chart illustrates the distribution of these classifications, highlighting the percentage of forested areas, non-forested areas, and regions classified by only one of the two classifiers. This visual representation helps to quantify the areas of agreement and disagreement between the classifiers, providing a clearer picture of their performance.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"kpBuMi5p2M"}],"key":"yCKJt5NiWB"}],"key":"FppimaNwll"},{"type":"block","kind":"notebook-code","data":{"code-fold":true},"children":[{"type":"code","lang":"python","executable":true,"value":"counts = {}\nfor num in range(0, 4):\n    num_2_class = {0: \"None\", 1: \"Naive Bayes\", 2: \"Random Forest\", 3: \"Both\"}\n    counts[num_2_class[num]] = int((double_clf == num).sum().values)\n\nclass_counts_df = pd.DataFrame(list(counts.items()), columns=[\"Class\", \"Count\"])\nclass_counts_df[\"Percentage\"] = (\n    class_counts_df[\"Count\"] / class_counts_df[\"Count\"].sum()\n) * 100\nax = class_counts_df.plot.bar(\n    x=\"Class\",\n    y=\"Percentage\",\n    rot=0,\n    color=\"darkgreen\",\n    ylim=(0, 100),\n    title=\"Classified Areas per Classificator (%)\",\n)\n\n# Annotate the bars with the percentage values\nfor p in ax.patches:\n    ax.annotate(\n        f\"{p.get_height():.1f}%\",\n        (p.get_x() + p.get_width() / 2.0, p.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        xytext=(0, 9),\n        textcoords=\"offset points\",\n    )","key":"VmZJCfP565"},{"type":"output","id":"DJtCeCAwuC2oZfbV7AaZ4","data":[{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 640x480 with 1 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"23fb6b31812e99fd48291d5a6ea41fb5","path":"/eo-datascience-cookbook/build/23fb6b31812e99fd48291d5a6ea41fb5.png"}}}],"key":"XWHyrU9mzB"}],"key":"ACuSwHPXK1"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Conclusion","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SO19VpPITp"}],"identifier":"conclusion","label":"Conclusion","html_id":"conclusion","implicit":true,"key":"FIPWryW8J4"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"In this chapter, we utilized machine learning to classify satellite imagery into forested and non-forested areas, comparing Naive Bayes and Random Forest classifiers. The Random Forest classifier generally outperformed Naive Bayes, with fewer errors in classification, although it misclassified the Danube River as forested, while Naive Bayes incorrectly identified cropland as forest. The analysis, supported by the bar chart, revealed that about 18% of the scene was classified as forest, 66% as non-forest, and the remainder included ambiguous categories. This comparison highlights the strengths and limitations of each classifier, underscoring the need for careful selection and evaluation of classification methods.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"joaYBamRUA"}],"key":"yLPHEJkakb"}],"key":"Nq6YpNyvdp"}],"key":"on41OYyQ8w"},"references":{"cite":{"order":["nasa2020","rouse1974monitoring"],"data":{"nasa2020":{"label":"nasa2020","enumerator":"1","html":"NASA. (2020). \u003ci\u003eEarth Observatory\u003c/i\u003e. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://earthobservatory.nasa.gov/features/MeasuringVegetation/measuring_vegetation_2.php\"\u003ehttps://earthobservatory.nasa.gov/features/MeasuringVegetation/measuring_vegetation_2.php\u003c/a\u003e","url":"https://earthobservatory.nasa.gov/features/MeasuringVegetation/measuring_vegetation_2.php"},"rouse1974monitoring":{"label":"rouse1974monitoring","enumerator":"2","html":"Rouse, J. W., Haas, R. H., Schell, J. A., Deering, D. W., \u0026 others. (1974). Monitoring vegetation systems in the Great Plains with ERTS. \u003ci\u003eNASA Spec. Publ\u003c/i\u003e, \u003ci\u003e351\u003c/i\u003e(1), 309."}}}},"footer":{"navigation":{"prev":{"title":"Templates","url":"/notebooks/templates/prereqs-templates","group":"Templates"},"next":{"title":"Tutorials","url":"/notebooks/tutorials/prereqs-tutorials","group":"Tutorials"}}},"domain":"http://localhost:3000"},"project":{"open_access":true,"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"Apache-2.0","url":"https://opensource.org/licenses/Apache-2.0","name":"Apache License 2.0","free":true,"osi":true}},"title":"Earth Observation Data Science Cookbook","authors":[{"nameParsed":{"literal":"Wolfgang Wagner","given":"Wolfgang","family":"Wagner"},"name":"Wolfgang Wagner","id":"contributors-myst-generated-uid-0"},{"nameParsed":{"literal":"Martin Schobben","given":"Martin","family":"Schobben"},"name":"Martin Schobben","id":"contributors-myst-generated-uid-1"},{"nameParsed":{"literal":"Nikolas Pikall","given":"Nikolas","family":"Pikall"},"name":"Nikolas Pikall","id":"contributors-myst-generated-uid-2"},{"nameParsed":{"literal":"Joseph Wagner","given":"Joseph","family":"Wagner"},"name":"Joseph Wagner","id":"contributors-myst-generated-uid-3"},{"nameParsed":{"literal":"Davide Festa","given":"Davide","family":"Festa"},"name":"Davide Festa","id":"contributors-myst-generated-uid-4"},{"nameParsed":{"literal":"Felix David ReuÃŸ","given":"Felix David","family":"ReuÃŸ"},"name":"Felix David ReuÃŸ","id":"contributors-myst-generated-uid-5"},{"nameParsed":{"literal":"Luka JoviÄ‡","given":"Luka","family":"JoviÄ‡"},"name":"Luka JoviÄ‡","id":"contributors-myst-generated-uid-6"}],"contributors":[{"id":"brian-rose","nameParsed":{"literal":"Brian E. J. Rose","given":"Brian E. J.","family":"Rose"},"name":"Brian E. J. Rose","orcid":"0000-0002-9961-3821","email":"brose@albany.edu","affiliations":["UAlbany"],"url":"https://brian-rose.github.io","github":"brian-rose","bluesky":"@brianejrose.bsky.social","linkedin":"https://www.linkedin.com/in/brian-rose-37bb55106/"},{"id":"clyne","nameParsed":{"literal":"John Clyne","given":"John","family":"Clyne"},"name":"John Clyne","orcid":"0000-0003-2788-9017","affiliations":["CISL"],"github":"clyne"},{"id":"jukent","nameParsed":{"literal":"Julia Kent","given":"Julia","family":"Kent"},"name":"Julia Kent","orcid":"0000-0002-5611-8986","affiliations":["CISL"],"github":"jukent"},{"id":"ktyle","nameParsed":{"literal":"Kevin Tyle","given":"Kevin","family":"Tyle"},"name":"Kevin Tyle","orcid":"0000-0001-5249-9665","affiliations":["UAlbany"],"github":"ktyle"},{"id":"andersy005","nameParsed":{"literal":"Anderson Banihirwe","given":"Anderson","family":"Banihirwe"},"name":"Anderson Banihirwe","orcid":"0000-0001-6583-571X","affiliations":["CarbonPlan"],"github":"andersy005"},{"id":"dcamron","nameParsed":{"literal":"Drew Camron","given":"Drew","family":"Camron"},"name":"Drew Camron","orcid":"0000-0001-7246-6502","affiliations":["Unidata"],"github":"dcamron"},{"id":"dopplershift","nameParsed":{"literal":"Ryan May","given":"Ryan","family":"May"},"name":"Ryan May","orcid":"0000-0003-2907-038X","affiliations":["Unidata"],"github":"dopplershift"},{"id":"mgrover1","nameParsed":{"literal":"Maxwell Grover","given":"Maxwell","family":"Grover"},"name":"Maxwell Grover","orcid":"0000-0002-0370-8974","affiliations":["Argonne"],"github":"mgrover1"},{"id":"r-ford","nameParsed":{"literal":"Robert R. Ford","given":"Robert R.","family":"Ford"},"name":"Robert R. Ford","orcid":"0000-0001-5483-4965","affiliations":["UAlbany"],"github":"r-ford"},{"id":"kmpaul","nameParsed":{"literal":"Kevin Paul","given":"Kevin","family":"Paul"},"name":"Kevin Paul","orcid":"0000-0001-8155-8038","affiliations":["NVIDIA"],"github":"kmpaul"},{"id":"jnmorley","nameParsed":{"literal":"James Morley","given":"James","family":"Morley"},"name":"James Morley","orcid":"0009-0005-5193-7981","github":"jnmorley"},{"id":"erogluorhan","nameParsed":{"literal":"Orhan Eroglu","given":"Orhan","family":"Eroglu"},"name":"Orhan Eroglu","orcid":"0000-0003-3099-8775","affiliations":["CISL"],"github":"erogluorhan"},{"id":"lkailynncar","nameParsed":{"literal":"Lily Kailyn","given":"Lily","family":"Kailyn"},"name":"Lily Kailyn","orcid":"0009-0002-0125-5091","affiliations":["CISL"],"github":"lkailynncar"},{"id":"anissa111","nameParsed":{"literal":"Anissa Zacharias","given":"Anissa","family":"Zacharias"},"name":"Anissa Zacharias","orcid":"0000-0002-2666-8493","affiliations":["CISL"],"github":"anissa111"},{"id":"kafitzgerald","nameParsed":{"literal":"Katelyn FitzGerald","given":"Katelyn","family":"FitzGerald"},"name":"Katelyn FitzGerald","orcid":"0000-0003-4184-1917","affiliations":["CISL"],"github":"kafitzgerald"}],"affiliations":[{"id":"UAlbany","name":"University at Albany (SUNY)","department":"Atmospheric and Environmental Sciences","url":"https://www.albany.edu/daes"},{"id":"CISL","name":"NSF National Center for Atmospheric Research","department":"Computational and Information Systems Lab","url":"https://www.cisl.ucar.edu"},{"id":"Unidata","name":"NSF Unidata","url":"https://www.unidata.ucar.edu"},{"id":"Argonne","name":"Argonne National Laboratory","department":"Environmental Science Division","url":"https://www.anl.gov/evs"},{"id":"CarbonPlan","name":"CarbonPlan","url":"https://carbonplan.org"},{"id":"NVIDIA","name":"NVIDIA Corporation","url":"https://www.nvidia.com/"}],"copyright":"2024","references":{"foundations":{"url":"https://foundations.projectpythia.org"},"xarray":{"url":"https://docs.xarray.dev/en/stable"},"matplotlib":{"url":"https://matplotlib.org/stable"}},"thebe":{"binder":{"url":"https://mybinder.org/","provider":"github","repo":"projectpythia/eo-datascience-cookbook","ref":"HEAD"}},"toc":[{"file":"README.md"},{"children":[{"file":"notebooks/how-to-cite.md"}],"title":"Preamble"},{"children":[{"children":[{"file":"notebooks/courses/microwave-remote-sensing/unit_01/01_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_01/02_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_01/03_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_02/04_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_02/05_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_02/06_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_03/07_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_03/08_in_class_exercise.ipynb"},{"file":"notebooks/courses/microwave-remote-sensing/unit_03/09_in_class_exercise.ipynb"}],"file":"notebooks/courses/microwave-remote-sensing.ipynb"},{"children":[{"file":"notebooks/courses/environmental-remote-sensing/unit_01/01_handout_drought.ipynb"},{"file":"notebooks/courses/environmental-remote-sensing/unit_01/02_handout_drought.ipynb"},{"file":"notebooks/courses/environmental-remote-sensing/unit_01/03_handout_drought.ipynb"},{"file":"notebooks/courses/environmental-remote-sensing/unit_01/04_handout_drought.ipynb"}],"file":"notebooks/courses/environmental-remote-sensing.ipynb"}],"title":"Courses"},{"children":[{"children":[{"file":"notebooks/templates/classification.ipynb"}],"file":"notebooks/templates/prereqs-templates.ipynb"}],"title":"Templates"},{"children":[{"children":[{"file":"notebooks/tutorials/floodmapping.ipynb"}],"file":"notebooks/tutorials/prereqs-tutorials.ipynb"}],"title":"Tutorials"},{"children":[{"file":"notebooks/references.ipynb"}],"title":"References"}],"thumbnail":"/eo-datascience-cookbook/build/tuw-geo_eodc_logo_ho-a768683616f0967cd254152a2de90c57.png","exports":[],"bibliography":[],"index":"index","pages":[{"level":1,"title":"Preamble"},{"slug":"notebooks.how-to-cite","title":"How to Cite This Cookbook","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/940d5ffd64b58a1e369604298a5c4034.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Courses"},{"slug":"notebooks.courses.microwave-remote-sensing","title":"Microwave Remote Sensing","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"notebooks.courses.microwave-remote-sensing.unit-01.in-class-exercise","title":"Discover and Read SAR Data","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-01.in-class-exercise-1","title":"Unit Conversion","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-01.in-class-exercise-2","title":"Backscattering Coefficients","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/side_looking_image_d-09f49644d88e802a414d9dd462acaaac.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-02.in-class-exercise","title":"Datacubes","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/dbe80838a9862c93c705efd1a8edd024.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-02.in-class-exercise-1","title":"Wavelength and Polarization","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-02.in-class-exercise-2","title":"Dielectric Properties","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-03.in-class-exercise","title":"Speckle Statistics","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/speckle_effect-9973bd7f075fdf526b747812edcc7f04.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-03.in-class-exercise-1","title":"Interferograms","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/8494624449702f8b89e05532470631cf.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.microwave-remote-sensing.unit-03.in-class-exercise-2","title":"Phase Unwrapping","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/3f26b4039a027763e4d44920b3792e29.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.environmental-remote-sensing","title":"Environmental Remote Sensing","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"notebooks.courses.environmental-remote-sensing.unit-01.handout-drought","title":"Remotely Sensed Droughts in Mozambique","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/0e0633405e49fe13238fa11c831f4f0f.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.environmental-remote-sensing.unit-01.handout-drought-1","title":"Evaluating the Reliability of Remotely Sensed Soil Moisture","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/7334c1f7c83b0b6a3bd7d11ff35388d8.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.environmental-remote-sensing.unit-01.handout-drought-2","title":"Evaluating the Reliability of Remotely Sensed Droughts","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/cfde475a9031e72eedf656b180d652ed.webp","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"notebooks.courses.environmental-remote-sensing.unit-01.handout-drought-3","title":"Leads and lags in Drought Dynamics","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/f20c6145ef56baa28c62ffb1caf68f93.webp","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"level":1,"title":"Templates"},{"slug":"notebooks.templates.prereqs-templates","title":"Templates","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"notebooks.templates.classification","title":"Classification of Sentinel-2 imagery","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"level":1,"title":"Tutorials"},{"slug":"notebooks.tutorials.prereqs-tutorials","title":"Tutorials","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"notebooks.tutorials.floodmapping","title":"Reverend Bayes updates our Belief in Flood Detection","description":"","date":"","thumbnail":"/eo-datascience-cookbook/build/1ab490d3ba1d22952fb4c45d205afb33.gif","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"level":1,"title":"References"},{"slug":"notebooks.references","title":"References","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}}},"actionData":null,"errors":null},"future":{"unstable_dev":false,"unstable_postcss":false,"unstable_tailwind":false,"v2_errorBoundary":true,"v2_headers":true,"v2_meta":true,"v2_normalizeFormMethod":true,"v2_routeConvention":true}};</script><script type="module" async="">import "/eo-datascience-cookbook/build/manifest-ED236704.js";
import * as route0 from "/eo-datascience-cookbook/build/root-ZJOPFBMV.js";
import * as route1 from "/eo-datascience-cookbook/build/routes/$-CQPS5IOR.js";
window.__remixRouteModules = {"root":route0,"routes/$":route1};

import("/eo-datascience-cookbook/build/entry.client-UNPC4GT3.js");</script></body></html>